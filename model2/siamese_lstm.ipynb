{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Siamese-LSTM baseline\n",
    "- The baseline of Siamese LSTM, from https://www.kaggle.com/amoyyean/lstm-with-glove\n",
    "- With Glove pretrained model as the initial embedding matrix for embedding layer.\n",
    "- In the thesis of `Siamese Recurrent Architectures for Learning Sentence Similarity`, a siamese lstm model for qqp problem is defined with the name of Manhattan LSTM Model.(https://www.researchgate.net/profile/Aditya_Thyagarajan/publication/307558687_Siamese_Recurrent_Architectures_for_Learning_Sentence_Similarity/links/5bf2424ba6fdcc3a8de0e69e/Siamese-Recurrent-Architectures-for-Learning-Sentence-Similarity.pdf)\n",
    "  - Elior Cohen has done some experiments on MaLSTM with this dataset, results in: https://medium.com/mlreview/implementing-malstm-on-kaggles-quora-question-pairs-competition-8b31b0b16a07, the result is 80.xx% for validation accuracy.\n",
    "  - Here this model `Siamese-LSTM baseline` adds some leaky feature and changes some network structure, achiving a better performance. \n",
    "- So I plan to use this model as a baseline for NN solution in our project, and make changes based on that to see if we can construct a better model and achive a even better performance.\n",
    "   \n",
    "   \n",
    "   \n",
    "- **Output:** \n",
    "  - `lstm.csv`: 4 epoch, score: 0.193\n",
    "  - `lstm_1.csv`: 20 epoch, score 0.18662"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from string import punctuation\n",
    "from collections import defaultdict\n",
    "# from tqdm import tqdm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "# Keras package\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, Embedding, Dropout, Activation, LSTM, Lambda\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.layers.pooling import GlobalAveragePooling1D\n",
    "import keras.backend as K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM Stucture:\n",
      "Num_Lstm: 198\n",
      "Num_Dense: 142\n",
      "Dropout rate in LSTM layer: 0.4117299071146573\n",
      "Dropout rate in Dense layer:: 0.3726788153726159\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter defination\n",
    "\n",
    "# Use the following instructions to download glove and unzip it, if already installed, just comment them.\n",
    "# !wget http://nlp.stanford.edu/data/glove.840B.300d.zip\n",
    "# !unzip glove.840B.300d.zip\n",
    "EMBEDDING_FILE = 'glove.840B.300d.txt'\n",
    "\n",
    "TRAIN_DATA_FILE = 'Data/train.csv'\n",
    "TEST_DATA_FILE = 'Data/test.csv'\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 60  \n",
    "MAX_NUM_WORDS = 200000  # There are about 201000 unique words in training dataset, 200000 is enough for tokenization\n",
    "EMBEDDING_DIM = 300  # word-embedded-vector dimension(300 is for 'glove.840B.300d')\n",
    "VALIDATION_SPLIT_RATE = 0.1 \n",
    "N_HIDDEN = np.random.randint(175, 275) # 250-400\n",
    "N_DENSE = np.random.randint(100, 150)  # 120-200\n",
    "DROPOUT_RATE_LSTM = 0.15 + np.random.rand() * 0.33  # drop-out possibility, random set to avoid outfitting\n",
    "DROUPOUT_RATE_DENSE = 0.15 + np.random.rand() * 0.33\n",
    "\n",
    "VERSION = 'Temp/lstm'\n",
    "print('LSTM Stucture:')\n",
    "print('Num_Lstm:', N_HIDDEN)\n",
    "print('Num_Dense:', N_DENSE)\n",
    "print('Dropout rate in LSTM layer:', DROPOUT_RATE_LSTM) \n",
    "print('Dropout rate in Dense layer::', DROUPOUT_RATE_DENSE)\n",
    "\n",
    "ACTIVE_FUNC = 'relu'\n",
    "re_weight = True  # whether to re-weight classes to fit the 17.4% share in test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create word embedding dictionary\n",
      "Found 2195892 word vectors of glove.\n",
      "Processing text dataset\n",
      "404290 texts are found in train.csv\n",
      "2345796 texts are found in test.csv\n"
     ]
    }
   ],
   "source": [
    "# Create word embedding dictionary from 'glove.840B.300d.txt', {key:value} is {word: glove vector(300,)}\n",
    "print('Create word embedding dictionary')\n",
    "\n",
    "embeddings_index = {}  # the output dictionary\n",
    "f = open(EMBEDDING_FILE, encoding='utf-8')\n",
    "for line in f:  # tqdm\n",
    "    values = line.split()\n",
    "    word = ''.join(values[:-300])   \n",
    "    coefs = np.asarray(values[-300:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found {} word vectors of glove.'.format(len(embeddings_index)))\n",
    "\n",
    "\n",
    "# Preprocess text in dataset\n",
    "print('Processing text dataset')\n",
    "\n",
    "def text_to_wordlist(text, remove_stopwords=False, stem_words=False):\n",
    "    # Clean the text, with the option to remove stopwords and to stem words.\n",
    "    \n",
    "    # Convert words to lower case and split them\n",
    "    text = text.lower().split()\n",
    "\n",
    "    # Optionally, remove stop words\n",
    "    if remove_stopwords:\n",
    "        stop_words = set(stopwords.words(\"english\"))\n",
    "        text = [w for w in text if not w in stop_words]\n",
    "    \n",
    "    text = \" \".join(text)\n",
    "\n",
    "    # Use re to clean the text\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    # text = re.sub(r\"\\0s\", \"0\", text) # It doesn't make sense to me\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    \n",
    "    # Optionally, shorten words to their stems\n",
    "    if stem_words:\n",
    "        text = text.split()\n",
    "        stemmer = SnowballStemmer('english')\n",
    "        stemmed_words = [stemmer.stem(word) for word in text]\n",
    "        text = \" \".join(stemmed_words)\n",
    "    # or nltk package\n",
    "    # if lemma:\n",
    "    #    text = text.split()\n",
    "    #    wn = nltk.WordNetLemmatizer()\n",
    "    #    lemm_words = [wn.lemmatize(word) for word in text]\n",
    "    #    text = \" \".join(lemm_words)\n",
    "    \n",
    "    # Return a list of words\n",
    "    return(text)\n",
    "\n",
    "# Load training data and process with text_to_wordlist (Preprocessing)\n",
    "train_texts_1 = []  # the preprocessed text of q1\n",
    "train_texts_2 = []  # the preprocessed text of q2\n",
    "train_labels = []  # training labels\n",
    "\n",
    "df_train = pd.read_csv(TRAIN_DATA_FILE, encoding='utf-8')  # the original training data\n",
    "df_train = df_train.fillna('empty')\n",
    "train_q1 = df_train.question1.values  # the original text of q1\n",
    "train_q2 = df_train.question2.values  # the original text fo q2\n",
    "train_labels = df_train.is_duplicate.values  # the original label('is_duplicate')\n",
    "\n",
    "for text in train_q1:\n",
    "    train_texts_1.append(text_to_wordlist(text, remove_stopwords=False, stem_words=False))\n",
    "    \n",
    "for text in train_q2:\n",
    "    train_texts_2.append(text_to_wordlist(text, remove_stopwords=False, stem_words=False))\n",
    "\n",
    "print('{} texts are found in train.csv'.format(len(train_texts_1)))\n",
    "\n",
    "# Load testing data and process with text_to_wordlist (Preprocessing)\n",
    "test_texts_1 = []  # the preprocessed text of q1_test\n",
    "test_texts_2 = []  # the preprocessed text of q2_test\n",
    "test_ids = []  # id..\n",
    "\n",
    "df_test = pd.read_csv(TEST_DATA_FILE, encoding='utf-8')  # the original testing data\n",
    "df_test = df_test.fillna('empty')\n",
    "test_q1 = df_test.question1.values  # the original text of q1_test\n",
    "test_q2 = df_test.question2.values  # the original text of q2_test\n",
    "test_ids = df_test.test_id.values  # id..\n",
    "\n",
    "for text in test_q1:\n",
    "    test_texts_1.append(text_to_wordlist(text, remove_stopwords=False, stem_words=False))\n",
    "    \n",
    "for text in test_q2:\n",
    "    test_texts_2.append(text_to_wordlist(text, remove_stopwords=False, stem_words=False))\n",
    "    \n",
    "print('{} texts are found in test.csv'.format(len(test_texts_1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120499 unique tokens are found\n",
      "Shape of train data tensor: (404290, 60)\n",
      "Shape of train labels tensor: (404290,)\n",
      "Shape of test data vtensor: (2345796, 60)\n",
      "Shape of test ids tensor: (2345796,)\n"
     ]
    }
   ],
   "source": [
    "# Keras.Tokenize for all text:\n",
    "# First construct a Tokenizer()\n",
    "# Then use tokenizer_on_texts() method to learn the dictionary of the corpus(all texts(sentences)). We can use .word_index to map between the each word (distinct) with the corresponding number.\n",
    "# Then use text_to_sequence() method to transfer every text(sentence) in texts into sequences of word_indexes.\n",
    "# Then add the same length by padding method: padding_sequences().\n",
    "# Finally use the embedding layer in keras to carry out a vectorization, and input it into LSTM.\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(train_texts_1 + train_texts_2 + test_texts_1 + test_texts_2)  # generate a token dictionary, \n",
    "\n",
    "train_sequences_1 = tokenizer.texts_to_sequences(train_texts_1)  # sequence of q1\n",
    "train_sequences_2 = tokenizer.texts_to_sequences(train_texts_2)  # sequence of q2\n",
    "test_sequences_1 = tokenizer.texts_to_sequences(test_texts_1)  # sequence of q1_test\n",
    "test_sequences_2 = tokenizer.texts_to_sequences(test_texts_2)  # sequence of q2_test\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('{} unique tokens are found'.format(len(word_index)))\n",
    "\n",
    "# Pad all train with Max_Sequence_Length: 60\n",
    "train_data_1 = pad_sequences(train_sequences_1, maxlen=MAX_SEQUENCE_LENGTH)  # padded_sequence of q1 as train_data\n",
    "train_data_2 = pad_sequences(train_sequences_2, maxlen=MAX_SEQUENCE_LENGTH)  # padded_sequence of q2 as train_data\n",
    "print('Shape of train data tensor:', train_data_1.shape)\n",
    "print('Shape of train labels tensor:', train_labels.shape)\n",
    "\n",
    "# Pad all test with Max_Sequence_Length\n",
    "test_data_1 = pad_sequences(test_sequences_1, maxlen=MAX_SEQUENCE_LENGTH)  # padded_sequence of q1_test as test_data\n",
    "test_data_2 = pad_sequences(test_sequences_2, maxlen=MAX_SEQUENCE_LENGTH)  # padded_sequence of q2_test as test_data\n",
    "print('Shape of test data vtensor:', test_data_2.shape)\n",
    "print('Shape of test ids tensor:', test_ids.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get leaky features (NLP features)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "questions = pd.concat([df_train[['question1', 'question2']], df_test[['question1', 'question2']]], axis=0).reset_index(drop='index')\n",
    "q_dict = defaultdict(set)\n",
    "for i in range(questions.shape[0]):\n",
    "        q_dict[questions.question1[i]].add(questions.question2[i])\n",
    "        q_dict[questions.question2[i]].add(questions.question1[i])\n",
    "\n",
    "def q1_freq_train(row):\n",
    "    return len(q_dict.get(row[3]))\n",
    "    # return(len(q_dict[row['question1']]))\n",
    "\n",
    "def q2_freq_train(row):\n",
    "    return len(q_dict.get(row[4]))\n",
    "    # return(len(q_dict[row['question2']]))\n",
    "\n",
    "def q1_q2_intersect_train(row):\n",
    "    return(len(set(q_dict.get(row[3])).intersection(set(q_dict.get(row[4])))))\n",
    "    # return(len(set(q_dict[row['question1']]).intersection(set(q_dict[row['question2']]))))\n",
    "\n",
    "def q1_freq_test(row):\n",
    "    return len(q_dict.get(row[1]))\n",
    "    # return(len(q_dict[row['question1']]))\n",
    "\n",
    "def q2_freq_test(row):\n",
    "    return len(q_dict.get(row[2]))\n",
    "    # return(len(q_dict[row['question2']]))\n",
    "\n",
    "def q1_q2_intersect_test(row):\n",
    "    return(len(set(q_dict.get(row[1])).intersection(set(q_dict.get(row[2])))))\n",
    "    # return(len(set(q_dict[row['question1']]).intersection(set(q_dict[row['question2']]))))\n",
    "\n",
    "df_train['q1_q2_intersect'] = df_train.apply(q1_q2_intersect_train, axis=1, raw=True)\n",
    "df_train['q1_freq'] = df_train.apply(q1_freq_train, axis=1, raw=True)\n",
    "df_train['q2_freq'] = df_train.apply(q2_freq_train, axis=1, raw=True)\n",
    "\n",
    "df_test['q1_q2_intersect'] = df_test.apply(q1_q2_intersect_test, axis=1, raw=True)\n",
    "df_test['q1_freq'] = df_test.apply(q1_freq_test, axis=1, raw=True)\n",
    "df_test['q2_freq'] = df_test.apply(q2_freq_test, axis=1, raw=True)\n",
    "\n",
    "leaks = df_train[['q1_q2_intersect', 'q1_freq', 'q2_freq']]  # the leaky feature\n",
    "test_leaks = df_test[['q1_q2_intersect', 'q1_freq', 'q2_freq']]  # the leaky feature_test\n",
    "\n",
    "\n",
    "# Make scaling for leaky feature\n",
    "ss = StandardScaler()\n",
    "ss.fit(np.vstack((leaks, test_leaks)))\n",
    "leaks = ss.transform(leaks)  # the leaky feature\n",
    "test_leaks = ss.transform(test_leaks)  # the leaky feature_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train & Validation split\n",
    "perm = np.random.permutation(len(train_data_1))\n",
    "idx_train = perm[:int(len(train_data_1)*(1-VALIDATION_SPLIT_RATE))]\n",
    "idx_val = perm[int(len(train_data_1)*(1-VALIDATION_SPLIT_RATE)):]\n",
    "\n",
    "data_1_train = np.vstack((train_data_1[idx_train], train_data_2[idx_train]))\n",
    "data_2_train = np.vstack((train_data_2[idx_train], train_data_1[idx_train]))\n",
    "leaks_train = np.vstack((leaks[idx_train], leaks[idx_train]))\n",
    "labels_train = np.concatenate((train_labels[idx_train], train_labels[idx_train]))\n",
    "\n",
    "data_1_val = np.vstack((train_data_1[idx_val], train_data_2[idx_val]))\n",
    "data_2_val = np.vstack((train_data_2[idx_val], train_data_1[idx_val]))\n",
    "leaks_val = np.vstack((leaks[idx_val], leaks[idx_val]))\n",
    "labels_val = np.concatenate((train_labels[idx_val], train_labels[idx_val]))\n",
    "\n",
    "weight_val = np.ones(len(labels_val))\n",
    "if re_weight:\n",
    "    weight_val *= 0.471544715\n",
    "    weight_val[labels_val==0] = 1.309033281\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing embedding matrix\n",
      "Null word embeddings: \n",
      "Starting the model training\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 60)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 60)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 60, 300)      36150000    input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 3)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 198)          395208      embedding_1[0][0]                \n",
      "                                                                 embedding_1[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 71)           284         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 467)          0           lstm_1[0][0]                     \n",
      "                                                                 lstm_1[1][0]                     \n",
      "                                                                 dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 467)          1868        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 467)          0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 142)          66456       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 142)          568         dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 142)          0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1)            143         dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 36,614,527\n",
      "Trainable params: 463,309\n",
      "Non-trainable params: 36,151,218\n",
      "__________________________________________________________________________________________________\n",
      "Train on 727722 samples, validate on 80858 samples\n",
      "Epoch 1/4\n",
      "727722/727722 [==============================] - 1826s 3ms/step - loss: 0.2805 - acc: 0.8232 - val_loss: 0.2930 - val_acc: 0.7955\n",
      "Epoch 2/4\n",
      "727722/727722 [==============================] - 1838s 3ms/step - loss: 0.2457 - acc: 0.8344 - val_loss: 0.2331 - val_acc: 0.8351\n",
      "Epoch 3/4\n",
      "727722/727722 [==============================] - 1804s 2ms/step - loss: 0.2362 - acc: 0.8380 - val_loss: 0.2216 - val_acc: 0.8437\n",
      "Epoch 4/4\n",
      "727722/727722 [==============================] - 1909s 3ms/step - loss: 0.2287 - acc: 0.8406 - val_loss: 0.2246 - val_acc: 0.8559\n"
     ]
    }
   ],
   "source": [
    "# Create embedding matrix for embedding layer, which is used in the keras.embedding weight as the initializer.\n",
    "print('Preparing embedding matrix')\n",
    "\n",
    "num_words = min(MAX_NUM_WORDS, len(word_index))+1\n",
    "\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))  # the weight of Embedding layer\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "print('Null word embeddings: '.format(np.sum(np.sum(embedding_matrix, axis=1) == 0)))\n",
    "\n",
    "\n",
    "# NN Model design\n",
    "# Structure: (q1-embedding-lstm + q2-embedding-lstm + leaky-dense)-dense-sigmoid-result\n",
    "\n",
    "# The embedding layer containing the word vectors\n",
    "emb_layer = Embedding(\n",
    "    input_dim=num_words,\n",
    "    output_dim=EMBEDDING_DIM,\n",
    "    weights=[embedding_matrix],\n",
    "    input_length=MAX_SEQUENCE_LENGTH,\n",
    "    trainable=False\n",
    ")    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Input layer\n",
    "seq1 = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "seq2 = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "\n",
    "# Embedding layer\n",
    "emb1 = emb_layer(seq1)\n",
    "emb2 = emb_layer(seq2)\n",
    "\n",
    "# LSTM layer\n",
    "lstm_layer = LSTM(N_HIDDEN, dropout=DROPOUT_RATE_LSTM, recurrent_dropout=DROPOUT_RATE_LSTM)\n",
    "lstm_a = lstm_layer(emb1)\n",
    "lstm_b = lstm_layer(emb2)\n",
    "\n",
    "# add features\n",
    "leaky_input = Input(shape=(leaks.shape[1],))\n",
    "leaky_dense = Dense(int(N_DENSE/2), activation=ACTIVE_FUNC)(leaky_input)\n",
    "\n",
    "# merge \n",
    "merged = concatenate([lstm_a, lstm_b, leaky_dense])\n",
    "merged = BatchNormalization()(merged)\n",
    "merged = Dropout(DROUPOUT_RATE_DENSE)(merged)\n",
    "merged = Dense(N_DENSE, activation=ACTIVE_FUNC)(merged)\n",
    "merged = BatchNormalization()(merged)\n",
    "merged = Dropout(DROUPOUT_RATE_DENSE)(merged)\n",
    "\n",
    "preds = Dense(1, activation='sigmoid')(merged)\n",
    "\n",
    "\n",
    "# Add class weight, magic feature for the unbalancement of training labels.\n",
    "if re_weight:\n",
    "    class_weight = {0: 1.309033281, 1: 0.471544715}\n",
    "else:\n",
    "    class_weight = None\n",
    "    \n",
    "\n",
    "# Train the model\n",
    "\n",
    "print('Starting the model training')\n",
    "\n",
    "model = Model(inputs=[seq1, seq2, magic_input], outputs=preds)\n",
    "model.compile(loss='binary_crossentropy', optimizer='nadam', metrics=['acc'])\n",
    "\n",
    "# Summerization of model\n",
    "model.summary()\n",
    "\n",
    "# Set early stopping (large patience should be useful)\n",
    "early_stopping =EarlyStopping(monitor='val_loss', patience=10)\n",
    "bst_model_path = VERSION + '.h5' \n",
    "model_checkpoint = ModelCheckpoint(bst_model_path, save_best_only=True, save_weights_only=True)\n",
    "\n",
    "\n",
    "hist = model.fit([data_1_train, data_2_train, leaks_train], labels_train, \\\n",
    "        validation_data=([data_1_val, data_2_val, leaks_val], labels_val, weight_val), \\\n",
    "        epochs=4, batch_size=2048, shuffle=True, \\\n",
    "        class_weight=class_weight, callbacks=[early_stopping, model_checkpoint])\n",
    "\n",
    "model.load_weights(bst_model_path) # sotre model parameters in .h5 file\n",
    "bst_val_score = min(hist.history['val_loss'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 60)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_8 (InputLayer)            (None, 60)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 60, 300)      36150000    input_7[0][0]                    \n",
      "                                                                 input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_9 (InputLayer)            (None, 3)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) (None, 446)          934816      embedding_4[0][0]                \n",
      "                                                                 embedding_4[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 60)           240         input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 952)          0           bidirectional_3[0][0]            \n",
      "                                                                 bidirectional_3[1][0]            \n",
      "                                                                 dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 952)          3808        concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 952)          0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 121)          115313      dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 121)          484         dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 121)          0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 1)            122         dropout_6[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 37,204,783\n",
      "Trainable params: 1,052,637\n",
      "Non-trainable params: 36,152,146\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making the submission\n",
      "2345796/2345796 [==============================] - 5586s 2ms/step\n",
      "2345796/2345796 [==============================] - 2313s 986us/step\n"
     ]
    }
   ],
   "source": [
    "# Make the submission\n",
    "\n",
    "print('Making the submission')\n",
    "\n",
    "preds = model.predict([test_data_1, test_data_2, test_leaks], batch_size=8192, verbose=1)\n",
    "preds += model.predict([test_data_2, test_data_1, test_leaks], batch_size=8192, verbose=1)\n",
    "preds /= 2\n",
    "\n",
    "submission = pd.DataFrame({'test_id':test_ids, 'is_duplicate':preds.ravel()})\n",
    "submission.to_csv('Models/lstm_1.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also tried many **indivisual** comparison experiments on model hyperparameters and network stuctures, here is a briefly introduction on what we did:\n",
    "- Hyperparameters:\n",
    "  - Here the baseline is `lstm.ipynb`\n",
    "  - We did some changes on hyperparameters such as hidden neurons in LSTM layer, neurons in Dense layer to find the 'best' parameter values: (250, 120).\n",
    "    - it is not convincible to say 'the best', here we just put up our experiment results.\n",
    "  - We also did some changes on: (under the hyperparameter set: (250,120,0.33,0.33))\n",
    "    - preprocess raw data with stopwords and stemwords:\n",
    "      - scored 0.19642 while 0.18839 is the baseline score\n",
    "    - don't use leaky features (just 3 features):\n",
    "      - scored 0.31223 while 0.18839 is the baseline score\n",
    "      - the result shows that leaky feature may have a big impact on the final score, so we tried to add features from previous work (feature_engineering_train,ipynb) and change the neural network structure accordingly to build a new model. Actually it works even better (details in the next chapter). \n",
    "    - don't use class reweight:\n",
    "      - scored 0.31730 while 0.18839 is the baseline score\n",
    "      - reweight method is a magic method provided by kagglers, it's inpiring! \n",
    "- Network Sturcutres \n",
    "  - We also changed the neural network structure to see if there is an improvement on the final score\n",
    "    - Here the baseline is the `lstm_featured.ipynb`, which scores 0.16515\n",
    "    - Add substract and multiply on **featured_lstm** feature\n",
    "      - Scored 0.16674, no obvious improvements\n",
    "    - Change N_DENSE to N_DENSE/2\n",
    "      - Scored 0.16681 while 0.16515 is the baseline\n",
    "    - Add Dense layer after merging features\n",
    "      - Scored 0.1840 while 0.16515 is the baseline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Analysis/Models\n",
    "\n",
    "Apart from the baseline model and the comparison experiments. We also implemented new analysis/models based on the baseline model, and all of them have achieve an improvement on the test_data score. Here are the details:\n",
    "- BiLSTM model, see `bilstm.ipynb`\n",
    "- Siamese-LSTM with Features (tm+nlp), see `lstm_featured.ipynb`\n",
    "- Combined model: combine all the improvements to a final model, see `lstm_final.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
