{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Siamese-LSTM with Features(tm+nlp)\n",
    "- Plus features in `feature_nlp.csv` and `feature_tm.csv` from previous work to leaky feature in Siamese_LSTM baseline.\n",
    "- Change: adding around 40 more features to leaky features, and doubled N_DENSE in Dense layer. (from N_DENSE/2 to N_DENSE).\n",
    "\n",
    "    \n",
    "- **Output:**\n",
    "  - `lstm_featured.csv`: 4 epoch, score: 0.16515\n",
    "    - 1st epoch: loss: 0.2489 - acc: 0.8280 - val_loss: 0.1927 - val_acc: 0.8630\n",
    "    - last epoch: loss: 0.1790 - acc: 0.8714 - val_loss: 0.1700 - val_acc: 0.8723"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from string import punctuation\n",
    "from collections import defaultdict\n",
    "# from tqdm import tqdm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "# Keras package\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, Embedding, Dropout, Activation, LSTM, Lambda\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.layers.pooling import GlobalAveragePooling1D\n",
    "import keras.backend as K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM Stucture:\n",
      "Num_Lstm: 250\n",
      "Num_Dense: 120\n",
      "Dropout rate in LSTM layer: 0.33\n",
      "Dropout rate in Dense layer:: 0.33\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter defination\n",
    "\n",
    "# Use the following instructions to download glove and unzip it, if already installed, just comment them.\n",
    "# !wget http://nlp.stanford.edu/data/glove.840B.300d.zip\n",
    "# !unzip glove.840B.300d.zip\n",
    "EMBEDDING_FILE = 'glove.840B.300d.txt'\n",
    "\n",
    "TRAIN_DATA_FILE = 'Data/train.csv'\n",
    "TEST_DATA_FILE = 'Data/test.csv'\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 60  \n",
    "MAX_NUM_WORDS = 200000  # There are about 201000 unique words in training dataset, 200000 is enough for tokenization\n",
    "EMBEDDING_DIM = 300  # word-embedded-vector dimension(300 is for 'glove.840B.300d')\n",
    "VALIDATION_SPLIT_RATE = 0.1 \n",
    "N_HIDDEN = 250\n",
    "N_DENSE = 120\n",
    "DROPOUT_RATE_LSTM = 0.33  # drop-out possibility, random set to avoid outfitting  # 0.20\n",
    "DROUPOUT_RATE_DENSE = 0.33  \n",
    "\n",
    "VERSION = 'Temp/lstm_featured'\n",
    "print('LSTM Stucture:')\n",
    "print('Num_Lstm:', N_HIDDEN)\n",
    "print('Num_Dense:', N_DENSE)\n",
    "print('Dropout rate in LSTM layer:', DROPOUT_RATE_LSTM) \n",
    "print('Dropout rate in Dense layer::', DROUPOUT_RATE_DENSE)\n",
    "\n",
    "ACTIVE_FUNC = 'relu'\n",
    "re_weight = True  # whether to re-weight classes to fit the 17.4% share in test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create word embedding dictionary\n",
      "Found 2195892 word vectors of glove.\n",
      "Processing text dataset\n",
      "404290 texts are found in train.csv\n",
      "2345796 texts are found in test.csv\n"
     ]
    }
   ],
   "source": [
    "# Create word embedding dictionary from 'glove.840B.300d.txt', {key:value} is {word: glove vector(300,)}\n",
    "print('Create word embedding dictionary')\n",
    "\n",
    "embeddings_index = {}  # the output dictionary\n",
    "f = open(EMBEDDING_FILE, encoding='utf-8')\n",
    "for line in f:  # tqdm\n",
    "    values = line.split()\n",
    "    word = ''.join(values[:-300])   \n",
    "    coefs = np.asarray(values[-300:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found {} word vectors of glove.'.format(len(embeddings_index)))\n",
    "\n",
    "\n",
    "# Preprocess text in dataset\n",
    "print('Processing text dataset')\n",
    "\n",
    "def text_to_wordlist(text, remove_stopwords=False, stem_words=False):\n",
    "    # Clean the text, with the option to remove stopwords and to stem words.\n",
    "    \n",
    "    # Convert words to lower case and split them\n",
    "    text = text.lower().split()\n",
    "\n",
    "    # Optionally, remove stop words\n",
    "    if remove_stopwords:\n",
    "        stop_words = set(stopwords.words(\"english\"))\n",
    "        text = [w for w in text if not w in stop_words]\n",
    "    \n",
    "    text = \" \".join(text)\n",
    "\n",
    "    # Use re to clean the text\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    # text = re.sub(r\"\\0s\", \"0\", text) # It doesn't make sense to me\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    \n",
    "    # Optionally, shorten words to their stems\n",
    "    if stem_words:\n",
    "        text = text.split()\n",
    "        stemmer = SnowballStemmer('english')\n",
    "        stemmed_words = [stemmer.stem(word) for word in text]\n",
    "        text = \" \".join(stemmed_words)\n",
    "    # or nltk package\n",
    "    # if lemma:\n",
    "    #    text = text.split()\n",
    "    #    wn = nltk.WordNetLemmatizer()\n",
    "    #    lemm_words = [wn.lemmatize(word) for word in text]\n",
    "    #    text = \" \".join(lemm_words)\n",
    "    \n",
    "    # Return a list of words\n",
    "    return(text)\n",
    "\n",
    "# Load training data and process with text_to_wordlist (Preprocessing)\n",
    "train_texts_1 = []  # the preprocessed text of q1\n",
    "train_texts_2 = []  # the preprocessed text of q2\n",
    "train_labels = []  # training labels\n",
    "\n",
    "df_train = pd.read_csv(TRAIN_DATA_FILE, encoding='utf-8')  # the original training data\n",
    "df_train = df_train.fillna('empty')\n",
    "train_q1 = df_train.question1.values  # the original text of q1\n",
    "train_q2 = df_train.question2.values  # the original text fo q2\n",
    "train_labels = df_train.is_duplicate.values  # the original label('is_duplicate')\n",
    "\n",
    "for text in train_q1:\n",
    "    train_texts_1.append(text_to_wordlist(text, remove_stopwords=False, stem_words=False))\n",
    "    \n",
    "for text in train_q2:\n",
    "    train_texts_2.append(text_to_wordlist(text, remove_stopwords=False, stem_words=False))\n",
    "\n",
    "print('{} texts are found in train.csv'.format(len(train_texts_1)))\n",
    "\n",
    "# Load testing data and process with text_to_wordlist (Preprocessing)\n",
    "test_texts_1 = []  # the preprocessed text of q1_test\n",
    "test_texts_2 = []  # the preprocessed text of q2_test\n",
    "test_ids = []  # id..\n",
    "\n",
    "df_test = pd.read_csv(TEST_DATA_FILE, encoding='utf-8')  # the original testing data\n",
    "df_test = df_test.fillna('empty')\n",
    "test_q1 = df_test.question1.values  # the original text of q1_test\n",
    "test_q2 = df_test.question2.values  # the original text of q2_test\n",
    "test_ids = df_test.test_id.values  # id..\n",
    "\n",
    "for text in test_q1:\n",
    "    test_texts_1.append(text_to_wordlist(text, remove_stopwords=False, stem_words=False))\n",
    "    \n",
    "for text in test_q2:\n",
    "    test_texts_2.append(text_to_wordlist(text, remove_stopwords=False, stem_words=False))\n",
    "    \n",
    "print('{} texts are found in test.csv'.format(len(test_texts_1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120499 unique tokens are found\n",
      "Shape of train data tensor: (404290, 60)\n",
      "Shape of train labels tensor: (404290,)\n",
      "Shape of test data vtensor: (2345796, 60)\n",
      "Shape of test ids tensor: (2345796,)\n"
     ]
    }
   ],
   "source": [
    "# Keras.Tokenize for all text:\n",
    "# First construct a Tokenizer()\n",
    "# Then use tokenizer_on_texts() method to learn the dictionary of the corpus(all texts(sentences)). We can use .word_index to map between the each word (distinct) with the corresponding number.\n",
    "# Then use text_to_sequence() method to transfer every text(sentence) in texts into sequences of word_indexes.\n",
    "# Then add the same length by padding method: padding_sequences().\n",
    "# Finally use the embedding layer in keras to carry out a vectorization, and input it into LSTM.\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(train_texts_1 + train_texts_2 + test_texts_1 + test_texts_2)  # generate a token dictionary, \n",
    "\n",
    "train_sequences_1 = tokenizer.texts_to_sequences(train_texts_1)  # sequence of q1\n",
    "train_sequences_2 = tokenizer.texts_to_sequences(train_texts_2)  # sequence of q2\n",
    "test_sequences_1 = tokenizer.texts_to_sequences(test_texts_1)  # sequence of q1_test\n",
    "test_sequences_2 = tokenizer.texts_to_sequences(test_texts_2)  # sequence of q2_test\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('{} unique tokens are found'.format(len(word_index)))\n",
    "\n",
    "# Pad all train with Max_Sequence_Length: 60\n",
    "train_data_1 = pad_sequences(train_sequences_1, maxlen=MAX_SEQUENCE_LENGTH)  # padded_sequence of q1 as train_data\n",
    "train_data_2 = pad_sequences(train_sequences_2, maxlen=MAX_SEQUENCE_LENGTH)  # padded_sequence of q2 as train_data\n",
    "print('Shape of train data tensor:', train_data_1.shape)\n",
    "print('Shape of train labels tensor:', train_labels.shape)\n",
    "\n",
    "# Pad all test with Max_Sequence_Length\n",
    "test_data_1 = pad_sequences(test_sequences_1, maxlen=MAX_SEQUENCE_LENGTH)  # padded_sequence of q1_test as test_data\n",
    "test_data_2 = pad_sequences(test_sequences_2, maxlen=MAX_SEQUENCE_LENGTH)  # padded_sequence of q2_test as test_data\n",
    "print('Shape of test data vtensor:', test_data_2.shape)\n",
    "print('Shape of test ids tensor:', test_ids.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic features created\n",
      "Features(nlp&tm) Loaded\n",
      "Final features created\n"
     ]
    }
   ],
   "source": [
    "questions = pd.concat([df_train[['question1', 'question2']], df_test[['question1', 'question2']]], axis=0).reset_index(drop='index')\n",
    "q_dict = defaultdict(set)\n",
    "for i in range(questions.shape[0]):\n",
    "        q_dict[questions.question1[i]].add(questions.question2[i])\n",
    "        q_dict[questions.question2[i]].add(questions.question1[i])\n",
    "\n",
    "def q1_freq_train(row):\n",
    "    return len(q_dict.get(row[3]))\n",
    "    # return(len(q_dict[row['question1']]))\n",
    "\n",
    "def q2_freq_train(row):\n",
    "    return len(q_dict.get(row[4]))\n",
    "    # return(len(q_dict[row['question2']]))\n",
    "\n",
    "def q1_q2_intersect_train(row):\n",
    "    return(len(set(q_dict.get(row[3])).intersection(set(q_dict.get(row[4])))))\n",
    "    # return(len(set(q_dict[row['question1']]).intersection(set(q_dict[row['question2']]))))\n",
    "\n",
    "def q1_freq_test(row):\n",
    "    return len(q_dict.get(row[1]))\n",
    "    # return(len(q_dict[row['question1']]))\n",
    "\n",
    "def q2_freq_test(row):\n",
    "    return len(q_dict.get(row[2]))\n",
    "    # return(len(q_dict[row['question2']]))\n",
    "\n",
    "def q1_q2_intersect_test(row):\n",
    "    return(len(set(q_dict.get(row[1])).intersection(set(q_dict.get(row[2])))))\n",
    "    # return(len(set(q_dict[row['question1']]).intersection(set(q_dict[row['question2']]))))\n",
    "\n",
    "df_train['q1_q2_intersect'] = df_train.apply(q1_q2_intersect_train, axis=1, raw=True)\n",
    "df_train['q1_freq'] = df_train.apply(q1_freq_train, axis=1, raw=True)\n",
    "df_train['q2_freq'] = df_train.apply(q2_freq_train, axis=1, raw=True)\n",
    "\n",
    "df_test['q1_q2_intersect'] = df_test.apply(q1_q2_intersect_test, axis=1, raw=True)\n",
    "df_test['q1_freq'] = df_test.apply(q1_freq_test, axis=1, raw=True)\n",
    "df_test['q2_freq'] = df_test.apply(q2_freq_test, axis=1, raw=True)\n",
    "print('Basic features created')\n",
    "\n",
    "# Load features from previous work on feature_tm and feature_nlp\n",
    "import pandas as pd\n",
    "\n",
    "df_feature = pd.read_csv('Features/feature_tm+nlp.csv')\n",
    "df_feature = df_feature.fillna(0)\n",
    "df_feature_test = pd.read_csv('Features/feature_tm+nlp_test.csv')\n",
    "print('Features(nlp&tm) Loaded')\n",
    "\n",
    "df_train['id'] = df_feature['id']\n",
    "df_test['test_id'] = df_feature_test['test_id']\n",
    "\n",
    "df_train = df_train.merge(df_feature,on='id',how='left')\n",
    "leak = df_train.drop(['id','qid1','qid2','question1','question2','is_duplicate'], axis=1)\n",
    "df_test = df_test.merge(df_feature_test,on='test_id',how='left')\n",
    "test_leak = df_test.drop(['test_id','question1','question2'], axis=1)\n",
    "print('Final features created')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make scaling for leaky feature (normalization)\n",
    "ss = StandardScaler()\n",
    "ss.fit(np.vstack((leak, test_leak)))\n",
    "leaks = ss.transform(leak)  # the leaky feature, array\n",
    "test_leaks = ss.transform(test_leak)  # the leaky feature_test, array\n",
    "print('Normalization finished')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.11478188, -0.05687322, -0.04436891, ..., -1.67102839,\n",
       "        -1.4782846 , -1.4782846 ],\n",
       "       [-0.11478188,  0.03837623, -0.03081647, ..., -0.20451842,\n",
       "        -0.18610259, -0.18610259],\n",
       "       [-0.11478188, -0.04326616, -0.05792136, ..., -0.7200645 ,\n",
       "        -0.5830698 , -0.5830698 ],\n",
       "       ...,\n",
       "       [-0.11478188, -0.05687322, -0.05792136, ..., -0.83913556,\n",
       "         0.44826012,  0.44826012],\n",
       "       [-0.11478188, -0.05687322, -0.05792136, ...,  1.54412763,\n",
       "         0.72736336,  0.72736336],\n",
       "       [-0.11478188, -0.05687322, -0.05792136, ..., -3.97671095,\n",
       "        -2.33599635, -2.33599635]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train & Validation split\n",
    "perm = np.random.permutation(len(train_data_1))\n",
    "idx_train = perm[:int(len(train_data_1)*(1-VALIDATION_SPLIT_RATE))]\n",
    "idx_val = perm[int(len(train_data_1)*(1-VALIDATION_SPLIT_RATE)):]\n",
    "\n",
    "data_1_train = np.vstack((train_data_1[idx_train], train_data_2[idx_train]))\n",
    "data_2_train = np.vstack((train_data_2[idx_train], train_data_1[idx_train]))\n",
    "leaks_train = np.vstack((leaks[idx_train], leaks[idx_train]))\n",
    "labels_train = np.concatenate((train_labels[idx_train], train_labels[idx_train]))\n",
    "\n",
    "data_1_val = np.vstack((train_data_1[idx_val], train_data_2[idx_val]))\n",
    "data_2_val = np.vstack((train_data_2[idx_val], train_data_1[idx_val]))\n",
    "leaks_val = np.vstack((leaks[idx_val], leaks[idx_val]))\n",
    "labels_val = np.concatenate((train_labels[idx_val], train_labels[idx_val]))\n",
    "\n",
    "weight_val = np.ones(len(labels_val))\n",
    "if re_weight:\n",
    "    weight_val *= 0.471544715\n",
    "    weight_val[labels_val==0] = 1.309033281\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here i change the N_DENSE in dense layer from N_DENSE to N_DENSE/2, obviously the results are worse.\n",
    "- give up if improve too little (<0.00x) or worse\n",
    "- Scores: 0.17540, so **give it up**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing embedding matrix\n",
      "Null word embeddings: \n",
      "Starting the model training\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 60)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 60)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 60, 300)      36150000    input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 44)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 250)          551000      embedding_1[0][0]                \n",
      "                                                                 embedding_1[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 60)           2700        input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 560)          0           lstm_1[0][0]                     \n",
      "                                                                 lstm_1[1][0]                     \n",
      "                                                                 dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 560)          2240        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 560)          0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 120)          67320       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 120)          480         dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 120)          0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1)            121         dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 36,773,861\n",
      "Trainable params: 622,501\n",
      "Non-trainable params: 36,151,360\n",
      "__________________________________________________________________________________________________\n",
      "Train on 727722 samples, validate on 80858 samples\n",
      "Epoch 1/4\n",
      "727722/727722 [==============================] - 2510s 3ms/step - loss: 0.2619 - acc: 0.8204 - val_loss: 0.2051 - val_acc: 0.8370\n",
      "Epoch 2/4\n",
      "727722/727722 [==============================] - 15280s 21ms/step - loss: 0.2004 - acc: 0.8543 - val_loss: 0.1897 - val_acc: 0.8583\n",
      "Epoch 3/4\n",
      "727722/727722 [==============================] - 2463s 3ms/step - loss: 0.1895 - acc: 0.8630 - val_loss: 0.1836 - val_acc: 0.8577\n",
      "Epoch 4/4\n",
      "727722/727722 [==============================] - 2570s 4ms/step - loss: 0.1820 - acc: 0.8692 - val_loss: 0.1771 - val_acc: 0.8797\n"
     ]
    }
   ],
   "source": [
    "# Reset the hyperparameter\n",
    "VERSION = 'Temp/lstm_featured_2'\n",
    "\n",
    "# Create embedding matrix for embedding layer, which is used in the keras.embedding weight as the initializer.\n",
    "print('Preparing embedding matrix')\n",
    "\n",
    "num_words = min(MAX_NUM_WORDS, len(word_index))+1\n",
    "\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))  # the weight of Embedding layer\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "print('Null word embeddings: '.format(np.sum(np.sum(embedding_matrix, axis=1) == 0)))\n",
    "\n",
    "\n",
    "# NN Model design\n",
    "# Structure: (q1-embedding-lstm + q2-embedding-lstm + leaky-dense)-dense-sigmoid-result\n",
    "\n",
    "# The embedding layer containing the word vectors\n",
    "emb_layer = Embedding(\n",
    "    input_dim=num_words,\n",
    "    output_dim=EMBEDDING_DIM,\n",
    "    weights=[embedding_matrix],\n",
    "    input_length=MAX_SEQUENCE_LENGTH,\n",
    "    trainable=False\n",
    ")    \n",
    "\n",
    "# LSTM layer\n",
    "lstm_layer = LSTM(N_HIDDEN, dropout=DROPOUT_RATE_LSTM, recurrent_dropout=DROPOUT_RATE_LSTM)\n",
    "# try bidirectional?\n",
    "# LSTM_ = Bidirectional(LSTM(1024, kernel_regularizer=keras.regularizers.l2(0.2),return_sequences=False))\n",
    "\n",
    "# Define inputs\n",
    "seq1 = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "seq2 = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "\n",
    "# Run inputs through embedding\n",
    "emb1 = emb_layer(seq1)\n",
    "emb2 = emb_layer(seq2)\n",
    "\n",
    "# Run through LSTM layers\n",
    "lstm_a = lstm_layer(emb1)\n",
    "# glob1a = GlobalAveragePooling1D()(lstm_a)\n",
    "lstm_b = lstm_layer(emb2)\n",
    "# glob1b = GlobalAveragePooling1D()(lstm_b)\n",
    "\n",
    "magic_input = Input(shape=(leaks.shape[1],))\n",
    "# magic_dense = BatchNormalization()(magic_input)\n",
    "magic_dense = Dense(int(N_DENSE/2), activation=ACTIVE_FUNC)(magic_input)  # change N_DENSE/2 to N_DENSE because of the addition of leaky features\n",
    "\n",
    "merged = concatenate([lstm_a, lstm_b, magic_dense])\n",
    "merged = BatchNormalization()(merged)\n",
    "merged = Dropout(DROUPOUT_RATE_DENSE)(merged)\n",
    "\n",
    "merged = Dense(N_DENSE, activation=ACTIVE_FUNC)(merged)\n",
    "merged = BatchNormalization()(merged)\n",
    "merged = Dropout(DROUPOUT_RATE_DENSE)(merged)\n",
    "\n",
    "preds = Dense(1, activation='sigmoid')(merged)\n",
    "\n",
    "\n",
    "# Add class weight, magic feature for the unbalancement of training labels.\n",
    "if re_weight:\n",
    "    class_weight = {0: 1.309033281, 1: 0.471544715}\n",
    "else:\n",
    "    class_weight = None\n",
    "    \n",
    "\n",
    "# Train the model\n",
    "\n",
    "print('Starting the model training')\n",
    "\n",
    "model = Model(inputs=[seq1, seq2, magic_input], outputs=preds)\n",
    "model.compile(loss='binary_crossentropy', optimizer='nadam', metrics=['acc'])\n",
    "\n",
    "# Summerization of model\n",
    "model.summary()\n",
    "\n",
    "# Set early stopping (large patience should be useful)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
    "bst_model_path = VERSION + '.h5' \n",
    "model_checkpoint = ModelCheckpoint(bst_model_path, save_best_only=True, save_weights_only=True)\n",
    "\n",
    "\n",
    "hist = model.fit([data_1_train, data_2_train, leaks_train], labels_train, \\\n",
    "        validation_data=([data_1_val, data_2_val, leaks_val], labels_val, weight_val), \\\n",
    "        epochs=4, batch_size=2048, shuffle=True, \\\n",
    "        class_weight=class_weight, callbacks=[early_stopping, model_checkpoint])\n",
    "\n",
    "model.load_weights(bst_model_path) # sotre model parameters in .h5 file\n",
    "bst_val_score = min(hist.history['val_loss'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making the submission\n",
      "2345796/2345796 [==============================] - 2747s 1ms/step\n",
      "2345796/2345796 [==============================] - 2679s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "# Make the submission\n",
    "\n",
    "print('Making the submission')\n",
    "\n",
    "preds = model.predict([test_data_1, test_data_2, test_leaks], batch_size=8192, verbose=1)\n",
    "preds += model.predict([test_data_2, test_data_1, test_leaks], batch_size=8192, verbose=1)\n",
    "preds /= 2\n",
    "\n",
    "submission = pd.DataFrame({'test_id':test_ids, 'is_duplicate':preds.ravel()})\n",
    "submission.to_csv('Models/lstm_featured_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here i add one more dense layer\n",
    "- just look at the first epoch, give up if improve too little (<0.00x) or worse\n",
    "- in magic dense, scores: 0.17125, not better, **give it up**.\n",
    "- in merge dense, worse, **give it up**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset one hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing embedding matrix\n",
      "Null word embeddings: \n",
      "Start modeling\n",
      "Starting the model training\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 60)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 60)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 44)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 60, 300)      36150000    input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 120)          5400        input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 250)          551000      embedding_1[0][0]                \n",
      "                                                                 embedding_1[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 60)           7260        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 560)          0           lstm_1[0][0]                     \n",
      "                                                                 lstm_1[1][0]                     \n",
      "                                                                 dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 560)          2240        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 560)          0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 120)          67320       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 120)          480         dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 120)          0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 1)            121         dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 36,783,821\n",
      "Trainable params: 632,461\n",
      "Non-trainable params: 36,151,360\n",
      "__________________________________________________________________________________________________\n",
      "Train on 727722 samples, validate on 80858 samples\n",
      "Epoch 1/4\n",
      "727722/727722 [==============================] - 2551s 4ms/step - loss: 0.2438 - acc: 0.8331 - val_loss: 0.2176 - val_acc: 0.8739\n",
      "Epoch 2/4\n",
      "727722/727722 [==============================] - 2523s 3ms/step - loss: 0.1927 - acc: 0.8616 - val_loss: 0.1823 - val_acc: 0.8692\n",
      "Epoch 3/4\n",
      "727722/727722 [==============================] - 2511s 3ms/step - loss: 0.1836 - acc: 0.8684 - val_loss: 0.1902 - val_acc: 0.8868\n",
      "Epoch 4/4\n",
      "727722/727722 [==============================] - 2509s 3ms/step - loss: 0.1766 - acc: 0.8733 - val_loss: 0.2316 - val_acc: 0.8853\n"
     ]
    }
   ],
   "source": [
    "# Reset the hyperparameter\n",
    "VERSION = 'Temp/lstm_featured_3'\n",
    "\n",
    "# Create embedding matrix for embedding layer, which is used in the keras.embedding weight as the initializer.\n",
    "print('Preparing embedding matrix')\n",
    "\n",
    "num_words = min(MAX_NUM_WORDS, len(word_index))+1\n",
    "\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))  # the weight of Embedding layer\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "print('Null word embeddings: '.format(np.sum(np.sum(embedding_matrix, axis=1) == 0)))\n",
    "print('Start modeling')\n",
    "\n",
    "\n",
    "# NN Model design\n",
    "# Structure: (q1-embedding-lstm + q2-embedding-lstm + leaky-dense)-dense-sigmoid-result\n",
    "\n",
    "# The embedding layer containing the word vectors\n",
    "emb_layer = Embedding(\n",
    "    input_dim=num_words,\n",
    "    output_dim=EMBEDDING_DIM,\n",
    "    weights=[embedding_matrix],\n",
    "    input_length=MAX_SEQUENCE_LENGTH,\n",
    "    trainable=False\n",
    ")    \n",
    "\n",
    "# LSTM layer\n",
    "lstm_layer = LSTM(N_HIDDEN, dropout=DROPOUT_RATE_LSTM, recurrent_dropout=DROPOUT_RATE_LSTM)\n",
    "# try bidirectional?\n",
    "# LSTM_ = Bidirectional(LSTM(1024, kernel_regularizer=keras.regularizers.l2(0.2),return_sequences=False))\n",
    "\n",
    "# Define inputs\n",
    "seq1 = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "seq2 = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "\n",
    "# Run inputs through embedding\n",
    "emb1 = emb_layer(seq1)\n",
    "emb2 = emb_layer(seq2)\n",
    "\n",
    "# Run through LSTM layers\n",
    "lstm_a = lstm_layer(emb1)\n",
    "# glob1a = GlobalAveragePooling1D()(lstm_a)\n",
    "lstm_b = lstm_layer(emb2)\n",
    "# glob1b = GlobalAveragePooling1D()(lstm_b)\n",
    "\n",
    "magic_input = Input(shape=(leaks.shape[1],))\n",
    "# magic_dense = BatchNormalization()(magic_input)\n",
    "magic_dense = Dense(int(N_DENSE), activation=ACTIVE_FUNC)(magic_input)  \n",
    "magic_dense = Dense(int(N_DENSE/2), activation=ACTIVE_FUNC)(magic_dense)  # change\n",
    "\n",
    "merged = concatenate([lstm_a, lstm_b, magic_dense])\n",
    "merged = BatchNormalization()(merged)\n",
    "merged = Dropout(DROUPOUT_RATE_DENSE)(merged)\n",
    "\n",
    "merged = Dense(N_DENSE, activation=ACTIVE_FUNC)(merged)\n",
    "merged = BatchNormalization()(merged)\n",
    "merged = Dropout(DROUPOUT_RATE_DENSE)(merged)\n",
    "\n",
    "preds = Dense(1, activation='sigmoid')(merged)\n",
    "\n",
    "\n",
    "# Add class weight, magic feature for the unbalancement of training labels.\n",
    "if re_weight:\n",
    "    class_weight = {0: 1.309033281, 1: 0.471544715}\n",
    "else:\n",
    "    class_weight = None\n",
    "    \n",
    "\n",
    "# Train the model\n",
    "\n",
    "print('Starting the model training')\n",
    "\n",
    "model = Model(inputs=[seq1, seq2, magic_input], outputs=preds)\n",
    "model.compile(loss='binary_crossentropy', optimizer='nadam', metrics=['acc'])\n",
    "\n",
    "# Summerization of model\n",
    "model.summary()\n",
    "\n",
    "# Set early stopping (large patience should be useful)\n",
    "early_stopping =EarlyStopping(monitor='val_loss', patience=10)\n",
    "bst_model_path = VERSION + '.h5' \n",
    "model_checkpoint = ModelCheckpoint(bst_model_path, save_best_only=True, save_weights_only=True)\n",
    "\n",
    "\n",
    "hist = model.fit([data_1_train, data_2_train, leaks_train], labels_train, \\\n",
    "        validation_data=([data_1_val, data_2_val, leaks_val], labels_val, weight_val), \\\n",
    "        epochs=4, batch_size=2048, shuffle=True, \\\n",
    "        class_weight=class_weight, callbacks=[early_stopping, model_checkpoint])\n",
    "\n",
    "model.load_weights(bst_model_path) # sotre model parameters in .h5 file\n",
    "bst_val_score = min(hist.history['val_loss'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making the submission\n",
      "2345796/2345796 [==============================] - 2756s 1ms/step\n",
      "2345796/2345796 [==============================] - 4978s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "# Make the submission\n",
    "\n",
    "print('Making the submission')\n",
    "\n",
    "preds = model.predict([test_data_1, test_data_2, test_leaks], batch_size=8192, verbose=1)\n",
    "preds += model.predict([test_data_2, test_data_1, test_leaks], batch_size=8192, verbose=1)\n",
    "preds /= 2\n",
    "\n",
    "submission = pd.DataFrame({'test_id':test_ids, 'is_duplicate':preds.ravel()})\n",
    "submission.to_csv('Models/lstm_featured_3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing embedding matrix\n",
      "Null word embeddings: \n",
      "Start modeling\n",
      "Start modeling\n",
      "Starting the model training\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 60)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 60)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 60, 300)      36150000    input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 44)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 250)          551000      embedding_1[0][0]                \n",
      "                                                                 embedding_1[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 60)           2700        input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 560)          0           lstm_1[0][0]                     \n",
      "                                                                 lstm_1[1][0]                     \n",
      "                                                                 dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 560)          2240        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 560)          0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 120)          67320       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 120)          480         dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 120)          0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1)            121         dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 36,773,861\n",
      "Trainable params: 622,501\n",
      "Non-trainable params: 36,151,360\n",
      "__________________________________________________________________________________________________\n",
      "Train on 727722 samples, validate on 80858 samples\n",
      "Epoch 1/4\n",
      "727722/727722 [==============================] - 2478s 3ms/step - loss: 0.2631 - acc: 0.8199 - val_loss: 0.1970 - val_acc: 0.8339\n",
      "Epoch 2/4\n",
      "727722/727722 [==============================] - 2651s 4ms/step - loss: 0.1993 - acc: 0.8551 - val_loss: 0.1799 - val_acc: 0.8660\n",
      "Epoch 3/4\n",
      "727722/727722 [==============================] - 2580s 4ms/step - loss: 0.1880 - acc: 0.8638 - val_loss: 0.1762 - val_acc: 0.8741\n",
      "Epoch 4/4\n",
      "727722/727722 [==============================] - 2516s 3ms/step - loss: 0.1810 - acc: 0.8694 - val_loss: 0.1729 - val_acc: 0.8773\n"
     ]
    }
   ],
   "source": [
    "# Reset the hyperparameter\n",
    "VERSION = 'Temp/lstm_featured_3_1'\n",
    "# Create embedding matrix for embedding layer, which is used in the keras.embedding weight as the initializer.\n",
    "print('Preparing embedding matrix')\n",
    "\n",
    "num_words = min(MAX_NUM_WORDS, len(word_index))+1\n",
    "\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))  # the weight of Embedding layer\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "print('Null word embeddings: '.format(np.sum(np.sum(embedding_matrix, axis=1) == 0)))\n",
    "print('Start modeling')\n",
    "print('Start modeling')\n",
    "# NN Model design\n",
    "# Structure: (q1-embedding-lstm + q2-embedding-lstm + leaky-dense)-dense-sigmoid-result\n",
    "\n",
    "# The embedding layer containing the word vectors\n",
    "emb_layer = Embedding(\n",
    "    input_dim=num_words,\n",
    "    output_dim=EMBEDDING_DIM,\n",
    "    weights=[embedding_matrix],\n",
    "    input_length=MAX_SEQUENCE_LENGTH,\n",
    "    trainable=False\n",
    ")    \n",
    "\n",
    "# LSTM layer\n",
    "lstm_layer = LSTM(N_HIDDEN, dropout=DROPOUT_RATE_LSTM, recurrent_dropout=DROPOUT_RATE_LSTM)\n",
    "# try bidirectional?\n",
    "# LSTM_ = Bidirectional(LSTM(1024, kernel_regularizer=keras.regularizers.l2(0.2),return_sequences=False))\n",
    "\n",
    "# Define inputs\n",
    "seq1 = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "seq2 = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "\n",
    "# Run inputs through embedding\n",
    "emb1 = emb_layer(seq1)\n",
    "emb2 = emb_layer(seq2)\n",
    "\n",
    "# Run through LSTM layers\n",
    "lstm_a = lstm_layer(emb1)\n",
    "# glob1a = GlobalAveragePooling1D()(lstm_a)\n",
    "lstm_b = lstm_layer(emb2)\n",
    "# glob1b = GlobalAveragePooling1D()(lstm_b)\n",
    "\n",
    "magic_input = Input(shape=(leaks.shape[1],))\n",
    "# magic_dense = BatchNormalization()(magic_input)\n",
    "magic_dense = Dense(int(N_DENSE/2), activation=ACTIVE_FUNC)(magic_input)  \n",
    "\n",
    "\n",
    "merged = concatenate([lstm_a, lstm_b, magic_dense])\n",
    "merged = BatchNormalization()(merged)\n",
    "merged = Dropout(DROUPOUT_RATE_DENSE)(merged)\n",
    "\n",
    "merged = Dense(N_DENSE, activation=ACTIVE_FUNC)(merged)\n",
    "merged = BatchNormalization()(merged)\n",
    "merged = Dropout(DROUPOUT_RATE_DENSE)(merged)\n",
    "\n",
    "preds = Dense(1, activation='sigmoid')(merged)\n",
    "\n",
    "\n",
    "# Add class weight, magic feature for the unbalancement of training labels.\n",
    "if re_weight:\n",
    "    class_weight = {0: 1.309033281, 1: 0.471544715}\n",
    "else:\n",
    "    class_weight = None\n",
    "    \n",
    "\n",
    "# Train the model\n",
    "\n",
    "print('Starting the model training')\n",
    "\n",
    "model = Model(inputs=[seq1, seq2, magic_input], outputs=preds)\n",
    "model.compile(loss='binary_crossentropy', optimizer='nadam', metrics=['acc'])\n",
    "\n",
    "# Summerization of model\n",
    "model.summary()\n",
    "\n",
    "# Set early stopping (large patience should be useful)\n",
    "early_stopping =EarlyStopping(monitor='val_loss', patience=10)\n",
    "bst_model_path ='lstm_featured_4.h5' \n",
    "model_checkpoint = ModelCheckpoint(bst_model_path, save_best_only=True, save_weights_only=True)\n",
    "\n",
    "\n",
    "hist = model.fit([data_1_train, data_2_train, leaks_train], labels_train, \\\n",
    "        validation_data=([data_1_val, data_2_val, leaks_val], labels_val, weight_val), \\\n",
    "        epochs=4, batch_size=2048, shuffle=True, \\\n",
    "        class_weight=class_weight, callbacks=[early_stopping, model_checkpoint])\n",
    "\n",
    "model.load_weights(bst_model_path) # sotre model parameters in .h5 file\n",
    "bst_val_score = min(hist.history['val_loss'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making the submission\n",
      "2345796/2345796 [==============================] - 2819s 1ms/step\n",
      "2345796/2345796 [==============================] - 2858s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "# Make the submission\n",
    "\n",
    "print('Making the submission')\n",
    "\n",
    "preds = model.predict([test_data_1, test_data_2, test_leaks], batch_size=8192, verbose=1)\n",
    "preds += model.predict([test_data_2, test_data_1, test_leaks], batch_size=8192, verbose=1)\n",
    "preds /= 2\n",
    "\n",
    "submission = pd.DataFrame({'test_id':test_ids, 'is_duplicate':preds.ravel()})\n",
    "submission.to_csv('Models/lstm_featured_3_1.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here i change some hyperparameters\n",
    "- just look at the first epoch\n",
    "- n-hidden\n",
    "- dense_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6.1559826e-03, 1.3150275e-04, 5.4044873e-03, 2.3005173e-01])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('Models/lstm_featured_1.csv')\n",
    "df = df.is_duplicate[-4:]\n",
    "df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wen Ze's part, he will do these experiments based on lstm.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
