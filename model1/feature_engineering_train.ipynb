{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering for Train_data of  `Quora Question Pairs`\n",
    "\n",
    "- This is the **2nd iteration**, the change is focused on the feature_tm and feature_nlp\n",
    "- Feature engineering of training data.\n",
    "- Extracting features according to the order of csv file below.\n",
    "-  ****Due to limited computing resources of my laptop, i couldn't merge features from en_core_web_md (which is 300 dimensions), so in the `modeling.ipynb` i used features from en_core_web_sm (have tried before and saved as features locally). so if you want to rebuild my project results, you should change the model to `en_core_web_sm` in `1.4`(vector features) . ****\n",
    "\n",
    "\n",
    "- Input: `train.csv`\n",
    "- Output: `feature_tm.csv`, `feature_nlp.csv`, `feature_vectors.csv`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from subprocess import check_output\n",
    "%matplotlib inline\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "import os\n",
    "import gc\n",
    "\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import distance\n",
    "from nltk.stem import PorterStemmer\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import distance\n",
    "from nltk.stem import PorterStemmer\n",
    "from bs4 import BeautifulSoup\n",
    "from fuzzywuzzy import fuzz\n",
    "from sklearn.manifold import TSNE\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from os import path\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>Which fish would survive in salt water?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  qid1  qid2                                          question1  \\\n",
       "0   0     1     2  What is the step by step guide to invest in sh...   \n",
       "1   1     3     4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2   2     5     6  How can I increase the speed of my internet co...   \n",
       "3   3     7     8  Why am I mentally very lonely? How can I solve...   \n",
       "4   4     9    10  Which one dissolve in water quikly sugar, salt...   \n",
       "\n",
       "                                           question2  is_duplicate  \n",
       "0  What is the step by step guide to invest in sh...             0  \n",
       "1  What would happen if the Indian government sto...             0  \n",
       "2  How can Internet speed be increased by hacking...             0  \n",
       "3  Find the remainder when [math]23^{24}[/math] i...             0  \n",
       "4            Which fish would survive in salt water?             0  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load train data from csv file\n",
    "\n",
    "df = pd.read_csv(\"Data/train.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 404290 entries, 0 to 404289\n",
      "Data columns (total 6 columns):\n",
      "id              404290 non-null int64\n",
      "qid1            404290 non-null int64\n",
      "qid2            404290 non-null int64\n",
      "question1       404289 non-null object\n",
      "question2       404288 non-null object\n",
      "is_duplicate    404290 non-null int64\n",
      "dtypes: int64(4), object(2)\n",
      "memory usage: 18.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are  63.08 % duplicated question pairs in the training dataset\n",
      "There are  36.92 % question pairs that are not duplicated in the training dataset\n",
      "The number of duplicate question pairs is: 0\n",
      "There are: 537933  unique questions.\n",
      "There are: 111780  question that appear more than once.\n"
     ]
    }
   ],
   "source": [
    "# Basic statistical analysis\n",
    "\n",
    "# The proportion of duplicated question-pairs\n",
    "print(\"There are \", round(df['is_duplicate'].mean()*100, 2),\"% duplicated question pairs in the training dataset\")\n",
    "print(\"There are \", 100 - round(df['is_duplicate'].mean()*100, 2),\"% question pairs that are not duplicated in the training dataset\")\n",
    "\n",
    "# Check the dupcated question-pairs (appears more than once)\n",
    "question_pair_duplicates = df[['qid1','qid2','is_duplicate']].groupby(['qid1','qid2']).count().reset_index()\n",
    "print (\"The number of duplicate question pairs is:\",(question_pair_duplicates).shape[0] - df.shape[0])\n",
    "\n",
    "# Check the unique questions\n",
    "qid_all = pd.Series(df['qid1'].tolist() + df['qid2'].tolist())\n",
    "unique_questions = len(np.unique(qid_all))\n",
    "un_unique_questions = np.sum(qid_all.value_counts() > 1)\n",
    "print(\"There are:\", unique_questions,\" unique questions.\")\n",
    "print(\"There are:\", un_unique_questions,\" question that appear more than once.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are NaN data rows:\n",
      "            id    qid1    qid2                         question1  \\\n",
      "105780  105780  174363  174364    How can I develop android app?   \n",
      "201841  201841  303951  174364  How can I create an Android app?   \n",
      "363362  363362  493340  493341                               NaN   \n",
      "\n",
      "                                                question2  is_duplicate  \n",
      "105780                                                NaN             0  \n",
      "201841                                                NaN             0  \n",
      "363362  My Chinese name is Haichao Yu. What English na...             0  \n",
      "---------------Now start data cleansing for NaN values:-------------\n",
      "Here are NaN data rows:\n",
      "Empty DataFrame\n",
      "Columns: [id, qid1, qid2, question1, question2, is_duplicate]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Data preprocessing: deal with null values\n",
    "\n",
    "nan_data = df[df.isnull().any(1)]\n",
    "print(\"Here are NaN data rows:\")\n",
    "print(nan_data)\n",
    "print(\"---------------Now start data cleansing for NaN values:-------------\")\n",
    "df = df.fillna('')\n",
    "nan_data = df[df.isnull().any(1)]\n",
    "print(\"Here are. NaN data rows:\")\n",
    "print(nan_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Enginerring on Text Mining Features\n",
    "\n",
    "Extract text mining or statistical features from training data.\n",
    " - ___q1len___ = Length of q1\n",
    " - ___q2len___ = Length of q2\n",
    " - ___diff_len___ = len(q1)-len(q2)       \n",
    "\n",
    "\n",
    " - ___q1_n_words___ = Number of words in q1\n",
    " - ___q2_n_words___ = Number of words in q2\n",
    " - ___diff_n_words___ = The difference       \n",
    "\n",
    "\n",
    " - ___caps_count_q1___ = Number of capital words of q1\n",
    " - ___caps_count_q2___ = Number of capital words of q2\n",
    " - ___diff_caps___ = The difference       \n",
    "\n",
    "\n",
    " - ___len_char_q1___ = Number of characters of q1\n",
    " - ___len_char_q2___ = Number of characters of q2\n",
    " - ___diff_len_char___ = The difference      \n",
    "\n",
    "\n",
    " - ___avg_word_len1___ = len(char)/len(word) of q1\n",
    " - ___avg_word_len2___ = len(char)/len(word) of q2\n",
    " - ___diff_avg_word___ = The difference      \n",
    "\n",
    "\n",
    " - ___word_Common___ = Number of common unique words in q1 and q2\n",
    " - ___word_Total___ = Total num of words in Question 1 + Total num of words in q2\n",
    " - ___word_share___ = (word_common)/(word_Total)    \n",
    " - ___2_gram_share___ = word share on 2 gram\n",
    "\n",
    "\n",
    " - ___exactly_same___ = exactly the same\n",
    "\n",
    "\n",
    " \n",
    " \n",
    " - **Ouput: feature_tm.csv**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>q1len</th>\n",
       "      <th>q2len</th>\n",
       "      <th>diff_len</th>\n",
       "      <th>len_word_q1</th>\n",
       "      <th>...</th>\n",
       "      <th>len_char_q1</th>\n",
       "      <th>len_char_q2</th>\n",
       "      <th>diff_len_char</th>\n",
       "      <th>avg_world_len1</th>\n",
       "      <th>avg_world_len2</th>\n",
       "      <th>diff_avg_word</th>\n",
       "      <th>word_Common</th>\n",
       "      <th>word_Total</th>\n",
       "      <th>word_share</th>\n",
       "      <th>share_2_gram</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "      <td>66</td>\n",
       "      <td>57</td>\n",
       "      <td>9</td>\n",
       "      <td>14</td>\n",
       "      <td>...</td>\n",
       "      <td>53</td>\n",
       "      <td>46</td>\n",
       "      <td>7</td>\n",
       "      <td>3.785714</td>\n",
       "      <td>3.833333</td>\n",
       "      <td>-0.047619</td>\n",
       "      <td>10.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.434783</td>\n",
       "      <td>0.416667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "      <td>51</td>\n",
       "      <td>88</td>\n",
       "      <td>-37</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>44</td>\n",
       "      <td>76</td>\n",
       "      <td>-32</td>\n",
       "      <td>5.500000</td>\n",
       "      <td>5.846154</td>\n",
       "      <td>-0.346154</td>\n",
       "      <td>4.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.052632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "      <td>73</td>\n",
       "      <td>59</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>...</td>\n",
       "      <td>60</td>\n",
       "      <td>50</td>\n",
       "      <td>10</td>\n",
       "      <td>4.285714</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>-0.714286</td>\n",
       "      <td>4.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.045455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>65</td>\n",
       "      <td>-15</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>40</td>\n",
       "      <td>57</td>\n",
       "      <td>-17</td>\n",
       "      <td>3.636364</td>\n",
       "      <td>6.333333</td>\n",
       "      <td>-2.696970</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>Which fish would survive in salt water?</td>\n",
       "      <td>0</td>\n",
       "      <td>76</td>\n",
       "      <td>39</td>\n",
       "      <td>37</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>64</td>\n",
       "      <td>33</td>\n",
       "      <td>31</td>\n",
       "      <td>4.923077</td>\n",
       "      <td>4.714286</td>\n",
       "      <td>0.208791</td>\n",
       "      <td>2.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  qid1  qid2                                          question1  \\\n",
       "0   0     1     2  What is the step by step guide to invest in sh...   \n",
       "1   1     3     4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2   2     5     6  How can I increase the speed of my internet co...   \n",
       "3   3     7     8  Why am I mentally very lonely? How can I solve...   \n",
       "4   4     9    10  Which one dissolve in water quikly sugar, salt...   \n",
       "\n",
       "                                           question2  is_duplicate  q1len  \\\n",
       "0  What is the step by step guide to invest in sh...             0     66   \n",
       "1  What would happen if the Indian government sto...             0     51   \n",
       "2  How can Internet speed be increased by hacking...             0     73   \n",
       "3  Find the remainder when [math]23^{24}[/math] i...             0     50   \n",
       "4            Which fish would survive in salt water?             0     76   \n",
       "\n",
       "   q2len  diff_len  len_word_q1  ...  len_char_q1  len_char_q2  diff_len_char  \\\n",
       "0     57         9           14  ...           53           46              7   \n",
       "1     88       -37            8  ...           44           76            -32   \n",
       "2     59        14           14  ...           60           50             10   \n",
       "3     65       -15           11  ...           40           57            -17   \n",
       "4     39        37           13  ...           64           33             31   \n",
       "\n",
       "   avg_world_len1  avg_world_len2  diff_avg_word  word_Common  word_Total  \\\n",
       "0        3.785714        3.833333      -0.047619         10.0        23.0   \n",
       "1        5.500000        5.846154      -0.346154          4.0        20.0   \n",
       "2        4.285714        5.000000      -0.714286          4.0        24.0   \n",
       "3        3.636364        6.333333      -2.696970          0.0        19.0   \n",
       "4        4.923077        4.714286       0.208791          2.0        20.0   \n",
       "\n",
       "   word_share  share_2_gram  \n",
       "0    0.434783      0.416667  \n",
       "1    0.200000      0.052632  \n",
       "2    0.166667      0.045455  \n",
       "3    0.000000      0.000000  \n",
       "4    0.100000      0.000000  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if os.path.isfile('Features/feature_tm.csv'):\n",
    "    df = pd.read_csv(\"Features/feature_tm.csv\",encoding='latin-1')\n",
    "else:\n",
    "    df['q1len'] = df['question1'].str.len() \n",
    "    df['q2len'] = df['question2'].str.len()\n",
    "    df['diff_len'] = df['q1len'] - df['q2len']\n",
    "    \n",
    "    df['len_word_q1'] = df['question1'].apply(lambda row: len(row.split(\" \")))\n",
    "    df['len_word_q2'] = df['question2'].apply(lambda row: len(row.split(\" \")))\n",
    "    df['diff_words'] = df['len_word_q1'] - df['len_word_q2']\n",
    "    \n",
    "    df['caps_count_q1'] = df['question1'].apply(lambda x:sum(1 for i in str(x) if i.isupper()))\n",
    "    df['caps_count_q2'] = df['question2'].apply(lambda x:sum(1 for i in str(x) if i.isupper()))\n",
    "    df['diff_caps'] = df['caps_count_q1'] - df['caps_count_q2']\n",
    "    \n",
    "    df['len_char_q1'] = df['question1'].apply(lambda x: len(str(x).replace(' ', '')))\n",
    "    df['len_char_q2'] = df['question2'].apply(lambda x: len(str(x).replace(' ', '')))\n",
    "    df['diff_len_char'] = df['len_char_q1'] - df['len_char_q2']\n",
    "    \n",
    "    df['avg_world_len1'] = df['len_char_q1'] / df['len_word_q1']\n",
    "    df['avg_world_len2'] = df['len_char_q2'] / df['len_word_q2']\n",
    "    df['diff_avg_word'] = df['avg_world_len1'] - df['avg_world_len2']\n",
    "    \n",
    "\n",
    "    def normalized_word_Common(row):\n",
    "        w1 = set(map(lambda word: word.lower().strip(), row['question1'].split(\" \")))\n",
    "        w2 = set(map(lambda word: word.lower().strip(), row['question2'].split(\" \")))    \n",
    "        return 1.0 * len(w1 & w2)\n",
    "    df['word_Common'] = df.apply(normalized_word_Common, axis=1)\n",
    "\n",
    "    def normalized_word_Total(row):\n",
    "        w1 = set(map(lambda word: word.lower().strip(), row['question1'].split(\" \")))\n",
    "        w2 = set(map(lambda word: word.lower().strip(), row['question2'].split(\" \")))    \n",
    "        return 1.0 * (len(w1) + len(w2))\n",
    "    df['word_Total'] = df.apply(normalized_word_Total, axis=1)\n",
    "\n",
    "    def normalized_word_share(row):\n",
    "        w1 = set(map(lambda word: word.lower().strip(), row['question1'].split(\" \")))\n",
    "        w2 = set(map(lambda word: word.lower().strip(), row['question2'].split(\" \")))    \n",
    "        return 1.0 * len(w1 & w2)/(len(w1) + len(w2))\n",
    "    df['word_share'] = df.apply(normalized_word_share, axis=1)\n",
    "\n",
    "    def get_2_gram_share(row):\n",
    "        q1_list = str(row['question1']).lower().split()\n",
    "        q2_list = str(row['question2']).lower().split()\n",
    "        q1_2_gram = set([i for i in zip(q1_list, q1_list[1:])])\n",
    "        q2_2_gram = set([i for i in zip(q2_list, q2_list[1:])])\n",
    "        shared_2_gram = q1_2_gram.intersection(q2_2_gram)\n",
    "        if len(q1_2_gram) + len(q2_2_gram) == 0:\n",
    "            R2gram = 0\n",
    "        else:\n",
    "            R2gram = len(shared_2_gram) / (len(q1_2_gram) + len(q2_2_gram))\n",
    "        return R2gram\n",
    "    df['share_2_gram'] = df.apply(get_2_gram_share, axis=1) \n",
    "\n",
    "    df.to_csv(\"Features/feature_tm.csv\", index=False)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id                0\n",
      "qid1              0\n",
      "qid2              0\n",
      "question1         1\n",
      "question2         2\n",
      "is_duplicate      0\n",
      "q1len             0\n",
      "q2len             0\n",
      "diff_len          0\n",
      "len_word_q1       0\n",
      "len_word_q2       0\n",
      "diff_words        0\n",
      "caps_count_q1     0\n",
      "caps_count_q2     0\n",
      "diff_caps         0\n",
      "len_char_q1       0\n",
      "len_char_q2       0\n",
      "diff_len_char     0\n",
      "avg_world_len1    0\n",
      "avg_world_len2    0\n",
      "diff_avg_word     0\n",
      "word_Common       0\n",
      "word_Total        0\n",
      "word_share        0\n",
      "share_2_gram      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check the NaN data\n",
    "\n",
    "if os.path.isfile('Features/feature_tm.csv'):\n",
    "    df_tm = pd.read_csv(\"Features/feature_tm.csv\",encoding='latin-1')\n",
    "    print(df_tm.isna().sum())\n",
    "    df_tm = df_tm.fillna('')\n",
    "    df_tm.head()\n",
    "else:\n",
    "    # If there are no existing file then you need to create a csv file, make sure you have run the previous code in 1.2 chapter\n",
    "    print(\"There is no Features/feature_tm.csv!\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id                0\n",
      "qid1              0\n",
      "qid2              0\n",
      "question1         0\n",
      "question2         0\n",
      "is_duplicate      0\n",
      "q1len             0\n",
      "q2len             0\n",
      "diff_len          0\n",
      "len_word_q1       0\n",
      "len_word_q2       0\n",
      "diff_words        0\n",
      "caps_count_q1     0\n",
      "caps_count_q2     0\n",
      "diff_caps         0\n",
      "len_char_q1       0\n",
      "len_char_q2       0\n",
      "diff_len_char     0\n",
      "avg_world_len1    0\n",
      "avg_world_len2    0\n",
      "diff_avg_word     0\n",
      "word_Common       0\n",
      "word_Total        0\n",
      "word_share        0\n",
      "share_2_gram      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering on NLP Features\n",
    "\n",
    "Extracting NLP features, including:\n",
    "- Statistical features of NLP nouns like stop_word, token, substring etc\n",
    "- NLP distances\n",
    "- Fuzzy features\n",
    "\n",
    "Features:\n",
    "\n",
    "- __last_word_eq__ :  Check if Last word of both questions is equal or not<br>last_word_eq = int(q1_tokens[-1] == q2_tokens[-1])\n",
    "\n",
    "\n",
    "- __first_word_eq__ :  Check if First word of both questions is equal or not<br>first_word_eq = int(q1_tokens[0] == q2_tokens[0])\n",
    "\n",
    "\n",
    "- __abs_len_diff__ :  Abs. length difference<br>abs_len_diff = abs(len(q1_tokens) - len(q2_tokens))\n",
    "\n",
    "\n",
    "- __mean_len__ :  Average Token Length of both Questions<br>mean_len = (len(q1_tokens) + len(q2_tokens))/2\n",
    "\n",
    "- __cwc_min__ :  Ratio of common_word_count to min lenghth of word count of Q1 and Q2 <br>cwc_min = common_word_count / (min(len(q1_words), len(q2_words))\n",
    "\n",
    "\n",
    "- __cwc_max__ :  Ratio of common_word_count to max lenghth of word count of Q1 and Q2 <br>cwc_max = common_word_count / (max(len(q1_words), len(q2_words))\n",
    "\n",
    "\n",
    "- __csc_min__ :  Ratio of common_stop_count to min lenghth of stop count of Q1 and Q2 <br> csc_min = common_stop_count / (min(len(q1_stops), len(q2_stops))\n",
    "\n",
    "\n",
    "- __csc_max__ :  Ratio of common_stop_count to max lenghth of stop count of Q1 and Q2<br>csc_max = common_stop_count / (max(len(q1_stops), len(q2_stops))\n",
    "\n",
    "\n",
    "- __ctc_min__ :  Ratio of common_token_count to min lenghth of token count of Q1 and Q2<br>ctc_min = common_token_count / (min(len(q1_tokens), len(q2_tokens))\n",
    "\n",
    "\n",
    "- __ctc_max__ :  Ratio of common_token_count to max lenghth of token count of Q1 and Q2<br>ctc_max = common_token_count / (max(len(q1_tokens), len(q2_tokens))\n",
    "\n",
    "\n",
    "- __wmd_dist__: Thesis reference: http://proceedings.mlr.press/v37/kusnerb15.pdf \n",
    "\n",
    "\n",
    "- __cosine_dist__: Cosine distance here is the cosine distance between two glove based vectors. Different from cosine similarity of tf/tfidf in Chapter 1.5\n",
    "\n",
    "\n",
    "- __cityblock_dist__: just follow the official defination\n",
    "\n",
    "\n",
    "- __canberra_dist__: just follow the official defination\n",
    "\n",
    "\n",
    "- __euclidean_dist__: just follow the official defination\n",
    "\n",
    "\n",
    "- __minkowski_dist__: just follow the official defination\n",
    "\n",
    "\n",
    "- __fuzz_ratio__ :  https://github.com/seatgeek/fuzzywuzzy#usage\n",
    "http://chairnerd.seatgeek.com/fuzzywuzzy-fuzzy-string-matching-in-python/\n",
    "\n",
    "\n",
    "- __fuzz_partial_ratio__ :  https://github.com/seatgeek/fuzzywuzzy#usage\n",
    "http://chairnerd.seatgeek.com/fuzzywuzzy-fuzzy-string-matching-in-python/\n",
    "\n",
    "\n",
    "- __token_sort_ratio__ : https://github.com/seatgeek/fuzzywuzzy#usage\n",
    "http://chairnerd.seatgeek.com/fuzzywuzzy-fuzzy-string-matching-in-python/\n",
    "\n",
    "\n",
    "- __token_set_ratio__ : https://github.com/seatgeek/fuzzywuzzy#usage\n",
    "http://chairnerd.seatgeek.com/fuzzywuzzy-fuzzy-string-matching-in-python/\n",
    "\n",
    "\n",
    "- __longest_substr_ratio__ :  Ratio of length longest common substring to min lenghth of token count of Q1 and Q2<br>longest_substr_ratio = len(longest common substring) / (min(len(q1_tokens), len(q2_tokens))\n",
    "Thesis refered from /http://static.hongbozhang.me/doc/STAT_441_Report.pdf\n",
    "\n",
    "\n",
    "\n",
    "- **Ouput: feature_nlp.csv**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/yanzheyuan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Data Preprocessing: remove noise like html-tags, punctuations, stemming, stopwords, etc.\n",
    "# Idea from kaggle notebooks\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# To get the results in 4 decemal points\n",
    "SAFE_DIV = 0.0001 \n",
    "\n",
    "# STOP_WORDS = nltk.stopwords.words(\"english\")\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "STOP_WORDS = stopwords.words('english')\n",
    "\n",
    "# Preprocessing\n",
    "def preprocess(x):\n",
    "    x = str(x).lower()\n",
    "    x = x.replace(\",000,000\", \"m\").replace(\",000\", \"k\").replace(\"′\", \"'\").replace(\"’\", \"'\")\\\n",
    "                           .replace(\"won't\", \"will not\").replace(\"cannot\", \"can not\").replace(\"can't\", \"can not\")\\\n",
    "                           .replace(\"n't\", \" not\").replace(\"what's\", \"what is\").replace(\"it's\", \"it is\")\\\n",
    "                           .replace(\"'ve\", \" have\").replace(\"i'm\", \"i am\").replace(\"'re\", \" are\")\\\n",
    "                           .replace(\"he's\", \"he is\").replace(\"she's\", \"she is\").replace(\"'s\", \" own\")\\\n",
    "                           .replace(\"%\", \" percent \").replace(\"₹\", \" rupee \").replace(\"$\", \" dollar \")\\\n",
    "                           .replace(\"€\", \" euro \").replace(\"'ll\", \" will\")\n",
    "    x = re.sub(r\"([0-9]+)000000\", r\"\\1m\", x)\n",
    "    x = re.sub(r\"([0-9]+)000\", r\"\\1k\", x)\n",
    "    \n",
    "    \n",
    "    porter = PorterStemmer()\n",
    "    pattern = re.compile('\\W')\n",
    "    \n",
    "    if type(x) == type(''):\n",
    "        x = re.sub(pattern, ' ', x)\n",
    "    \n",
    "    \n",
    "    if type(x) == type(''):\n",
    "        x = porter.stem(x)\n",
    "        example1 = BeautifulSoup(x)\n",
    "        x = example1.get_text()\n",
    "               \n",
    "    \n",
    "    return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-11-03 02:56:35--  http://nlp.stanford.edu/data/glove.840B.300d.zip\n",
      "正在解析主机 nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
      "正在连接 nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... 已连接。\n",
      "已发出 HTTP 请求，正在等待回应... 302 Found\n",
      "位置：https://nlp.stanford.edu/data/glove.840B.300d.zip [跟随至新的 URL]\n",
      "--2020-11-03 02:56:35--  https://nlp.stanford.edu/data/glove.840B.300d.zip\n",
      "正在连接 nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... 已连接。\n",
      "已发出 HTTP 请求，正在等待回应... 301 Moved Permanently\n",
      "位置：http://downloads.cs.stanford.edu/nlp/data/glove.840B.300d.zip [跟随至新的 URL]\n",
      "--2020-11-03 02:56:36--  http://downloads.cs.stanford.edu/nlp/data/glove.840B.300d.zip\n",
      "正在解析主机 downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
      "正在连接 downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... 已连接。\n",
      "已发出 HTTP 请求，正在等待回应... 200 OK\n",
      "长度：2176768927 (2.0G) [application/zip]\n",
      "正在保存至: “glove.840B.300d.zip”\n",
      "\n",
      "glove.840B.300d.zip 100%[===================>]   2.03G  2.03MB/s  用时 17m 4s    \n",
      "\n",
      "2020-11-03 03:13:41 (2.03 MB/s) - 已保存 “glove.840B.300d.zip” [2176768927/2176768927])\n",
      "\n",
      "Archive:  glove.840B.300d.zip\n",
      "  inflating: glove.840B.300d.txt     \n"
     ]
    }
   ],
   "source": [
    "# Preparations for distance calculations\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "import pickle\n",
    "import gensim\n",
    "\n",
    "# Download GloVe model\n",
    "!wget http://nlp.stanford.edu/data/glove.840B.300d.zip\n",
    "!unzip glove.840B.300d.zip\n",
    "\n",
    "# Use gensim package to do word-to-vec\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "glove2word2vec(glove_input_file=\"glove.840B.300d.txt\", word2vec_output_file=\"glove_vectors.txt\")\n",
    "\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "glove_model = KeyedVectors.load_word2vec_format(\"glove_vectors.txt\", binary=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distance calculations\n",
    "from scipy.stats import skew, kurtosis\n",
    "from scipy.spatial.distance import cosine, cityblock, canberra, euclidean, minkowski\n",
    "\n",
    "# Preprocessing: simple version\n",
    "def remove_stop(sentence):\n",
    "    sentence  = str(sentence)\n",
    "    if sentence == None:\n",
    "        return ' '\n",
    "    if sentence == np.nan:\n",
    "        return ' '\n",
    "    if sentence == 'NaN':\n",
    "        return ' '\n",
    "    z = [i for i in sentence.split() if i not in STOP_WORDS]\n",
    "    return ' '.join(z)\n",
    "\n",
    "# wmd_dist calculation\n",
    "def wmd(s1, s2, model):\n",
    "    s1 = str(s1)\n",
    "    s2 = str(s2)\n",
    "    s1 = s1.split()\n",
    "    s2 = s2.split()\n",
    "    return model.wmdistance(s1, s2)\n",
    "\n",
    "# the average glove2word2vec-based vectors of every word in a sentence\n",
    "def g2w2v(list_of_sent, model, d):\n",
    "    # Returns average of word vectors for each sentance with dimension of model given\n",
    "    sent_vectors = []\n",
    "    for sentence in list_of_sent: # for each review/sentence\n",
    "        doc = [word for word in sentence if word in model.wv.vocab] # .wv: return a numpy vector of a word\n",
    "        if doc:\n",
    "            sent_vec = np.mean(model.wv[doc],axis=0) # get the average of vector, namely avgw2v.\n",
    "        else:\n",
    "            sent_vec = np.zeros(d)\n",
    "        sent_vectors.append(sent_vec)\n",
    "    return sent_vectors\n",
    "\n",
    "# Gathering all calculations\n",
    "def get_distance_features(df):\n",
    "    \n",
    "    print(\"Extracting Distance Features..\")\n",
    "    \n",
    "    # wmd_distance\n",
    "    df['question1'] = df.question1.apply(remove_stop)\n",
    "    df['question2'] = df.question2.apply(remove_stop)\n",
    "    df['word_mover_dist'] = df.apply(lambda x: wmd(x['question1'], x['question2'],glove_model), axis=1)\n",
    "    \n",
    "    print(\"- wmd done...\")\n",
    "    \n",
    "    # Other Distances\n",
    "    # Converting questions into lists of sentences\n",
    "    list_of_question1=[]\n",
    "    for sentence in df.question1.values:\n",
    "        list_of_question1.append(sentence.split())\n",
    "    \n",
    "    list_of_question2=[]\n",
    "    for sentence in df.question2.values:\n",
    "        list_of_question2.append(sentence.split())\n",
    "    \n",
    "    # Get embeded vectors from a pre-trained model(GloVe2Word2Vec)\n",
    "    g2w2v_q1 = g2w2v(list_of_question1, glove_model, 300)\n",
    "    g2w2v_q2 = g2w2v(list_of_question2, glove_model, 300)\n",
    "    \n",
    "    # !!! Glove based word-vectors: can added to NLP features but i removed, but may be useful for model improving\n",
    "    # df_g2w2v = pd.DataFrame()\n",
    "    # df_g2w2v['q1_vec'] = list(g2w2v_q1)\n",
    "    # df_g2w2v['q2_vec'] = list(g2w2v_q2)\n",
    "    # df_q1 = pd.DataFrame(df_g2w2v.q1_vec.values.tolist())\n",
    "    # df_q2 = pd.DataFrame(df_g2w2v.q2_vec.values.tolist())\n",
    "    \n",
    "    print(\"- embedding done...\")\n",
    "    \n",
    "    # Spatial Distances on vectors of questions\n",
    "    df['cosine_dist'] = [cosine(q1, q2) for (q1, q2) in zip(g2w2v_q1,g2w2v_q2)]\n",
    "    df['cityblock_dist'] = [cityblock(q1, q2) for (q1, q2) in zip(g2w2v_q1,g2w2v_q2)]\n",
    "    df['canberra_dist'] = [canberra(q1, q2) for (q1, q2) in zip(g2w2v_q1,g2w2v_q2)]\n",
    "    df['euclidean_dist'] = [euclidean(q1, q2) for (q1, q2) in zip(g2w2v_q1,g2w2v_q2)]\n",
    "    df['minkowski_dist'] = [minkowski(q1, q2) for (q1, q2) in zip(g2w2v_q1,g2w2v_q2)]\n",
    "    \n",
    "    print('- spatial distance done')\n",
    "    \n",
    "    # Deal with nan values\n",
    "    df.cosine_dist = df.cosine_dist.fillna(0)\n",
    "    df.word_mover_dist = df.word_mover_dist.apply(lambda wmd: 30 if wmd == np.inf else wmd )\n",
    "   \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical features on Text Tokens of questions\n",
    "def get_token_features(q1, q2):\n",
    "    token_features = [0.0]*10\n",
    "    \n",
    "    # Converting the sentence into Tokens: \n",
    "    q1_tokens = q1.split()\n",
    "    q2_tokens = q2.split()\n",
    "\n",
    "    if len(q1_tokens) == 0 or len(q2_tokens) == 0:\n",
    "        return token_features\n",
    "    \n",
    "    # Get the non-stopwords in questions\n",
    "    q1_words = set([word for word in q1_tokens if word not in STOP_WORDS])\n",
    "    q2_words = set([word for word in q2_tokens if word not in STOP_WORDS])\n",
    "    \n",
    "    # Get the stopwords in questions\n",
    "    q1_stops = set([word for word in q1_tokens if word in STOP_WORDS])\n",
    "    q2_stops = set([word for word in q2_tokens if word in STOP_WORDS])\n",
    "    \n",
    "    # Get the common non-stopwords from question pair\n",
    "    common_word_count = len(q1_words.intersection(q2_words))\n",
    "    \n",
    "    # Get the common stopwords from question pair\n",
    "    common_stop_count = len(q1_stops.intersection(q2_stops))\n",
    "    \n",
    "    # Get the common Tokens from question pair\n",
    "    common_token_count = len(set(q1_tokens).intersection(set(q2_tokens)))\n",
    "    \n",
    "    # Add safety div\n",
    "    token_features[0] = common_word_count / (min(len(q1_words), len(q2_words)) + SAFE_DIV)\n",
    "    token_features[1] = common_word_count / (max(len(q1_words), len(q2_words)) + SAFE_DIV)\n",
    "    token_features[2] = common_stop_count / (min(len(q1_stops), len(q2_stops)) + SAFE_DIV)\n",
    "    token_features[3] = common_stop_count / (max(len(q1_stops), len(q2_stops)) + SAFE_DIV)\n",
    "    token_features[4] = common_token_count / (min(len(q1_tokens), len(q2_tokens)) + SAFE_DIV)\n",
    "    token_features[5] = common_token_count / (max(len(q1_tokens), len(q2_tokens)) + SAFE_DIV)\n",
    "    \n",
    "    # Last word of both question is same or not\n",
    "    token_features[6] = int(q1_tokens[-1] == q2_tokens[-1])\n",
    "    \n",
    "    # First word of both question is same or not\n",
    "    token_features[7] = int(q1_tokens[0] == q2_tokens[0])\n",
    "    \n",
    "    token_features[8] = abs(len(q1_tokens) - len(q2_tokens))\n",
    "    \n",
    "    # Average Token Length of both Questions\n",
    "    token_features[9] = (len(q1_tokens) + len(q2_tokens))/2\n",
    "    return token_features\n",
    "\n",
    "\n",
    "# Get the Longest Common sub string\n",
    "def get_longest_substr_ratio(a, b):\n",
    "    strs = list(distance.lcsubstrings(a, b))\n",
    "    if len(strs) == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return len(strs[0]) / (min(len(a), len(b)) + 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather all the NLP features\n",
    "def extract_features(df):\n",
    "    # preprocessing each question, apply self-defined function preprocess to filter text data with stopwords preparation\n",
    "    df[\"question1\"] = df[\"question1\"].fillna(\"\").apply(preprocess)\n",
    "    df[\"question2\"] = df[\"question2\"].fillna(\"\").apply(preprocess)\n",
    "\n",
    "    print(\"Extracting Token Features...\")\n",
    "    \n",
    "    token_features = df.apply(lambda x: get_token_features(x[\"question1\"], x[\"question2\"]), axis=1)\n",
    "    \n",
    "    df[\"cwc_min\"]       = list(map(lambda x: x[0], token_features))\n",
    "    df[\"cwc_max\"]       = list(map(lambda x: x[1], token_features))\n",
    "    df[\"csc_min\"]       = list(map(lambda x: x[2], token_features))\n",
    "    df[\"csc_max\"]       = list(map(lambda x: x[3], token_features))\n",
    "    df[\"ctc_min\"]       = list(map(lambda x: x[4], token_features))\n",
    "    df[\"ctc_max\"]       = list(map(lambda x: x[5], token_features))\n",
    "    df[\"last_word_eq\"]  = list(map(lambda x: x[6], token_features))\n",
    "    df[\"first_word_eq\"] = list(map(lambda x: x[7], token_features))\n",
    "    df[\"abs_len_diff\"]  = list(map(lambda x: x[8], token_features))\n",
    "    df[\"mean_len\"]      = list(map(lambda x: x[9], token_features))\n",
    "   \n",
    "    # Getting Fuzzy Features and Merging with Dataset\n",
    "    print(\"Extracting Fuzzy Features..\")\n",
    "\n",
    "    df[\"token_set_ratio\"]       = df.apply(lambda x: fuzz.token_set_ratio(x[\"question1\"], x[\"question2\"]), axis=1)\n",
    "    df[\"token_sort_ratio\"]      = df.apply(lambda x: fuzz.token_sort_ratio(x[\"question1\"], x[\"question2\"]), axis=1)\n",
    "    df[\"fuzz_ratio\"]            = df.apply(lambda x: fuzz.QRatio(x[\"question1\"], x[\"question2\"]), axis=1)\n",
    "    df[\"fuzz_partial_ratio\"]    = df.apply(lambda x: fuzz.partial_ratio(x[\"question1\"], x[\"question2\"]), axis=1)\n",
    "    df[\"longest_substr_ratio\"]  = df.apply(lambda x: get_longest_substr_ratio(x[\"question1\"], x[\"question2\"]), axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features for train:\n",
      "Extracting Token Features...\n",
      "Extracting Fuzzy Features..\n",
      "Extracting Distance Features..\n",
      "- wmd done...\n",
      "- embedding done...\n",
      "- spatial distance done\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>cwc_min</th>\n",
       "      <th>cwc_max</th>\n",
       "      <th>csc_min</th>\n",
       "      <th>csc_max</th>\n",
       "      <th>...</th>\n",
       "      <th>token_sort_ratio</th>\n",
       "      <th>fuzz_ratio</th>\n",
       "      <th>fuzz_partial_ratio</th>\n",
       "      <th>longest_substr_ratio</th>\n",
       "      <th>word_mover_dist</th>\n",
       "      <th>cosine_dist</th>\n",
       "      <th>cityblock_dist</th>\n",
       "      <th>canberra_dist</th>\n",
       "      <th>euclidean_dist</th>\n",
       "      <th>minkowski_dist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>step step guide invest share market india</td>\n",
       "      <td>step step guide invest share market</td>\n",
       "      <td>0</td>\n",
       "      <td>0.999980</td>\n",
       "      <td>0.833319</td>\n",
       "      <td>0.999983</td>\n",
       "      <td>0.999983</td>\n",
       "      <td>...</td>\n",
       "      <td>93</td>\n",
       "      <td>93</td>\n",
       "      <td>100</td>\n",
       "      <td>0.982759</td>\n",
       "      <td>1.216034</td>\n",
       "      <td>0.031762</td>\n",
       "      <td>14.274065</td>\n",
       "      <td>91.483062</td>\n",
       "      <td>1.047253</td>\n",
       "      <td>1.047253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>story kohinoor koh noor diamond</td>\n",
       "      <td>would happen indian government stole kohinoor ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.799984</td>\n",
       "      <td>0.399996</td>\n",
       "      <td>0.749981</td>\n",
       "      <td>0.599988</td>\n",
       "      <td>...</td>\n",
       "      <td>63</td>\n",
       "      <td>66</td>\n",
       "      <td>75</td>\n",
       "      <td>0.596154</td>\n",
       "      <td>4.897662</td>\n",
       "      <td>0.266555</td>\n",
       "      <td>33.272633</td>\n",
       "      <td>149.670092</td>\n",
       "      <td>2.624989</td>\n",
       "      <td>2.624989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>increase speed internet connection using vpn</td>\n",
       "      <td>internet speed increased hacking dns</td>\n",
       "      <td>0</td>\n",
       "      <td>0.399992</td>\n",
       "      <td>0.333328</td>\n",
       "      <td>0.399992</td>\n",
       "      <td>0.249997</td>\n",
       "      <td>...</td>\n",
       "      <td>63</td>\n",
       "      <td>43</td>\n",
       "      <td>47</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>4.011556</td>\n",
       "      <td>0.118900</td>\n",
       "      <td>28.457512</td>\n",
       "      <td>129.214660</td>\n",
       "      <td>2.140298</td>\n",
       "      <td>2.140298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>mentally lonely solve</td>\n",
       "      <td>find remainder math 23 24 math divided 24 23</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>24</td>\n",
       "      <td>9</td>\n",
       "      <td>14</td>\n",
       "      <td>0.039216</td>\n",
       "      <td>7.514702</td>\n",
       "      <td>0.619671</td>\n",
       "      <td>62.016426</td>\n",
       "      <td>200.899534</td>\n",
       "      <td>4.702347</td>\n",
       "      <td>4.702347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>one dissolve water quikly sugar salt methane c...</td>\n",
       "      <td>fish would survive salt water</td>\n",
       "      <td>0</td>\n",
       "      <td>0.399992</td>\n",
       "      <td>0.199998</td>\n",
       "      <td>0.999950</td>\n",
       "      <td>0.666644</td>\n",
       "      <td>...</td>\n",
       "      <td>47</td>\n",
       "      <td>35</td>\n",
       "      <td>56</td>\n",
       "      <td>0.175000</td>\n",
       "      <td>6.257260</td>\n",
       "      <td>0.244168</td>\n",
       "      <td>40.127296</td>\n",
       "      <td>156.627744</td>\n",
       "      <td>3.145122</td>\n",
       "      <td>3.145122</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  qid1  qid2                                          question1  \\\n",
       "0   0     1     2          step step guide invest share market india   \n",
       "1   1     3     4                    story kohinoor koh noor diamond   \n",
       "2   2     5     6       increase speed internet connection using vpn   \n",
       "3   3     7     8                              mentally lonely solve   \n",
       "4   4     9    10  one dissolve water quikly sugar salt methane c...   \n",
       "\n",
       "                                           question2  is_duplicate   cwc_min  \\\n",
       "0                step step guide invest share market             0  0.999980   \n",
       "1  would happen indian government stole kohinoor ...             0  0.799984   \n",
       "2               internet speed increased hacking dns             0  0.399992   \n",
       "3       find remainder math 23 24 math divided 24 23             0  0.000000   \n",
       "4                      fish would survive salt water             0  0.399992   \n",
       "\n",
       "    cwc_max   csc_min   csc_max  ...  token_sort_ratio  fuzz_ratio  \\\n",
       "0  0.833319  0.999983  0.999983  ...                93          93   \n",
       "1  0.399996  0.749981  0.599988  ...                63          66   \n",
       "2  0.333328  0.399992  0.249997  ...                63          43   \n",
       "3  0.000000  0.000000  0.000000  ...                24           9   \n",
       "4  0.199998  0.999950  0.666644  ...                47          35   \n",
       "\n",
       "   fuzz_partial_ratio  longest_substr_ratio  word_mover_dist  cosine_dist  \\\n",
       "0                 100              0.982759         1.216034     0.031762   \n",
       "1                  75              0.596154         4.897662     0.266555   \n",
       "2                  47              0.166667         4.011556     0.118900   \n",
       "3                  14              0.039216         7.514702     0.619671   \n",
       "4                  56              0.175000         6.257260     0.244168   \n",
       "\n",
       "   cityblock_dist  canberra_dist  euclidean_dist  minkowski_dist  \n",
       "0       14.274065      91.483062        1.047253        1.047253  \n",
       "1       33.272633     149.670092        2.624989        2.624989  \n",
       "2       28.457512     129.214660        2.140298        2.140298  \n",
       "3       62.016426     200.899534        4.702347        4.702347  \n",
       "4       40.127296     156.627744        3.145122        3.145122  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if os.path.isfile('Features/feature_nlp.csv'):\n",
    "    df_nlp = pd.read_csv(\"Features/feature_nlp.csv\",encoding='latin-1')\n",
    "    # df.fillna('')\n",
    "else:\n",
    "    # If there are no existing file then create a csv file, make sure you have run the previous code in 1.3 chapter\n",
    "    print(\"Extracting features for train:\")\n",
    "    df = pd.read_csv(\"Data/train.csv\")\n",
    "    df = extract_features(df)\n",
    "    df = get_distance_features(df)\n",
    "    # drop unecessary columns\n",
    "    df = df.drop(['qid1','qid2','question1','question2','is_duplicate'], axis=1)\n",
    "    df.to_csv(\"Features/feature_nlp.csv\", index=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(404290, 27)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.cosine_dist = df.cosine_dist.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                      0\n",
       "qid1                    0\n",
       "qid2                    0\n",
       "question1               0\n",
       "question2               0\n",
       "is_duplicate            0\n",
       "cwc_min                 0\n",
       "cwc_max                 0\n",
       "csc_min                 0\n",
       "csc_max                 0\n",
       "ctc_min                 0\n",
       "ctc_max                 0\n",
       "last_word_eq            0\n",
       "first_word_eq           0\n",
       "abs_len_diff            0\n",
       "mean_len                0\n",
       "token_set_ratio         0\n",
       "token_sort_ratio        0\n",
       "fuzz_ratio              0\n",
       "fuzz_partial_ratio      0\n",
       "longest_substr_ratio    0\n",
       "word_mover_dist         0\n",
       "cosine_dist             0\n",
       "cityblock_dist          0\n",
       "canberra_dist           0\n",
       "euclidean_dist          0\n",
       "minkowski_dist          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check on NaN values\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>cwc_min</th>\n",
       "      <th>cwc_max</th>\n",
       "      <th>csc_min</th>\n",
       "      <th>csc_max</th>\n",
       "      <th>...</th>\n",
       "      <th>token_sort_ratio</th>\n",
       "      <th>fuzz_ratio</th>\n",
       "      <th>fuzz_partial_ratio</th>\n",
       "      <th>longest_substr_ratio</th>\n",
       "      <th>word_mover_dist</th>\n",
       "      <th>cosine_dist</th>\n",
       "      <th>cityblock_dist</th>\n",
       "      <th>canberra_dist</th>\n",
       "      <th>euclidean_dist</th>\n",
       "      <th>minkowski_dist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [id, qid1, qid2, question1, question2, is_duplicate, cwc_min, cwc_max, csc_min, csc_max, ctc_min, ctc_max, last_word_eq, first_word_eq, abs_len_diff, mean_len, token_set_ratio, token_sort_ratio, fuzz_ratio, fuzz_partial_ratio, longest_substr_ratio, word_mover_dist, cosine_dist, cityblock_dist, canberra_dist, euclidean_dist, minkowski_dist]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 27 columns]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = df[df.isnull().any(1)]\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cwc_min</th>\n",
       "      <th>cwc_max</th>\n",
       "      <th>csc_min</th>\n",
       "      <th>csc_max</th>\n",
       "      <th>ctc_min</th>\n",
       "      <th>ctc_max</th>\n",
       "      <th>last_word_eq</th>\n",
       "      <th>first_word_eq</th>\n",
       "      <th>abs_len_diff</th>\n",
       "      <th>...</th>\n",
       "      <th>token_sort_ratio</th>\n",
       "      <th>fuzz_ratio</th>\n",
       "      <th>fuzz_partial_ratio</th>\n",
       "      <th>longest_substr_ratio</th>\n",
       "      <th>word_mover_dist</th>\n",
       "      <th>cosine_dist</th>\n",
       "      <th>cityblock_dist</th>\n",
       "      <th>canberra_dist</th>\n",
       "      <th>euclidean_dist</th>\n",
       "      <th>minkowski_dist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.999980</td>\n",
       "      <td>0.833319</td>\n",
       "      <td>0.999983</td>\n",
       "      <td>0.999983</td>\n",
       "      <td>0.916659</td>\n",
       "      <td>0.785709</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>93</td>\n",
       "      <td>93</td>\n",
       "      <td>100</td>\n",
       "      <td>0.982759</td>\n",
       "      <td>1.216034</td>\n",
       "      <td>0.031762</td>\n",
       "      <td>14.274065</td>\n",
       "      <td>91.483062</td>\n",
       "      <td>1.047253</td>\n",
       "      <td>1.047253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.799984</td>\n",
       "      <td>0.399996</td>\n",
       "      <td>0.749981</td>\n",
       "      <td>0.599988</td>\n",
       "      <td>0.699993</td>\n",
       "      <td>0.466664</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>63</td>\n",
       "      <td>66</td>\n",
       "      <td>75</td>\n",
       "      <td>0.596154</td>\n",
       "      <td>4.897662</td>\n",
       "      <td>0.266555</td>\n",
       "      <td>33.272633</td>\n",
       "      <td>149.670092</td>\n",
       "      <td>2.624989</td>\n",
       "      <td>2.624989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.399992</td>\n",
       "      <td>0.333328</td>\n",
       "      <td>0.399992</td>\n",
       "      <td>0.249997</td>\n",
       "      <td>0.399996</td>\n",
       "      <td>0.285712</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>63</td>\n",
       "      <td>43</td>\n",
       "      <td>47</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>4.011556</td>\n",
       "      <td>0.118900</td>\n",
       "      <td>28.457512</td>\n",
       "      <td>129.214660</td>\n",
       "      <td>2.140298</td>\n",
       "      <td>2.140298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>24</td>\n",
       "      <td>9</td>\n",
       "      <td>14</td>\n",
       "      <td>0.039216</td>\n",
       "      <td>7.514702</td>\n",
       "      <td>0.619671</td>\n",
       "      <td>62.016426</td>\n",
       "      <td>200.899534</td>\n",
       "      <td>4.702347</td>\n",
       "      <td>4.702347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.399992</td>\n",
       "      <td>0.199998</td>\n",
       "      <td>0.999950</td>\n",
       "      <td>0.666644</td>\n",
       "      <td>0.571420</td>\n",
       "      <td>0.307690</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>47</td>\n",
       "      <td>35</td>\n",
       "      <td>56</td>\n",
       "      <td>0.175000</td>\n",
       "      <td>6.257260</td>\n",
       "      <td>0.244168</td>\n",
       "      <td>40.127296</td>\n",
       "      <td>156.627744</td>\n",
       "      <td>3.145122</td>\n",
       "      <td>3.145122</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id   cwc_min   cwc_max   csc_min   csc_max   ctc_min   ctc_max  \\\n",
       "0   0  0.999980  0.833319  0.999983  0.999983  0.916659  0.785709   \n",
       "1   1  0.799984  0.399996  0.749981  0.599988  0.699993  0.466664   \n",
       "2   2  0.399992  0.333328  0.399992  0.249997  0.399996  0.285712   \n",
       "3   3  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4   4  0.399992  0.199998  0.999950  0.666644  0.571420  0.307690   \n",
       "\n",
       "   last_word_eq  first_word_eq  abs_len_diff  ...  token_sort_ratio  \\\n",
       "0           0.0            1.0           2.0  ...                93   \n",
       "1           0.0            1.0           5.0  ...                63   \n",
       "2           0.0            1.0           4.0  ...                63   \n",
       "3           0.0            0.0           2.0  ...                24   \n",
       "4           0.0            1.0           6.0  ...                47   \n",
       "\n",
       "   fuzz_ratio  fuzz_partial_ratio  longest_substr_ratio  word_mover_dist  \\\n",
       "0          93                 100              0.982759         1.216034   \n",
       "1          66                  75              0.596154         4.897662   \n",
       "2          43                  47              0.166667         4.011556   \n",
       "3           9                  14              0.039216         7.514702   \n",
       "4          35                  56              0.175000         6.257260   \n",
       "\n",
       "   cosine_dist  cityblock_dist  canberra_dist  euclidean_dist  minkowski_dist  \n",
       "0     0.031762       14.274065      91.483062        1.047253        1.047253  \n",
       "1     0.266555       33.272633     149.670092        2.624989        2.624989  \n",
       "2     0.118900       28.457512     129.214660        2.140298        2.140298  \n",
       "3     0.619671       62.016426     200.899534        4.702347        4.702347  \n",
       "4     0.244168       40.127296     156.627744        3.145122        3.145122  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop unecessary columns: I have added this code into the previous code, so the code below is unecessary now\n",
    "\n",
    "#if os.path.isfile('Features/feature_nlp.csv'):\n",
    "#    df_nlp = pd.read_csv(\"Features/feature_nlp.csv\",encoding='latin-1')\n",
    "#    df_nlp = df_nlp.drop(['qid1','qid2','question1','question2','is_duplicate'], axis=1)\n",
    "#    df_nlp.to_csv(\"Features/feature_nlp.csv\", index=False)\n",
    "#else:\n",
    "#    print('There is no feature_nlp.csv!')\n",
    "#df_nlp.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering on TFIDF weighted Word-Vector (Vector Features)\n",
    "\n",
    "Get the Extracting faltted tfidf-based vectors as a feature of every question.    \n",
    "I use en_core_web_sm package from spacy package (Industrial-Strength package for Natural Language Processing) to do the word-to-vec process. \n",
    "\n",
    "- Why TFIDF based? I use the idea of Smooth Inverse Frequency, to get every word a weight of tfidf.\n",
    "- Because the huge size of test data, 'en_core_web_md' can not run locally in my computer, so I run the en_core_web_md version in the Google Colab in `feature_engineering_test_md.ipynb`\n",
    "- Here I retain the 'en_core_web_sm'. en_core_web_sm is English multi-task CNN trained on OntoNotes while en_core_web_md is English multi-task CNN trained on OntoNotes, with GloVe vectors trained on Common Crawl.\n",
    "\n",
    "- Ouput: `feature_vectors.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import time\n",
    "import warnings\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import sys\n",
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>Which fish would survive in salt water?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  qid1  qid2                                          question1  \\\n",
       "0   0     1     2  What is the step by step guide to invest in sh...   \n",
       "1   1     3     4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2   2     5     6  How can I increase the speed of my internet co...   \n",
       "3   3     7     8  Why am I mentally very lonely? How can I solve...   \n",
       "4   4     9    10  Which one dissolve in water quikly sugar, salt...   \n",
       "\n",
       "                                           question2  is_duplicate  \n",
       "0  What is the step by step guide to invest in sh...             0  \n",
       "1  What would happen if the Indian government sto...             0  \n",
       "2  How can Internet speed be increased by hacking...             0  \n",
       "3  Find the remainder when [math]23^{24}[/math] i...             0  \n",
       "4            Which fish would survive in salt water?             0  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "\n",
    "df = pd.read_csv(\"Data/train.csv\")\n",
    " \n",
    "df['question1'] = df['question1'].apply(lambda x: str(x))\n",
    "df['question2'] = df['question2'].apply(lambda x: str(x))\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get TFIDF values of each question pair\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Merge question texts\n",
    "questions = list(df['question1']) + list(df['question2'])\n",
    "\n",
    "# Vectorizer = CountVectorizer+Transformer\n",
    "tfidf = TfidfVectorizer(lowercase=False,)\n",
    "tfidf.fit_transform(questions)\n",
    "\n",
    "# Here, dictionary: {key:word} = {value: tf-idf-value}\n",
    "word2tfidf = dict(zip(tfidf.get_feature_names(), tfidf.idf_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 404290/404290 [45:25<00:00, 148.35it/s] \n"
     ]
    }
   ],
   "source": [
    "# en_vectors_web_md, which includes over 1 million unique vectors.\n",
    "import en_core_web_md\n",
    "\n",
    "# en_vectors_web_md, which includes over 1 million unique vectors.\n",
    "nlp = en_core_web_md.load()\n",
    "\n",
    "vecs_1 = []\n",
    "\n",
    "for question_1 in tqdm(list(df['question1'])):  # tqdm is a progress bar\n",
    "    doc_1 = nlp(question_1) \n",
    "    # mean_vec1 = []\n",
    "    mean_vec_1 = np.zeros([len(doc_1), 300])  # in en_core_web_md, the output len of vector is 300\n",
    "    for word_1 in doc_1: \n",
    "        # word2vec\n",
    "        vec_1 = word_1.vector\n",
    "        # fetch df score\n",
    "        try:\n",
    "            idf = word2tfidf[str(word_1)]  # search for tfidf value in the dictionary\n",
    "        except:\n",
    "            idf = 0\n",
    "        # compute final vec\n",
    "        mean_vec_1 += vec_1 * idf\n",
    "        # mean_vec1.append(vec1 * idf)\n",
    "    mean_vec_1 = mean_vec_1.mean(axis=0)\n",
    "    # mean_vec1 = np.array(mean_vec1.mean(axis=0))\n",
    "    vecs_1.append(mean_vec_1)\n",
    "df['q1_vector_features'] = list(vecs_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 404290/404290 [46:13<00:00, 145.79it/s]\n"
     ]
    }
   ],
   "source": [
    "import en_core_web_md\n",
    "nlp = en_core_web_md.load()\n",
    "vecs_2 = []\n",
    "\n",
    "for question_2 in tqdm(list(df['question2'])):  # tqdm is a progress bar\n",
    "    doc_2 = nlp(question_2) \n",
    "    # mean_vec1 = []\n",
    "    mean_vec_2 = np.zeros([len(doc_2), 300])  # in en_core_web_md, the output len of vector is 300\n",
    "    for word_2 in doc_2: \n",
    "        # word2vec\n",
    "        vec_2 = word_2.vector\n",
    "        # fetch df score\n",
    "        try:\n",
    "            idf = word2tfidf[str(word_2)]\n",
    "        except:\n",
    "            idf = 0\n",
    "        # compute final vec\n",
    "        mean_vec_2 += vec_2 * idf\n",
    "        # mean_vec1.append(vec1 * idf)\n",
    "    mean_vec_2 = mean_vec_2.mean(axis=0)\n",
    "    # mean_vec1 = np.array(mean_vec1.mean(axis=0))\n",
    "    vecs_2.append(mean_vec_2)\n",
    "df['q2_vector_features'] = list(vecs_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                    0\n",
       "qid1                  0\n",
       "qid2                  0\n",
       "question1             0\n",
       "question2             0\n",
       "is_duplicate          0\n",
       "q1_vector_features    0\n",
       "q2_vector_features    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check nan values\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(404290, 601)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>0_x</th>\n",
       "      <th>1_x</th>\n",
       "      <th>2_x</th>\n",
       "      <th>3_x</th>\n",
       "      <th>4_x</th>\n",
       "      <th>5_x</th>\n",
       "      <th>6_x</th>\n",
       "      <th>7_x</th>\n",
       "      <th>8_x</th>\n",
       "      <th>...</th>\n",
       "      <th>290_y</th>\n",
       "      <th>291_y</th>\n",
       "      <th>292_y</th>\n",
       "      <th>293_y</th>\n",
       "      <th>294_y</th>\n",
       "      <th>295_y</th>\n",
       "      <th>296_y</th>\n",
       "      <th>297_y</th>\n",
       "      <th>298_y</th>\n",
       "      <th>299_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-5.856872</td>\n",
       "      <td>17.449559</td>\n",
       "      <td>4.862720</td>\n",
       "      <td>7.971019</td>\n",
       "      <td>20.345586</td>\n",
       "      <td>-5.514759</td>\n",
       "      <td>-4.077800</td>\n",
       "      <td>-2.820742</td>\n",
       "      <td>8.029026</td>\n",
       "      <td>...</td>\n",
       "      <td>-17.810438</td>\n",
       "      <td>7.231024</td>\n",
       "      <td>1.531186</td>\n",
       "      <td>-7.528823</td>\n",
       "      <td>0.473802</td>\n",
       "      <td>-11.864658</td>\n",
       "      <td>-11.293788</td>\n",
       "      <td>1.866265</td>\n",
       "      <td>3.616046</td>\n",
       "      <td>11.971096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-7.241549</td>\n",
       "      <td>10.424829</td>\n",
       "      <td>13.273801</td>\n",
       "      <td>-5.574235</td>\n",
       "      <td>5.964726</td>\n",
       "      <td>0.898817</td>\n",
       "      <td>4.561782</td>\n",
       "      <td>-11.213664</td>\n",
       "      <td>1.063151</td>\n",
       "      <td>...</td>\n",
       "      <td>18.333288</td>\n",
       "      <td>4.940264</td>\n",
       "      <td>-19.087384</td>\n",
       "      <td>1.978918</td>\n",
       "      <td>25.153889</td>\n",
       "      <td>1.649467</td>\n",
       "      <td>-10.371059</td>\n",
       "      <td>9.524476</td>\n",
       "      <td>-4.186575</td>\n",
       "      <td>24.111837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.909520</td>\n",
       "      <td>16.050299</td>\n",
       "      <td>-8.126856</td>\n",
       "      <td>-4.848289</td>\n",
       "      <td>-2.806190</td>\n",
       "      <td>9.752280</td>\n",
       "      <td>4.349992</td>\n",
       "      <td>-5.120332</td>\n",
       "      <td>6.785252</td>\n",
       "      <td>...</td>\n",
       "      <td>-24.310109</td>\n",
       "      <td>-1.216773</td>\n",
       "      <td>11.909693</td>\n",
       "      <td>9.591573</td>\n",
       "      <td>11.846737</td>\n",
       "      <td>1.397859</td>\n",
       "      <td>6.454157</td>\n",
       "      <td>-0.271460</td>\n",
       "      <td>-12.500337</td>\n",
       "      <td>27.634567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>-4.950745</td>\n",
       "      <td>17.098874</td>\n",
       "      <td>-15.474965</td>\n",
       "      <td>1.044680</td>\n",
       "      <td>-2.392017</td>\n",
       "      <td>-0.051889</td>\n",
       "      <td>2.650595</td>\n",
       "      <td>-8.451192</td>\n",
       "      <td>2.584123</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.435584</td>\n",
       "      <td>1.672591</td>\n",
       "      <td>-0.863278</td>\n",
       "      <td>-2.906553</td>\n",
       "      <td>-3.466688</td>\n",
       "      <td>-3.867892</td>\n",
       "      <td>-4.249463</td>\n",
       "      <td>-12.551012</td>\n",
       "      <td>4.494087</td>\n",
       "      <td>-6.223341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>-8.738103</td>\n",
       "      <td>21.689450</td>\n",
       "      <td>10.167188</td>\n",
       "      <td>-7.766195</td>\n",
       "      <td>-21.347514</td>\n",
       "      <td>17.447355</td>\n",
       "      <td>-19.232780</td>\n",
       "      <td>-1.405518</td>\n",
       "      <td>-21.179671</td>\n",
       "      <td>...</td>\n",
       "      <td>-10.407441</td>\n",
       "      <td>-8.444207</td>\n",
       "      <td>-14.450059</td>\n",
       "      <td>-12.709382</td>\n",
       "      <td>-4.449050</td>\n",
       "      <td>12.563987</td>\n",
       "      <td>-11.721362</td>\n",
       "      <td>-16.459300</td>\n",
       "      <td>3.626297</td>\n",
       "      <td>-9.790615</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 601 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id       0_x        1_x        2_x       3_x        4_x        5_x  \\\n",
       "0   0 -5.856872  17.449559   4.862720  7.971019  20.345586  -5.514759   \n",
       "1   1 -7.241549  10.424829  13.273801 -5.574235   5.964726   0.898817   \n",
       "2   2  0.909520  16.050299  -8.126856 -4.848289  -2.806190   9.752280   \n",
       "3   3 -4.950745  17.098874 -15.474965  1.044680  -2.392017  -0.051889   \n",
       "4   4 -8.738103  21.689450  10.167188 -7.766195 -21.347514  17.447355   \n",
       "\n",
       "         6_x        7_x        8_x  ...      290_y     291_y      292_y  \\\n",
       "0  -4.077800  -2.820742   8.029026  ... -17.810438  7.231024   1.531186   \n",
       "1   4.561782 -11.213664   1.063151  ...  18.333288  4.940264 -19.087384   \n",
       "2   4.349992  -5.120332   6.785252  ... -24.310109 -1.216773  11.909693   \n",
       "3   2.650595  -8.451192   2.584123  ...  -5.435584  1.672591  -0.863278   \n",
       "4 -19.232780  -1.405518 -21.179671  ... -10.407441 -8.444207 -14.450059   \n",
       "\n",
       "       293_y      294_y      295_y      296_y      297_y      298_y      299_y  \n",
       "0  -7.528823   0.473802 -11.864658 -11.293788   1.866265   3.616046  11.971096  \n",
       "1   1.978918  25.153889   1.649467 -10.371059   9.524476  -4.186575  24.111837  \n",
       "2   9.591573  11.846737   1.397859   6.454157  -0.271460 -12.500337  27.634567  \n",
       "3  -2.906553  -3.466688  -3.867892  -4.249463 -12.551012   4.494087  -6.223341  \n",
       "4 -12.709382  -4.449050  12.563987 -11.721362 -16.459300   3.626297  -9.790615  \n",
       "\n",
       "[5 rows x 601 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Flat vectors and merge together\n",
    "\n",
    "# flat\n",
    "columns_1 = ['0_x','1_x','2_x','3_x','4_x','5_x','6_x','7_x','8_x','9_x','10_x','11_x','12_x','13_x','14_x','15_x','16_x','17_x','18_x','19_x','20_x','21_x','22_x','23_x','24_x','25_x','26_x','27_x','28_x','29_x','30_x','31_x','32_x','33_x','34_x','35_x','36_x','37_x','38_x','39_x','40_x','41_x','42_x','43_x','44_x','45_x','46_x','47_x','48_x','49_x','50_x','51_x','52_x','53_x','54_x','55_x','56_x','57_x','58_x','59_x','60_x','61_x','62_x','63_x','64_x','65_x','66_x','67_x','68_x','69_x','70_x','71_x','72_x','73_x','74_x','75_x','76_x','77_x','78_x','79_x','80_x','81_x','82_x','83_x','84_x','85_x','86_x','87_x','88_x','89_x','90_x','91_x','92_x','93_x','94_x','95_x','96_x','97_x','98_x','99_x','100_x','101_x','102_x','103_x','104_x','105_x','106_x','107_x','108_x','109_x','110_x','111_x','112_x','113_x','114_x','115_x','116_x','117_x','118_x','119_x','120_x','121_x','122_x','123_x','124_x','125_x','126_x','127_x','128_x','129_x','130_x','131_x','132_x','133_x','134_x','135_x','136_x','137_x','138_x','139_x','140_x','141_x','142_x','143_x','144_x','145_x','146_x','147_x','148_x','149_x','150_x','151_x','152_x','153_x','154_x','155_x','156_x','157_x','158_x','159_x','160_x','161_x','162_x','163_x','164_x','165_x','166_x','167_x','168_x','169_x','170_x','171_x','172_x','173_x','174_x','175_x','176_x','177_x','178_x','179_x','180_x','181_x','182_x','183_x','184_x','185_x','186_x','187_x','188_x','189_x','190_x','191_x','192_x','193_x','194_x','195_x','196_x','197_x','198_x','199_x','200_x','201_x','202_x','203_x','204_x','205_x','206_x','207_x','208_x','209_x','210_x','211_x','212_x','213_x','214_x','215_x','216_x','217_x','218_x','219_x','220_x','221_x','222_x','223_x','224_x','225_x','226_x','227_x','228_x','229_x','230_x','231_x','232_x','233_x','234_x','235_x','236_x','237_x','238_x','239_x','240_x','241_x','242_x','243_x','244_x','245_x','246_x','247_x','248_x','249_x','250_x','251_x','252_x','253_x','254_x','255_x','256_x','257_x','258_x','259_x','260_x','261_x','262_x','263_x','264_x','265_x','266_x','267_x','268_x','269_x','270_x','271_x','272_x','273_x','274_x','275_x','276_x','277_x','278_x','279_x','280_x','281_x','282_x','283_x','284_x','285_x','286_x','287_x','288_x','289_x','290_x','291_x','292_x','293_x','294_x','295_x','296_x','297_x','298_x','299_x']\n",
    "columns_2 = ['0_y','1_y','2_y','3_y','4_y','5_y','6_y','7_y','8_y','9_y','10_y','11_y','12_y','13_y','14_y','15_y','16_y','17_y','18_y','19_y','20_y','21_y','22_y','23_y','24_y','25_y','26_y','27_y','28_y','29_y','30_y','31_y','32_y','33_y','34_y','35_y','36_y','37_y','38_y','39_y','40_y','41_y','42_y','43_y','44_y','45_y','46_y','47_y','48_y','49_y','50_y','51_y','52_y','53_y','54_y','55_y','56_y','57_y','58_y','59_y','60_y','61_y','62_y','63_y','64_y','65_y','66_y','67_y','68_y','69_y','70_y','71_y','72_y','73_y','74_y','75_y','76_y','77_y','78_y','79_y','80_y','81_y','82_y','83_y','84_y','85_y','86_y','87_y','88_y','89_y','90_y','91_y','92_y','93_y','94_y','95_y','96_y','97_y','98_y','99_y','100_y','101_y','102_y','103_y','104_y','105_y','106_y','107_y','108_y','109_y','110_y','111_y','112_y','113_y','114_y','115_y','116_y','117_y','118_y','119_y','120_y','121_y','122_y','123_y','124_y','125_y','126_y','127_y','128_y','129_y','130_y','131_y','132_y','133_y','134_y','135_y','136_y','137_y','138_y','139_y','140_y','141_y','142_y','143_y','144_y','145_y','146_y','147_y','148_y','149_y','150_y','151_y','152_y','153_y','154_y','155_y','156_y','157_y','158_y','159_y','160_y','161_y','162_y','163_y','164_y','165_y','166_y','167_y','168_y','169_y','170_y','171_y','172_y','173_y','174_y','175_y','176_y','177_y','178_y','179_y','180_y','181_y','182_y','183_y','184_y','185_y','186_y','187_y','188_y','189_y','190_y','191_y','192_y','193_y','194_y','195_y','196_y','197_y','198_y','199_y','200_y','201_y','202_y','203_y','204_y','205_y','206_y','207_y','208_y','209_y','210_y','211_y','212_y','213_y','214_y','215_y','216_y','217_y','218_y','219_y','220_y','221_y','222_y','223_y','224_y','225_y','226_y','227_y','228_y','229_y','230_y','231_y','232_y','233_y','234_y','235_y','236_y','237_y','238_y','239_y','240_y','241_y','242_y','243_y','244_y','245_y','246_y','247_y','248_y','249_y','250_y','251_y','252_y','253_y','254_y','255_y','256_y','257_y','258_y','259_y','260_y','261_y','262_y','263_y','264_y','265_y','266_y','267_y','268_y','269_y','270_y','271_y','272_y','273_y','274_y','275_y','276_y','277_y','278_y','279_y','280_y','281_y','282_y','283_y','284_y','285_y','286_y','287_y','288_y','289_y','290_y','291_y','292_y','293_y','294_y','295_y','296_y','297_y','298_y','299_y']\n",
    "df_temp = df\n",
    "df_temp = df_temp.drop(['qid1','qid2','question1','question2','is_duplicate','q1_vector_features','q2_vector_features'],axis=1)\n",
    "df_q1 = pd.DataFrame(df.q1_vector_features.values.tolist(), index= df.index, columns=columns_1)  # word-vector features\n",
    "df_q2 = pd.DataFrame(df.q2_vector_features.values.tolist(), index= df.index, columns=columns_2)  # word-vector features\n",
    "df_q1['id'] = df['id']\n",
    "df_q2['id'] = df['id']\n",
    "\n",
    "# merge\n",
    "df_vectors = df_temp.merge(df_q1, on='id', how='left')\n",
    "df_vectors = df_vectors.merge(df_q2, on='id', how='left')\n",
    "\n",
    "print(df_vectors.shape)\n",
    "df_vectors.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id       0\n",
       "0_x      0\n",
       "1_x      0\n",
       "2_x      0\n",
       "3_x      0\n",
       "        ..\n",
       "295_y    0\n",
       "296_y    0\n",
       "297_y    0\n",
       "298_y    0\n",
       "299_y    0\n",
       "Length: 601, dtype: int64"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check again because of the left join\n",
    "\n",
    "df_vectors.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tfidf weighted word2vector features...\n"
     ]
    }
   ],
   "source": [
    "# Output/Load \n",
    "\n",
    "if os.path.isfile('Features/feature_vectors.csv'):\n",
    "    df_vectors = pd.read_csv(\"Features/feature_vectors.csv\",encoding='latin-1')\n",
    "else:\n",
    "    print(\"Extracting tfidf weighted word2vector features...\")\n",
    "    # If there are no existing file then create a csv file, make sure you have run the previous code in 1.4 chapter\n",
    "    df_vectors.to_csv('Features/feature_vectors.csv')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****LOOK Due to limited computing resources of my laptop, i couldn't merge features from en_core_web_md (which is 300 dimensions), so in the `modeling.ipynb` i used features from en_core_web_sm (have tried before and saved as features locally). so if you want to rebuild my project results, you should change the model to `en_core_web_sm`. ****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering on Similarity Measurements\n",
    "\n",
    "Extracting sinmilarity measurements as a supplement of features. **Or**, they can be used in the model stacking because each one of the similarity measurements can be a independent creteria of duplicated/not duplicarted (i.e. similarities)\n",
    "\n",
    "- tf/tfidf cosine similarity (cosine distance actually): used\n",
    "- jaccord similarity (distance actually): used\n",
    "- simhash： \n",
    "  - Thesis reference: Detecting Near-duplicates for web crawling`\n",
    "  - https://leons.im/posts/a-python-implementation-of-simhash-algorithm/\n",
    "- LSI vetor: I was intended to use, but after reading thesis and papers i found that the usage of this algo is to find the lsi vector similarity of test text compared to the topic based model trained  on corpus (large amount of data). I think it can't be used here.\n",
    "  - LSI uses\n",
    "  - LSA(latent semantic analysis) also known as LSI(latent semantic index)，put forward by Scott Deerwester, Susan T. Dumais\n",
    "  - Deerwester, S., Dumais, S. T., Furnas, G. W., Landauer, T. K., & Harshman, R.(1990). Indexing By Latent Semantic Analysis. Journal of the American Society For Information Science, 41, 391-407. 10\n",
    "  - https://blog.csdn.net/qq_34333481/article/details/85014010\n",
    "\n",
    "- Output: features_similarity.csv\n",
    "- **This part is an addition to the model in STACKING step. For now it is not in the final features.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Data/train.csv\")\n",
    "df['question1'] = df['question1'].apply(lambda x: str(x))\n",
    "df['question2'] = df['question2'].apply(lambda x: str(x))\n",
    "# df['text'] = [df.question1, df.question2]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jaccard similarity based on tfidf vectors\n",
    "\n",
    "def jaccard_similarity_tfidf(s1, s2):\n",
    "    def add_space(s):\n",
    "        return ''.join(list(s))\n",
    "    \n",
    "    s1, s2 = add_space(s1), add_space(s2)\n",
    "    # convert into tfidf matrix\n",
    "    # print(s1)\n",
    "    cv = CountVectorizer(tokenizer=lambda s: s.split())\n",
    "    corpus = [s1, s2]\n",
    "    vectors = cv.fit_transform(corpus).toarray()\n",
    "    # intersection of tfidf matrix\n",
    "    numerator = np.sum(np.min(vectors, axis=0))\n",
    "    # union of tfidf matrix\n",
    "    denominator = np.sum(np.max(vectors, axis=0))\n",
    "    # calculate jaccard similarity\n",
    "    return 1.0 * numerator / denominator\n",
    "df_sim = df.copy()\n",
    "df_sim['jcs_tfidf_sim'] = df_sim.apply(lambda x: jaccard_similarity_tfidf(x['question1'],x['question2']), axis=1)\n",
    "df_sim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jaccard similarity\n",
    "\n",
    "def jaccard_similarity(s1, s2):\n",
    "    a = set(s1.split()) \n",
    "    b = set(s2.split())\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))\n",
    "\n",
    "df_sim['jcs_sim'] = df_sim.apply(lambda x: jaccard_similarity(x.loc['question1'],x.loc['question2']), axis=1)\n",
    "df_sim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tf vector cosine similarity\n",
    "\n",
    "from scipy.linalg import norm\n",
    "def tf_vector_similarity(s1, s2):\n",
    "    def add_space(s):\n",
    "        return ''.join(list(s))\n",
    "\n",
    "    s1, s2 = add_space(s1), add_space(s2)\n",
    "    # convert into tfidf matrix\n",
    "    cv = CountVectorizer(tokenizer=lambda s: s.split())\n",
    "    corpus = [s1, s2]\n",
    "    vectors = cv.fit_transform(corpus).toarray()\n",
    "    # calculate tf vector distance by cosine distance\n",
    "    return np.dot(vectors[0], vectors[1]) / (norm(vectors[0]) * norm(vectors[1]))\n",
    "\n",
    "df_sim['tf_sim'] = df_temp.apply(lambda x: tf_vector_similarity(x.loc['question1'],x.loc['question2']), axis=1)\n",
    "df_sim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf vector similarity\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "def tfidf_similarity(s1, s2):\n",
    "    def add_space(s):\n",
    "        return ''.join(list(s))\n",
    "    \n",
    "    s1, s2 = add_space(s1), add_space(s2)\n",
    "    # convert into tfidf matrix\n",
    "    cv = TfidfVectorizer(tokenizer=lambda s: s.split())\n",
    "    corpus = [s1, s2]\n",
    "    vectors = cv.fit_transform(corpus).toarray()\n",
    "    # calculate tfidf vector distance by cosine distance\n",
    "    return np.dot(vectors[0], vectors[1]) / (norm(vectors[0]) * norm(vectors[1]))\n",
    "\n",
    "df_sim['tfidf_similarity'] = df_sim.apply(lambda x: tfidf_similarity(x.loc['question1'],x.loc['question2']), axis=1)\n",
    "df_sim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n"
     ]
    }
   ],
   "source": [
    "# Simhash similarity\n",
    "\n",
    "import re\n",
    "from simhash import Simhash\n",
    "\n",
    "def simhash_similarity(s1,s2):\n",
    "    def add_space(s):\n",
    "        return ''.join(list(s))\n",
    "\n",
    "    def get_features(s):\n",
    "        width = 3\n",
    "        s = s.lower()\n",
    "        s = re.sub(r'[^\\w]+', '', s)\n",
    "        return [s[i:i + width] for i in range(max(len(s) - width + 1, 1))]\n",
    "    \n",
    "    s1, s2 = add_space(s1), add_space(s2)\n",
    "    return Simhash(get_features(s1)).distance(Simhash(get_features(s2)))\n",
    "\n",
    "\n",
    "df_sim['sh_similarity'] = df_sim.apply(lambda x: simhash_similarity(x.loc['question1'],x.loc['question2']), axis=1)\n",
    "df_sim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering on Other Vectors\n",
    "\n",
    "Extracting other vectors, maybe by expanding the len(features_fianl) the result will be better. But I am not doing this right now. Need experiments.\n",
    "\n",
    "- avg_w2v: (glove based) can expand, it is more recommended now since it is directly from gLOVe pretrained model, so maybe this can conncect with LSTM with glove model.\n",
    "https://cloud.tencent.com/developer/article/1145941\n",
    "- tfidf vectors can expand\n",
    "  - When modeling, TFIDF features don't need to scale since it has regularized in the extracting proces\n",
    "- Doc2Vec: gensim\n",
    "- Word2Vec: gensim, average vector of all words in a sentence as the vector of the sentence.\n",
    "\n",
    "\n",
    "- **This part is an addition to the model, i am not goona put it in the model for now.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
