{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering for Train_data for  `Quora Question pair similarity`\n",
    "\n",
    "- This is the **2nd iteration**, the change is focused on the feature_tm and feature_nlp\n",
    "- Feature engineering of training data.\n",
    "- Extracting features according to the order of csv file below.\n",
    "\n",
    "\n",
    "\n",
    "- Input: `test.csv`\n",
    "- Output: `feature_tm_test.csv`, `feature_nlp_test.csv`, `feature_vectors_test.csv`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from subprocess import check_output\n",
    "%matplotlib inline\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "import os\n",
    "import gc\n",
    "\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import distance\n",
    "from nltk.stem import PorterStemmer\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import distance\n",
    "from nltk.stem import PorterStemmer\n",
    "from bs4 import BeautifulSoup\n",
    "from fuzzywuzzy import fuzz\n",
    "from sklearn.manifold import TSNE\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from os import path\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_id</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>How does the Surface Pro himself 4 compare wit...</td>\n",
       "      <td>Why did Microsoft choose core m3 and not core ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Should I have a hair transplant at age 24? How...</td>\n",
       "      <td>How much cost does hair transplant require?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>What but is the best way to send money from Ch...</td>\n",
       "      <td>What you send money to China?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Which food not emulsifiers?</td>\n",
       "      <td>What foods fibre?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>How \"aberystwyth\" start reading?</td>\n",
       "      <td>How their can I start reading?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   test_id                                          question1  \\\n",
       "0        0  How does the Surface Pro himself 4 compare wit...   \n",
       "1        1  Should I have a hair transplant at age 24? How...   \n",
       "2        2  What but is the best way to send money from Ch...   \n",
       "3        3                        Which food not emulsifiers?   \n",
       "4        4                   How \"aberystwyth\" start reading?   \n",
       "\n",
       "                                           question2  \n",
       "0  Why did Microsoft choose core m3 and not core ...  \n",
       "1        How much cost does hair transplant require?  \n",
       "2                      What you send money to China?  \n",
       "3                                  What foods fibre?  \n",
       "4                     How their can I start reading?  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load train data from csv file\n",
    "\n",
    "df = pd.read_csv(\"Data/test.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2345796 entries, 0 to 2345795\n",
      "Data columns (total 3 columns):\n",
      "test_id      int64\n",
      "question1    object\n",
      "question2    object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 53.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are NaN data rows:\n",
      "         test_id                                    question1  \\\n",
      "379205    379205     How I can learn android app development?   \n",
      "817520    817520  How real can learn android app development?   \n",
      "943911    943911                         How app development?   \n",
      "1046690  1046690                                          NaN   \n",
      "1270024  1270024             How I can learn app development?   \n",
      "1461432  1461432                                          NaN   \n",
      "\n",
      "                                               question2  \n",
      "379205                                               NaN  \n",
      "817520                                               NaN  \n",
      "943911                                               NaN  \n",
      "1046690    How I what can learn android app development?  \n",
      "1270024                                              NaN  \n",
      "1461432  How distinct can learn android app development?  \n",
      "---------------Now start data cleansing for NaN values:-------------\n",
      "Here are NaN data rows:\n",
      "Empty DataFrame\n",
      "Columns: [test_id, question1, question2]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Data preprocessing: deal with null values\n",
    "\n",
    "nan_data = df[df.isnull().any(1)]\n",
    "print(\"Here are NaN data rows:\")\n",
    "print(nan_data)\n",
    "print(\"---------------Now start data cleansing for NaN values:-------------\")\n",
    "df = df.fillna('')\n",
    "nan_data = df[df.isnull().any(1)]\n",
    "print(\"Here are NaN data rows:\")\n",
    "print(nan_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Enginerring on Text Mining Features\n",
    "\n",
    "Extract text mining or statistical features from training data.\n",
    " - ___q1len___ = Length of q1\n",
    " - ___q2len___ = Length of q2\n",
    " - ___diff_len___ = len(q1)-len(q2)       \n",
    "\n",
    "\n",
    " - ___q1_n_words___ = Number of words in q1\n",
    " - ___q2_n_words___ = Number of words in q2\n",
    " - ___diff_n_words___ = The difference       \n",
    "\n",
    "\n",
    " - ___caps_count_q1___ = Number of capital words of q1\n",
    " - ___caps_count_q2___ = Number of capital words of q2\n",
    " - ___diff_caps___ = The difference       \n",
    "\n",
    "\n",
    " - ___len_char_q1___ = Number of characters of q1\n",
    " - ___len_char_q2___ = Number of characters of q2\n",
    " - ___diff_len_char___ = The difference      \n",
    "\n",
    "\n",
    " - ___avg_word_len1___ = len(char)/len(word) of q1\n",
    " - ___avg_word_len2___ = len(char)/len(word) of q2\n",
    " - ___diff_avg_word___ = The difference      \n",
    "\n",
    "\n",
    " - ___word_Common___ = Number of common unique words in q1 and q2\n",
    " - ___word_Total___ = Total num of words in Question 1 + Total num of words in q2\n",
    " - ___word_share___ = (word_common)/(word_Total)    \n",
    " - ___2_gram_share___ = word share on 2 gram\n",
    "\n",
    "\n",
    " - ___exactly_same___ = exactly the same\n",
    "\n",
    "\n",
    " \n",
    " \n",
    " - **Ouput: feature_tm.csv**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_id</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>q1len</th>\n",
       "      <th>q2len</th>\n",
       "      <th>diff_len</th>\n",
       "      <th>len_word_q1</th>\n",
       "      <th>len_word_q2</th>\n",
       "      <th>diff_words</th>\n",
       "      <th>caps_count_q1</th>\n",
       "      <th>...</th>\n",
       "      <th>len_char_q1</th>\n",
       "      <th>len_char_q2</th>\n",
       "      <th>diff_len_char</th>\n",
       "      <th>avg_world_len1</th>\n",
       "      <th>avg_world_len2</th>\n",
       "      <th>diff_avg_word</th>\n",
       "      <th>word_Common</th>\n",
       "      <th>word_Total</th>\n",
       "      <th>word_share</th>\n",
       "      <th>share_2_gram</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>How does the Surface Pro himself 4 compare wit...</td>\n",
       "      <td>Why did Microsoft choose core m3 and not core ...</td>\n",
       "      <td>57</td>\n",
       "      <td>68</td>\n",
       "      <td>-11</td>\n",
       "      <td>11</td>\n",
       "      <td>14</td>\n",
       "      <td>-3</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>47</td>\n",
       "      <td>55</td>\n",
       "      <td>-8</td>\n",
       "      <td>4.272727</td>\n",
       "      <td>3.928571</td>\n",
       "      <td>0.344156</td>\n",
       "      <td>2.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.043478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Should I have a hair transplant at age 24? How...</td>\n",
       "      <td>How much cost does hair transplant require?</td>\n",
       "      <td>66</td>\n",
       "      <td>43</td>\n",
       "      <td>23</td>\n",
       "      <td>14</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>53</td>\n",
       "      <td>37</td>\n",
       "      <td>16</td>\n",
       "      <td>3.785714</td>\n",
       "      <td>5.285714</td>\n",
       "      <td>-1.500000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>0.105263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>What but is the best way to send money from Ch...</td>\n",
       "      <td>What you send money to China?</td>\n",
       "      <td>60</td>\n",
       "      <td>29</td>\n",
       "      <td>31</td>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>47</td>\n",
       "      <td>24</td>\n",
       "      <td>23</td>\n",
       "      <td>3.357143</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>-0.642857</td>\n",
       "      <td>4.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.055556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Which food not emulsifiers?</td>\n",
       "      <td>What foods fibre?</td>\n",
       "      <td>27</td>\n",
       "      <td>17</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>24</td>\n",
       "      <td>15</td>\n",
       "      <td>9</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>How \"aberystwyth\" start reading?</td>\n",
       "      <td>How their can I start reading?</td>\n",
       "      <td>32</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>29</td>\n",
       "      <td>25</td>\n",
       "      <td>4</td>\n",
       "      <td>7.250000</td>\n",
       "      <td>4.166667</td>\n",
       "      <td>3.083333</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   test_id                                          question1  \\\n",
       "0        0  How does the Surface Pro himself 4 compare wit...   \n",
       "1        1  Should I have a hair transplant at age 24? How...   \n",
       "2        2  What but is the best way to send money from Ch...   \n",
       "3        3                        Which food not emulsifiers?   \n",
       "4        4                   How \"aberystwyth\" start reading?   \n",
       "\n",
       "                                           question2  q1len  q2len  diff_len  \\\n",
       "0  Why did Microsoft choose core m3 and not core ...     57     68       -11   \n",
       "1        How much cost does hair transplant require?     66     43        23   \n",
       "2                      What you send money to China?     60     29        31   \n",
       "3                                  What foods fibre?     27     17        10   \n",
       "4                     How their can I start reading?     32     30         2   \n",
       "\n",
       "   len_word_q1  len_word_q2  diff_words  caps_count_q1  ...  len_char_q1  \\\n",
       "0           11           14          -3              5  ...           47   \n",
       "1           14            7           7              3  ...           53   \n",
       "2           14            6           8              4  ...           47   \n",
       "3            4            3           1              1  ...           24   \n",
       "4            4            6          -2              1  ...           29   \n",
       "\n",
       "   len_char_q2  diff_len_char  avg_world_len1  avg_world_len2  diff_avg_word  \\\n",
       "0           55             -8        4.272727        3.928571       0.344156   \n",
       "1           37             16        3.785714        5.285714      -1.500000   \n",
       "2           24             23        3.357143        4.000000      -0.642857   \n",
       "3           15              9        6.000000        5.000000       1.000000   \n",
       "4           25              4        7.250000        4.166667       3.083333   \n",
       "\n",
       "   word_Common  word_Total  word_share  share_2_gram  \n",
       "0          2.0        24.0    0.083333      0.043478  \n",
       "1          4.0        21.0    0.190476      0.105263  \n",
       "2          4.0        18.0    0.222222      0.055556  \n",
       "3          0.0         7.0    0.000000      0.000000  \n",
       "4          3.0        10.0    0.300000      0.125000  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if os.path.isfile('Features/feature_tm_test.csv'):\n",
    "    df = pd.read_csv(\"Features/feature_tm_test.csv\",encoding='latin-1')\n",
    "else:\n",
    "    df['q1len'] = df['question1'].str.len() \n",
    "    df['q2len'] = df['question2'].str.len()\n",
    "    df['diff_len'] = df['q1len'] - df['q2len']\n",
    "    \n",
    "    df['len_word_q1'] = df['question1'].apply(lambda row: len(row.split(\" \")))\n",
    "    df['len_word_q2'] = df['question2'].apply(lambda row: len(row.split(\" \")))\n",
    "    df['diff_words'] = df['len_word_q1'] - df['len_word_q2']\n",
    "    \n",
    "    df['caps_count_q1'] = df['question1'].apply(lambda x:sum(1 for i in str(x) if i.isupper()))\n",
    "    df['caps_count_q2'] = df['question2'].apply(lambda x:sum(1 for i in str(x) if i.isupper()))\n",
    "    df['diff_caps'] = df['caps_count_q1'] - df['caps_count_q2']\n",
    "    \n",
    "    df['len_char_q1'] = df['question1'].apply(lambda x: len(str(x).replace(' ', '')))\n",
    "    df['len_char_q2'] = df['question2'].apply(lambda x: len(str(x).replace(' ', '')))\n",
    "    df['diff_len_char'] = df['len_char_q1'] - df['len_char_q2']\n",
    "    \n",
    "    df['avg_world_len1'] = df['len_char_q1'] / df['len_word_q1']\n",
    "    df['avg_world_len2'] = df['len_char_q2'] / df['len_word_q2']\n",
    "    df['diff_avg_word'] = df['avg_world_len1'] - df['avg_world_len2']\n",
    "    \n",
    "\n",
    "    def normalized_word_Common(row):\n",
    "        w1 = set(map(lambda word: word.lower().strip(), row['question1'].split(\" \")))\n",
    "        w2 = set(map(lambda word: word.lower().strip(), row['question2'].split(\" \")))    \n",
    "        return 1.0 * len(w1 & w2)\n",
    "    df['word_Common'] = df.apply(normalized_word_Common, axis=1)\n",
    "\n",
    "    def normalized_word_Total(row):\n",
    "        w1 = set(map(lambda word: word.lower().strip(), row['question1'].split(\" \")))\n",
    "        w2 = set(map(lambda word: word.lower().strip(), row['question2'].split(\" \")))    \n",
    "        return 1.0 * (len(w1) + len(w2))\n",
    "    df['word_Total'] = df.apply(normalized_word_Total, axis=1)\n",
    "\n",
    "    def normalized_word_share(row):\n",
    "        w1 = set(map(lambda word: word.lower().strip(), row['question1'].split(\" \")))\n",
    "        w2 = set(map(lambda word: word.lower().strip(), row['question2'].split(\" \")))    \n",
    "        return 1.0 * len(w1 & w2)/(len(w1) + len(w2))\n",
    "    df['word_share'] = df.apply(normalized_word_share, axis=1)\n",
    "\n",
    "    def get_2_gram_share(row):\n",
    "        q1_list = str(row['question1']).lower().split()\n",
    "        q2_list = str(row['question2']).lower().split()\n",
    "        q1_2_gram = set([i for i in zip(q1_list, q1_list[1:])])\n",
    "        q2_2_gram = set([i for i in zip(q2_list, q2_list[1:])])\n",
    "        shared_2_gram = q1_2_gram.intersection(q2_2_gram)\n",
    "        if len(q1_2_gram) + len(q2_2_gram) == 0:\n",
    "            R2gram = 0\n",
    "        else:\n",
    "            R2gram = len(shared_2_gram) / (len(q1_2_gram) + len(q2_2_gram))\n",
    "        return R2gram\n",
    "    df['share_2_gram'] = df.apply(get_2_gram_share, axis=1) \n",
    "\n",
    "    df.to_csv(\"Features/feature_tm_test.csv\", index=False)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_id           0\n",
      "question1         2\n",
      "question2         4\n",
      "q1len             0\n",
      "q2len             0\n",
      "diff_len          0\n",
      "len_word_q1       0\n",
      "len_word_q2       0\n",
      "diff_words        0\n",
      "caps_count_q1     0\n",
      "caps_count_q2     0\n",
      "diff_caps         0\n",
      "len_char_q1       0\n",
      "len_char_q2       0\n",
      "diff_len_char     0\n",
      "avg_world_len1    0\n",
      "avg_world_len2    0\n",
      "diff_avg_word     0\n",
      "word_Common       0\n",
      "word_Total        0\n",
      "word_share        0\n",
      "share_2_gram      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check the NaN data\n",
    "\n",
    "if os.path.isfile('Features/feature_tm_test.csv'):\n",
    "    df_tm = pd.read_csv(\"Features/feature_tm_test.csv\",encoding='latin-1')\n",
    "    print(df_tm.isna().sum())\n",
    "    df_tm = df_tm.fillna('')\n",
    "    df_tm.head()\n",
    "else:\n",
    "    # If there are no existing file then you need to create a csv file, make sure you have run the previous code in 1.2 chapter\n",
    "    print(\"There is no Features/feature_tm_test.csv!\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_id           0\n",
      "question1         0\n",
      "question2         0\n",
      "q1len             0\n",
      "q2len             0\n",
      "diff_len          0\n",
      "len_word_q1       0\n",
      "len_word_q2       0\n",
      "diff_words        0\n",
      "caps_count_q1     0\n",
      "caps_count_q2     0\n",
      "diff_caps         0\n",
      "len_char_q1       0\n",
      "len_char_q2       0\n",
      "diff_len_char     0\n",
      "avg_world_len1    0\n",
      "avg_world_len2    0\n",
      "diff_avg_word     0\n",
      "word_Common       0\n",
      "word_Total        0\n",
      "word_share        0\n",
      "share_2_gram      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering on NLP Features\n",
    "\n",
    "Extracting NLP features, including:\n",
    "- Statistical features of NLP nouns like stop_word, token, substring etc\n",
    "- NLP distances\n",
    "- Fuzzy features\n",
    "\n",
    "Features:\n",
    "\n",
    "- __last_word_eq__ :  Check if Last word of both questions is equal or not<br>last_word_eq = int(q1_tokens[-1] == q2_tokens[-1])\n",
    "\n",
    "\n",
    "- __first_word_eq__ :  Check if First word of both questions is equal or not<br>first_word_eq = int(q1_tokens[0] == q2_tokens[0])\n",
    "\n",
    "\n",
    "- __abs_len_diff__ :  Abs. length difference<br>abs_len_diff = abs(len(q1_tokens) - len(q2_tokens))\n",
    "\n",
    "\n",
    "- __mean_len__ :  Average Token Length of both Questions<br>mean_len = (len(q1_tokens) + len(q2_tokens))/2\n",
    "\n",
    "- __cwc_min__ :  Ratio of common_word_count to min lenghth of word count of Q1 and Q2 <br>cwc_min = common_word_count / (min(len(q1_words), len(q2_words))\n",
    "\n",
    "\n",
    "- __cwc_max__ :  Ratio of common_word_count to max lenghth of word count of Q1 and Q2 <br>cwc_max = common_word_count / (max(len(q1_words), len(q2_words))\n",
    "\n",
    "\n",
    "- __csc_min__ :  Ratio of common_stop_count to min lenghth of stop count of Q1 and Q2 <br> csc_min = common_stop_count / (min(len(q1_stops), len(q2_stops))\n",
    "\n",
    "\n",
    "- __csc_max__ :  Ratio of common_stop_count to max lenghth of stop count of Q1 and Q2<br>csc_max = common_stop_count / (max(len(q1_stops), len(q2_stops))\n",
    "\n",
    "\n",
    "- __ctc_min__ :  Ratio of common_token_count to min lenghth of token count of Q1 and Q2<br>ctc_min = common_token_count / (min(len(q1_tokens), len(q2_tokens))\n",
    "\n",
    "\n",
    "- __ctc_max__ :  Ratio of common_token_count to max lenghth of token count of Q1 and Q2<br>ctc_max = common_token_count / (max(len(q1_tokens), len(q2_tokens))\n",
    "\n",
    "\n",
    "- __wmd_dist__: Thesis reference: http://proceedings.mlr.press/v37/kusnerb15.pdf \n",
    "\n",
    "\n",
    "- __cosine_dist__: Cosine distance here is the cosine distance between two glove based vectors. Different from cosine similarity of tf/tfidf in Chapter 1.5\n",
    "\n",
    "\n",
    "- __cityblock_dist__: just follow the official defination\n",
    "\n",
    "\n",
    "- __canberra_dist__: just follow the official defination\n",
    "\n",
    "\n",
    "- __euclidean_dist__: just follow the official defination\n",
    "\n",
    "\n",
    "- __minkowski_dist__: just follow the official defination\n",
    "\n",
    "\n",
    "- __fuzz_ratio__ :  https://github.com/seatgeek/fuzzywuzzy#usage\n",
    "http://chairnerd.seatgeek.com/fuzzywuzzy-fuzzy-string-matching-in-python/\n",
    "\n",
    "\n",
    "- __fuzz_partial_ratio__ :  https://github.com/seatgeek/fuzzywuzzy#usage\n",
    "http://chairnerd.seatgeek.com/fuzzywuzzy-fuzzy-string-matching-in-python/\n",
    "\n",
    "\n",
    "- __token_sort_ratio__ : https://github.com/seatgeek/fuzzywuzzy#usage\n",
    "http://chairnerd.seatgeek.com/fuzzywuzzy-fuzzy-string-matching-in-python/\n",
    "\n",
    "\n",
    "- __token_set_ratio__ : https://github.com/seatgeek/fuzzywuzzy#usage\n",
    "http://chairnerd.seatgeek.com/fuzzywuzzy-fuzzy-string-matching-in-python/\n",
    "\n",
    "\n",
    "- __longest_substr_ratio__ :  Ratio of length longest common substring to min lenghth of token count of Q1 and Q2<br>longest_substr_ratio = len(longest common substring) / (min(len(q1_tokens), len(q2_tokens))\n",
    "Thesis refered from /http://static.hongbozhang.me/doc/STAT_441_Report.pdf\n",
    "\n",
    "\n",
    "\n",
    "- **Ouput: feature_nlp.csv**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/yanzheyuan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Data Preprocessing: remove noise like html-tags, punctuations, stemming, stopwords, etc.\n",
    "# Idea from kaggle notebooks\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# To get the results in 4 decemal points\n",
    "SAFE_DIV = 0.0001 \n",
    "\n",
    "# STOP_WORDS = nltk.stopwords.words(\"english\")\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "STOP_WORDS = stopwords.words('english')\n",
    "\n",
    "# Preprocessing\n",
    "def preprocess(x):\n",
    "    x = str(x).lower()\n",
    "    x = x.replace(\",000,000\", \"m\").replace(\",000\", \"k\").replace(\"′\", \"'\").replace(\"’\", \"'\")\\\n",
    "                           .replace(\"won't\", \"will not\").replace(\"cannot\", \"can not\").replace(\"can't\", \"can not\")\\\n",
    "                           .replace(\"n't\", \" not\").replace(\"what's\", \"what is\").replace(\"it's\", \"it is\")\\\n",
    "                           .replace(\"'ve\", \" have\").replace(\"i'm\", \"i am\").replace(\"'re\", \" are\")\\\n",
    "                           .replace(\"he's\", \"he is\").replace(\"she's\", \"she is\").replace(\"'s\", \" own\")\\\n",
    "                           .replace(\"%\", \" percent \").replace(\"₹\", \" rupee \").replace(\"$\", \" dollar \")\\\n",
    "                           .replace(\"€\", \" euro \").replace(\"'ll\", \" will\")\n",
    "    x = re.sub(r\"([0-9]+)000000\", r\"\\1m\", x)\n",
    "    x = re.sub(r\"([0-9]+)000\", r\"\\1k\", x)\n",
    "    \n",
    "    \n",
    "    porter = PorterStemmer()\n",
    "    pattern = re.compile('\\W')\n",
    "    \n",
    "    if type(x) == type(''):\n",
    "        x = re.sub(pattern, ' ', x)\n",
    "    \n",
    "    \n",
    "    if type(x) == type(''):\n",
    "        x = porter.stem(x)\n",
    "        example1 = BeautifulSoup(x)\n",
    "        x = example1.get_text()\n",
    "               \n",
    "    \n",
    "    return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparations for distance calculations\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "import pickle\n",
    "import gensim\n",
    "\n",
    "# Download GloVe model\n",
    "# !wget http://nlp.stanford.edu/data/glove.840B.300d.zip\n",
    "# !unzip glove.840B.300d.zip\n",
    "\n",
    "# Use gensim package to do word-to-vec\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "# glove2word2vec(glove_input_file=\"glove.840B.300d.txt\", word2vec_output_file=\"glove_vectors.txt\")\n",
    "\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "glove_model = KeyedVectors.load_word2vec_format(\"glove_vectors.txt\", binary=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distance calculations\n",
    "from scipy.stats import skew, kurtosis\n",
    "from scipy.spatial.distance import cosine, cityblock, canberra, euclidean, minkowski\n",
    "\n",
    "# Preprocessing: simple version\n",
    "def remove_stop(sentence):\n",
    "    sentence  = str(sentence)\n",
    "    if sentence == None:\n",
    "        return ' '\n",
    "    if sentence == np.nan:\n",
    "        return ' '\n",
    "    if sentence == 'NaN':\n",
    "        return ' '\n",
    "    z = [i for i in sentence.split() if i not in STOP_WORDS]\n",
    "    return ' '.join(z)\n",
    "\n",
    "# wmd_dist calculation\n",
    "def wmd(s1, s2, model):\n",
    "    s1 = str(s1)\n",
    "    s2 = str(s2)\n",
    "    s1 = s1.split()\n",
    "    s2 = s2.split()\n",
    "    return model.wmdistance(s1, s2)\n",
    "\n",
    "# the avg-w2v for each sentence is stored in this list\n",
    "def g2w2v(list_of_sent, model, d):\n",
    "    # Returns average of word vectors for each sentance with dimension of model given\n",
    "    sent_vectors = []\n",
    "    for sentence in list_of_sent: # for each review/sentence\n",
    "        doc = [word for word in sentence if word in model.wv.vocab] # .wv: return a numpy vector of a word\n",
    "        if doc:\n",
    "            sent_vec = np.mean(model.wv[doc],axis=0) # get the average of vector, namely avgw2v\n",
    "        else:\n",
    "            sent_vec = np.zeros(d)\n",
    "        sent_vectors.append(sent_vec)\n",
    "    return sent_vectors\n",
    "\n",
    "# Gathering all calculations\n",
    "def get_distance_features(df):\n",
    "    \n",
    "    print(\"Extracting Distance Features..\")\n",
    "    \n",
    "    # wmd_distance\n",
    "    df['question1'] = df.question1.apply(remove_stop)\n",
    "    df['question2'] = df.question2.apply(remove_stop)\n",
    "    df['word_mover_dist'] = df.apply(lambda x: wmd(x['question1'], x['question2'],glove_model), axis=1)\n",
    "    \n",
    "    print(\"- wmd done...\")\n",
    "    \n",
    "    # Other Distances\n",
    "    # Converting questions into lists of sentences\n",
    "    list_of_question1=[]\n",
    "    for sentence in df.question1.values:\n",
    "        list_of_question1.append(sentence.split())\n",
    "    \n",
    "    list_of_question2=[]\n",
    "    for sentence in df.question2.values:\n",
    "        list_of_question2.append(sentence.split())\n",
    "    \n",
    "    # Get embeded vectors from a pre-trained model(GloVe)\n",
    "    g2w2v_q1 = g2w2v(list_of_question1, glove_model, 300)\n",
    "    g2w2v_q2 = g2w2v(list_of_question2, glove_model, 300)\n",
    "    \n",
    "    print(\"- embedding done...\")\n",
    "    \n",
    "    # Spatial Distances on vectors of questions\n",
    "    df['cosine_dist'] = [cosine(q1, q2) for (q1, q2) in zip(g2w2v_q1,g2w2v_q2)]\n",
    "    df['cityblock_dist'] = [cityblock(q1, q2) for (q1, q2) in zip(g2w2v_q1,g2w2v_q2)]\n",
    "    df['canberra_dist'] = [canberra(q1, q2) for (q1, q2) in zip(g2w2v_q1,g2w2v_q2)]\n",
    "    df['euclidean_dist'] = [euclidean(q1, q2) for (q1, q2) in zip(g2w2v_q1,g2w2v_q2)]\n",
    "    df['minkowski_dist'] = [minkowski(q1, q2) for (q1, q2) in zip(g2w2v_q1,g2w2v_q2)]\n",
    "    \n",
    "    print('- spatial distance done')\n",
    "    \n",
    "    # Deal with nan values\n",
    "    df.cosine_dist = df.cosine_dist.fillna(0)\n",
    "    df.word_mover_dist = df.word_mover_dist.apply(lambda wmd: 30 if wmd == np.inf else wmd )\n",
    "   \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical features on Text Tokens of questions\n",
    "def get_token_features(q1, q2):\n",
    "    token_features = [0.0]*10\n",
    "    \n",
    "    # Converting the sentence into Tokens: \n",
    "    q1_tokens = q1.split()\n",
    "    q2_tokens = q2.split()\n",
    "\n",
    "    if len(q1_tokens) == 0 or len(q2_tokens) == 0:\n",
    "        return token_features\n",
    "    \n",
    "    # Get the non-stopwords in questions\n",
    "    q1_words = set([word for word in q1_tokens if word not in STOP_WORDS])\n",
    "    q2_words = set([word for word in q2_tokens if word not in STOP_WORDS])\n",
    "    \n",
    "    # Get the stopwords in questions\n",
    "    q1_stops = set([word for word in q1_tokens if word in STOP_WORDS])\n",
    "    q2_stops = set([word for word in q2_tokens if word in STOP_WORDS])\n",
    "    \n",
    "    # Get the common non-stopwords from question pair\n",
    "    common_word_count = len(q1_words.intersection(q2_words))\n",
    "    \n",
    "    # Get the common stopwords from question pair\n",
    "    common_stop_count = len(q1_stops.intersection(q2_stops))\n",
    "    \n",
    "    # Get the common Tokens from question pair\n",
    "    common_token_count = len(set(q1_tokens).intersection(set(q2_tokens)))\n",
    "    \n",
    "    # Add safety div\n",
    "    token_features[0] = common_word_count / (min(len(q1_words), len(q2_words)) + SAFE_DIV)\n",
    "    token_features[1] = common_word_count / (max(len(q1_words), len(q2_words)) + SAFE_DIV)\n",
    "    token_features[2] = common_stop_count / (min(len(q1_stops), len(q2_stops)) + SAFE_DIV)\n",
    "    token_features[3] = common_stop_count / (max(len(q1_stops), len(q2_stops)) + SAFE_DIV)\n",
    "    token_features[4] = common_token_count / (min(len(q1_tokens), len(q2_tokens)) + SAFE_DIV)\n",
    "    token_features[5] = common_token_count / (max(len(q1_tokens), len(q2_tokens)) + SAFE_DIV)\n",
    "    \n",
    "    # Last word of both question is same or not\n",
    "    token_features[6] = int(q1_tokens[-1] == q2_tokens[-1])\n",
    "    \n",
    "    # First word of both question is same or not\n",
    "    token_features[7] = int(q1_tokens[0] == q2_tokens[0])\n",
    "    \n",
    "    token_features[8] = abs(len(q1_tokens) - len(q2_tokens))\n",
    "    \n",
    "    # Average Token Length of both Questions\n",
    "    token_features[9] = (len(q1_tokens) + len(q2_tokens))/2\n",
    "    return token_features\n",
    "\n",
    "\n",
    "# Get the Longest Common sub string\n",
    "def get_longest_substr_ratio(a, b):\n",
    "    strs = list(distance.lcsubstrings(a, b))\n",
    "    if len(strs) == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return len(strs[0]) / (min(len(a), len(b)) + 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather all the NLP features\n",
    "def extract_features(df):\n",
    "    # preprocessing each question, apply self-defined function preprocess to filter text data with stopwords preparation\n",
    "    df[\"question1\"] = df[\"question1\"].fillna(\"\").apply(preprocess)\n",
    "    df[\"question2\"] = df[\"question2\"].fillna(\"\").apply(preprocess)\n",
    "\n",
    "    print(\"Extracting Token Features...\")\n",
    "    \n",
    "    token_features = df.apply(lambda x: get_token_features(x[\"question1\"], x[\"question2\"]), axis=1)\n",
    "    \n",
    "    df[\"cwc_min\"]       = list(map(lambda x: x[0], token_features))\n",
    "    df[\"cwc_max\"]       = list(map(lambda x: x[1], token_features))\n",
    "    df[\"csc_min\"]       = list(map(lambda x: x[2], token_features))\n",
    "    df[\"csc_max\"]       = list(map(lambda x: x[3], token_features))\n",
    "    df[\"ctc_min\"]       = list(map(lambda x: x[4], token_features))\n",
    "    df[\"ctc_max\"]       = list(map(lambda x: x[5], token_features))\n",
    "    df[\"last_word_eq\"]  = list(map(lambda x: x[6], token_features))\n",
    "    df[\"first_word_eq\"] = list(map(lambda x: x[7], token_features))\n",
    "    df[\"abs_len_diff\"]  = list(map(lambda x: x[8], token_features))\n",
    "    df[\"mean_len\"]      = list(map(lambda x: x[9], token_features))\n",
    "   \n",
    "    # Getting Fuzzy Features and Merging with Dataset\n",
    "    print(\"Extracting Fuzzy Features..\")\n",
    "\n",
    "    df[\"token_set_ratio\"]       = df.apply(lambda x: fuzz.token_set_ratio(x[\"question1\"], x[\"question2\"]), axis=1)\n",
    "    df[\"token_sort_ratio\"]      = df.apply(lambda x: fuzz.token_sort_ratio(x[\"question1\"], x[\"question2\"]), axis=1)\n",
    "    df[\"fuzz_ratio\"]            = df.apply(lambda x: fuzz.QRatio(x[\"question1\"], x[\"question2\"]), axis=1)\n",
    "    df[\"fuzz_partial_ratio\"]    = df.apply(lambda x: fuzz.partial_ratio(x[\"question1\"], x[\"question2\"]), axis=1)\n",
    "    df[\"longest_substr_ratio\"]  = df.apply(lambda x: get_longest_substr_ratio(x[\"question1\"], x[\"question2\"]), axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features for test:\n",
      "Extracting Token Features...\n",
      "Extracting Fuzzy Features..\n",
      "Extracting Distance Features..\n",
      "- wmd done...\n",
      "- embedding done...\n",
      "- spatial distance done\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_id</th>\n",
       "      <th>cwc_min</th>\n",
       "      <th>cwc_max</th>\n",
       "      <th>csc_min</th>\n",
       "      <th>csc_max</th>\n",
       "      <th>ctc_min</th>\n",
       "      <th>ctc_max</th>\n",
       "      <th>last_word_eq</th>\n",
       "      <th>first_word_eq</th>\n",
       "      <th>abs_len_diff</th>\n",
       "      <th>...</th>\n",
       "      <th>token_sort_ratio</th>\n",
       "      <th>fuzz_ratio</th>\n",
       "      <th>fuzz_partial_ratio</th>\n",
       "      <th>longest_substr_ratio</th>\n",
       "      <th>word_mover_dist</th>\n",
       "      <th>cosine_dist</th>\n",
       "      <th>cityblock_dist</th>\n",
       "      <th>canberra_dist</th>\n",
       "      <th>euclidean_dist</th>\n",
       "      <th>minkowski_dist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.599988</td>\n",
       "      <td>0.333330</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.272725</td>\n",
       "      <td>0.214284</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>50</td>\n",
       "      <td>37</td>\n",
       "      <td>45</td>\n",
       "      <td>0.241379</td>\n",
       "      <td>5.377619</td>\n",
       "      <td>0.212743</td>\n",
       "      <td>32.914707</td>\n",
       "      <td>158.719900</td>\n",
       "      <td>2.450075</td>\n",
       "      <td>2.450075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.799984</td>\n",
       "      <td>0.571420</td>\n",
       "      <td>0.499975</td>\n",
       "      <td>0.142855</td>\n",
       "      <td>0.714276</td>\n",
       "      <td>0.357140</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>...</td>\n",
       "      <td>58</td>\n",
       "      <td>47</td>\n",
       "      <td>56</td>\n",
       "      <td>0.386364</td>\n",
       "      <td>2.885574</td>\n",
       "      <td>0.082187</td>\n",
       "      <td>22.012440</td>\n",
       "      <td>127.354115</td>\n",
       "      <td>1.689144</td>\n",
       "      <td>1.689144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.999967</td>\n",
       "      <td>0.499992</td>\n",
       "      <td>0.666644</td>\n",
       "      <td>0.333328</td>\n",
       "      <td>0.833319</td>\n",
       "      <td>0.357140</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>55</td>\n",
       "      <td>57</td>\n",
       "      <td>83</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>3.004117</td>\n",
       "      <td>0.094164</td>\n",
       "      <td>26.997807</td>\n",
       "      <td>123.466020</td>\n",
       "      <td>1.951432</td>\n",
       "      <td>1.951432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>52</td>\n",
       "      <td>52</td>\n",
       "      <td>53</td>\n",
       "      <td>0.277778</td>\n",
       "      <td>7.164147</td>\n",
       "      <td>0.391609</td>\n",
       "      <td>73.987938</td>\n",
       "      <td>177.060048</td>\n",
       "      <td>5.201015</td>\n",
       "      <td>5.201015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.999950</td>\n",
       "      <td>0.666644</td>\n",
       "      <td>0.999900</td>\n",
       "      <td>0.249994</td>\n",
       "      <td>0.749981</td>\n",
       "      <td>0.499992</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>66</td>\n",
       "      <td>70</td>\n",
       "      <td>73</td>\n",
       "      <td>0.483871</td>\n",
       "      <td>3.298204</td>\n",
       "      <td>0.247688</td>\n",
       "      <td>40.107086</td>\n",
       "      <td>147.543857</td>\n",
       "      <td>3.130887</td>\n",
       "      <td>3.130887</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   test_id   cwc_min   cwc_max   csc_min   csc_max   ctc_min   ctc_max  \\\n",
       "0        0  0.599988  0.333330  0.000000  0.000000  0.272725  0.214284   \n",
       "1        1  0.799984  0.571420  0.499975  0.142855  0.714276  0.357140   \n",
       "2        2  0.999967  0.499992  0.666644  0.333328  0.833319  0.357140   \n",
       "3        3  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4        4  0.999950  0.666644  0.999900  0.249994  0.749981  0.499992   \n",
       "\n",
       "   last_word_eq  first_word_eq  abs_len_diff  ...  token_sort_ratio  \\\n",
       "0           0.0            0.0           3.0  ...                50   \n",
       "1           0.0            0.0           7.0  ...                58   \n",
       "2           0.0            1.0           8.0  ...                55   \n",
       "3           0.0            0.0           1.0  ...                52   \n",
       "4           1.0            1.0           2.0  ...                66   \n",
       "\n",
       "   fuzz_ratio  fuzz_partial_ratio  longest_substr_ratio  word_mover_dist  \\\n",
       "0          37                  45              0.241379         5.377619   \n",
       "1          47                  56              0.386364         2.885574   \n",
       "2          57                  83              0.400000         3.004117   \n",
       "3          52                  53              0.277778         7.164147   \n",
       "4          70                  73              0.483871         3.298204   \n",
       "\n",
       "   cosine_dist  cityblock_dist  canberra_dist  euclidean_dist  minkowski_dist  \n",
       "0     0.212743       32.914707     158.719900        2.450075        2.450075  \n",
       "1     0.082187       22.012440     127.354115        1.689144        1.689144  \n",
       "2     0.094164       26.997807     123.466020        1.951432        1.951432  \n",
       "3     0.391609       73.987938     177.060048        5.201015        5.201015  \n",
       "4     0.247688       40.107086     147.543857        3.130887        3.130887  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if os.path.isfile('Features/feature_nlp_test.csv'):\n",
    "    df_nlp = pd.read_csv(\"Features/feature_nlp_test.csv\",encoding='latin-1')\n",
    "    # df.fillna('')\n",
    "else:\n",
    "    # If there are no existing file then create a csv file, make sure you have run the previous code in 1.3 chapter\n",
    "    print(\"Extracting features for test:\")\n",
    "    df = pd.read_csv(\"Data/test.csv\")\n",
    "    df = extract_features(df)\n",
    "    df = get_distance_features(df)\n",
    "    # drop unecessary columns\n",
    "    df = df.drop(['question1','question2'], axis=1)\n",
    "    df.to_csv(\"Features/feature_nlp_test.csv\", index=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2345796, 22)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.cosine_dist = df.cosine_dist.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "test_id                 0\n",
       "cwc_min                 0\n",
       "cwc_max                 0\n",
       "csc_min                 0\n",
       "csc_max                 0\n",
       "ctc_min                 0\n",
       "ctc_max                 0\n",
       "last_word_eq            0\n",
       "first_word_eq           0\n",
       "abs_len_diff            0\n",
       "mean_len                0\n",
       "token_set_ratio         0\n",
       "token_sort_ratio        0\n",
       "fuzz_ratio              0\n",
       "fuzz_partial_ratio      0\n",
       "longest_substr_ratio    0\n",
       "word_mover_dist         0\n",
       "cosine_dist             0\n",
       "cityblock_dist          0\n",
       "canberra_dist           0\n",
       "euclidean_dist          0\n",
       "minkowski_dist          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check on NaN values\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_id</th>\n",
       "      <th>cwc_min</th>\n",
       "      <th>cwc_max</th>\n",
       "      <th>csc_min</th>\n",
       "      <th>csc_max</th>\n",
       "      <th>ctc_min</th>\n",
       "      <th>ctc_max</th>\n",
       "      <th>last_word_eq</th>\n",
       "      <th>first_word_eq</th>\n",
       "      <th>abs_len_diff</th>\n",
       "      <th>...</th>\n",
       "      <th>token_sort_ratio</th>\n",
       "      <th>fuzz_ratio</th>\n",
       "      <th>fuzz_partial_ratio</th>\n",
       "      <th>longest_substr_ratio</th>\n",
       "      <th>word_mover_dist</th>\n",
       "      <th>cosine_dist</th>\n",
       "      <th>cityblock_dist</th>\n",
       "      <th>canberra_dist</th>\n",
       "      <th>euclidean_dist</th>\n",
       "      <th>minkowski_dist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [test_id, cwc_min, cwc_max, csc_min, csc_max, ctc_min, ctc_max, last_word_eq, first_word_eq, abs_len_diff, mean_len, token_set_ratio, token_sort_ratio, fuzz_ratio, fuzz_partial_ratio, longest_substr_ratio, word_mover_dist, cosine_dist, cityblock_dist, canberra_dist, euclidean_dist, minkowski_dist]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 22 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = df[df.isnull().any(1)]\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cwc_min</th>\n",
       "      <th>cwc_max</th>\n",
       "      <th>csc_min</th>\n",
       "      <th>csc_max</th>\n",
       "      <th>ctc_min</th>\n",
       "      <th>ctc_max</th>\n",
       "      <th>last_word_eq</th>\n",
       "      <th>first_word_eq</th>\n",
       "      <th>abs_len_diff</th>\n",
       "      <th>...</th>\n",
       "      <th>token_sort_ratio</th>\n",
       "      <th>fuzz_ratio</th>\n",
       "      <th>fuzz_partial_ratio</th>\n",
       "      <th>longest_substr_ratio</th>\n",
       "      <th>word_mover_dist</th>\n",
       "      <th>cosine_dist</th>\n",
       "      <th>cityblock_dist</th>\n",
       "      <th>canberra_dist</th>\n",
       "      <th>euclidean_dist</th>\n",
       "      <th>minkowski_dist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.999980</td>\n",
       "      <td>0.833319</td>\n",
       "      <td>0.999983</td>\n",
       "      <td>0.999983</td>\n",
       "      <td>0.916659</td>\n",
       "      <td>0.785709</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>93</td>\n",
       "      <td>93</td>\n",
       "      <td>100</td>\n",
       "      <td>0.982759</td>\n",
       "      <td>1.216034</td>\n",
       "      <td>0.031762</td>\n",
       "      <td>14.274065</td>\n",
       "      <td>91.483062</td>\n",
       "      <td>1.047253</td>\n",
       "      <td>1.047253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.799984</td>\n",
       "      <td>0.399996</td>\n",
       "      <td>0.749981</td>\n",
       "      <td>0.599988</td>\n",
       "      <td>0.699993</td>\n",
       "      <td>0.466664</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>63</td>\n",
       "      <td>66</td>\n",
       "      <td>75</td>\n",
       "      <td>0.596154</td>\n",
       "      <td>4.897662</td>\n",
       "      <td>0.266555</td>\n",
       "      <td>33.272633</td>\n",
       "      <td>149.670092</td>\n",
       "      <td>2.624989</td>\n",
       "      <td>2.624989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.399992</td>\n",
       "      <td>0.333328</td>\n",
       "      <td>0.399992</td>\n",
       "      <td>0.249997</td>\n",
       "      <td>0.399996</td>\n",
       "      <td>0.285712</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>63</td>\n",
       "      <td>43</td>\n",
       "      <td>47</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>4.011556</td>\n",
       "      <td>0.118900</td>\n",
       "      <td>28.457512</td>\n",
       "      <td>129.214660</td>\n",
       "      <td>2.140298</td>\n",
       "      <td>2.140298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>24</td>\n",
       "      <td>9</td>\n",
       "      <td>14</td>\n",
       "      <td>0.039216</td>\n",
       "      <td>7.514702</td>\n",
       "      <td>0.619671</td>\n",
       "      <td>62.016426</td>\n",
       "      <td>200.899534</td>\n",
       "      <td>4.702347</td>\n",
       "      <td>4.702347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.399992</td>\n",
       "      <td>0.199998</td>\n",
       "      <td>0.999950</td>\n",
       "      <td>0.666644</td>\n",
       "      <td>0.571420</td>\n",
       "      <td>0.307690</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>47</td>\n",
       "      <td>35</td>\n",
       "      <td>56</td>\n",
       "      <td>0.175000</td>\n",
       "      <td>6.257260</td>\n",
       "      <td>0.244168</td>\n",
       "      <td>40.127296</td>\n",
       "      <td>156.627744</td>\n",
       "      <td>3.145122</td>\n",
       "      <td>3.145122</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id   cwc_min   cwc_max   csc_min   csc_max   ctc_min   ctc_max  \\\n",
       "0   0  0.999980  0.833319  0.999983  0.999983  0.916659  0.785709   \n",
       "1   1  0.799984  0.399996  0.749981  0.599988  0.699993  0.466664   \n",
       "2   2  0.399992  0.333328  0.399992  0.249997  0.399996  0.285712   \n",
       "3   3  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4   4  0.399992  0.199998  0.999950  0.666644  0.571420  0.307690   \n",
       "\n",
       "   last_word_eq  first_word_eq  abs_len_diff  ...  token_sort_ratio  \\\n",
       "0           0.0            1.0           2.0  ...                93   \n",
       "1           0.0            1.0           5.0  ...                63   \n",
       "2           0.0            1.0           4.0  ...                63   \n",
       "3           0.0            0.0           2.0  ...                24   \n",
       "4           0.0            1.0           6.0  ...                47   \n",
       "\n",
       "   fuzz_ratio  fuzz_partial_ratio  longest_substr_ratio  word_mover_dist  \\\n",
       "0          93                 100              0.982759         1.216034   \n",
       "1          66                  75              0.596154         4.897662   \n",
       "2          43                  47              0.166667         4.011556   \n",
       "3           9                  14              0.039216         7.514702   \n",
       "4          35                  56              0.175000         6.257260   \n",
       "\n",
       "   cosine_dist  cityblock_dist  canberra_dist  euclidean_dist  minkowski_dist  \n",
       "0     0.031762       14.274065      91.483062        1.047253        1.047253  \n",
       "1     0.266555       33.272633     149.670092        2.624989        2.624989  \n",
       "2     0.118900       28.457512     129.214660        2.140298        2.140298  \n",
       "3     0.619671       62.016426     200.899534        4.702347        4.702347  \n",
       "4     0.244168       40.127296     156.627744        3.145122        3.145122  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop unecessary columns: I have added this code into the previous code, so the code below is unecessary now\n",
    "\n",
    "#if os.path.isfile('Features/feature_nlp.csv'):\n",
    "#    df_nlp = pd.read_csv(\"Features/feature_nlp.csv\",encoding='latin-1')\n",
    "#    df_nlp = df_nlp.drop(['qid1','qid2','question1','question2','is_duplicate'], axis=1)\n",
    "#    df_nlp.to_csv(\"Features/feature_nlp.csv\", index=False)\n",
    "#else:\n",
    "#    print('There is no feature_nlp.csv!')\n",
    "#df_nlp.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering on TFIDF weighted Word-Vector\n",
    "\n",
    "Get the Extracting faltted tfidf-based vectors as a feature of every question.    \n",
    "I use en_core_web_sm package from spacy package (Industrial-Strength package for Natural Language Processing) to do the word-to-vec process. \n",
    "- Why TFIDF based? I use the idea of Smooth Inverse Frequency, to get every word a weight of tfidf.\n",
    "- Because the huge size of test data, 'en_core_web_md' can not run locally in my computer, so I run the en_core_web_md version in the Google Colab in `feature_engineering_test_md.ipynb`\n",
    "- Here I retain the 'en_core_web_sm'. en_core_web_sm is English multi-task CNN trained on OntoNotes while en_core_web_md is English multi-task CNN trained on OntoNotes, with GloVe vectors trained on Common Crawl.\n",
    "\n",
    "- Ouput: `feature_vectors_test.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import time\n",
    "import warnings\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import sys\n",
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_id</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>How does the Surface Pro himself 4 compare wit...</td>\n",
       "      <td>Why did Microsoft choose core m3 and not core ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Should I have a hair transplant at age 24? How...</td>\n",
       "      <td>How much cost does hair transplant require?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>What but is the best way to send money from Ch...</td>\n",
       "      <td>What you send money to China?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Which food not emulsifiers?</td>\n",
       "      <td>What foods fibre?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>How \"aberystwyth\" start reading?</td>\n",
       "      <td>How their can I start reading?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2345791</td>\n",
       "      <td>2345791</td>\n",
       "      <td>How do Peaks (TV series): Why did Leland kill ...</td>\n",
       "      <td>What is the most study scene in twin peaks?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2345792</td>\n",
       "      <td>2345792</td>\n",
       "      <td>What does be \"in transit\" mean on FedEx tracking?</td>\n",
       "      <td>How question FedEx packages delivered?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2345793</td>\n",
       "      <td>2345793</td>\n",
       "      <td>What are some famous Romanian drinks (alcoholi...</td>\n",
       "      <td>Can a non-alcoholic restaurant be a huge success?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2345794</td>\n",
       "      <td>2345794</td>\n",
       "      <td>What were the best and worst things about publ...</td>\n",
       "      <td>What are the best and worst things examination...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2345795</td>\n",
       "      <td>2345795</td>\n",
       "      <td>What is the best medication equation erectile ...</td>\n",
       "      <td>How do I out get rid of Erectile Dysfunction?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2345796 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         test_id                                          question1  \\\n",
       "0              0  How does the Surface Pro himself 4 compare wit...   \n",
       "1              1  Should I have a hair transplant at age 24? How...   \n",
       "2              2  What but is the best way to send money from Ch...   \n",
       "3              3                        Which food not emulsifiers?   \n",
       "4              4                   How \"aberystwyth\" start reading?   \n",
       "...          ...                                                ...   \n",
       "2345791  2345791  How do Peaks (TV series): Why did Leland kill ...   \n",
       "2345792  2345792  What does be \"in transit\" mean on FedEx tracking?   \n",
       "2345793  2345793  What are some famous Romanian drinks (alcoholi...   \n",
       "2345794  2345794  What were the best and worst things about publ...   \n",
       "2345795  2345795  What is the best medication equation erectile ...   \n",
       "\n",
       "                                                 question2  \n",
       "0        Why did Microsoft choose core m3 and not core ...  \n",
       "1              How much cost does hair transplant require?  \n",
       "2                            What you send money to China?  \n",
       "3                                        What foods fibre?  \n",
       "4                           How their can I start reading?  \n",
       "...                                                    ...  \n",
       "2345791        What is the most study scene in twin peaks?  \n",
       "2345792             How question FedEx packages delivered?  \n",
       "2345793  Can a non-alcoholic restaurant be a huge success?  \n",
       "2345794  What are the best and worst things examination...  \n",
       "2345795      How do I out get rid of Erectile Dysfunction?  \n",
       "\n",
       "[2345796 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "\n",
    "df = pd.read_csv(\"Data/test.csv\")\n",
    " \n",
    "df['question1'] = df['question1'].apply(lambda x: str(x))\n",
    "df['question2'] = df['question2'].apply(lambda x: str(x))\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get TFIDF values of each question pair\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Merge question texts\n",
    "questions = list(df['question1']) + list(df['question2'])\n",
    "\n",
    "# Vectorizer = CountVectorizer+Transformer\n",
    "tfidf = TfidfVectorizer(lowercase=False,)\n",
    "tfidf.fit_transform(questions)\n",
    "\n",
    "# Here, dictionary: {key:word} = {value: tf-idf-value}\n",
    "word2tfidf = dict(zip(tfidf.get_feature_names(), tfidf.idf_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you haven't download en_core_web_small you will need to try this: (or you can read docs from the official website)\n",
    "#!pip install spacy\n",
    "#!python -m spacy download en_core_web_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2345796/2345796 [3:50:44<00:00, 169.44it/s]  \n"
     ]
    }
   ],
   "source": [
    "# en_vectors_web_md, which includes over 1 million unique vectors.\n",
    "import en_core_web_sm\n",
    "\n",
    "# en_vectors_web_md, which includes over 1 million unique vectors.\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "vecs_1 = []\n",
    "\n",
    "for question_1 in tqdm(list(df['question1'])):  # tqdm is a progress bar\n",
    "    doc_1 = nlp(question_1) \n",
    "    # mean_vec1 = []\n",
    "    mean_vec_1 = np.zeros([len(doc_1), 96])  # in en_core_web_md, the output len of vector is 300\n",
    "    for word_1 in doc_1: \n",
    "        # word2vec\n",
    "        vec_1 = word_1.vector\n",
    "        # fetch df score\n",
    "        try:\n",
    "            idf = word2tfidf[str(word_1)]  # search for tfidf value in the dictionary\n",
    "        except:\n",
    "            idf = 0\n",
    "        # compute final vec\n",
    "        mean_vec_1 += vec_1 * idf\n",
    "        # mean_vec1.append(vec1 * idf)\n",
    "    mean_vec_1 = mean_vec_1.mean(axis=0)\n",
    "    # mean_vec1 = np.array(mean_vec1.mean(axis=0))\n",
    "    vecs_1.append(mean_vec_1)\n",
    "df['q1_vector_features'] = list(vecs_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2345796/2345796 [5:08:10<00:00, 126.87it/s]    \n"
     ]
    }
   ],
   "source": [
    "import en_core_web_md\n",
    "nlp = en_core_web_md.load()\n",
    "vecs_2 = []\n",
    "\n",
    "for question_2 in tqdm(list(df['question2'])):  # tqdm is a progress bar\n",
    "    doc_2 = nlp(question_2) \n",
    "    # mean_vec1 = []\n",
    "    mean_vec_2 = np.zeros([len(doc_2), 96])  # in en_core_web_md, the output len of vector is 300\n",
    "    for word_2 in doc_2: \n",
    "        # word2vec\n",
    "        vec_2 = word_2.vector\n",
    "        # fetch df score\n",
    "        try:\n",
    "            idf = word2tfidf[str(word_2)]\n",
    "        except:\n",
    "            idf = 0\n",
    "        # compute final vec\n",
    "        mean_vec_2 += vec_2 * idf\n",
    "        # mean_vec1.append(vec1 * idf)\n",
    "    mean_vec_2 = mean_vec_2.mean(axis=0)\n",
    "    # mean_vec1 = np.array(mean_vec1.mean(axis=0))\n",
    "    vecs_2.append(mean_vec_2)\n",
    "df['q2_vector_features'] = list(vecs_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_id</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>q1_vector_features</th>\n",
       "      <th>q2_vector_features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>How does the Surface Pro himself 4 compare wit...</td>\n",
       "      <td>Why did Microsoft choose core m3 and not core ...</td>\n",
       "      <td>[3.4787711799144745, 5.263742800801992, -8.544...</td>\n",
       "      <td>[-0.6109025329351425, 11.273080721497536, 9.50...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Should I have a hair transplant at age 24? How...</td>\n",
       "      <td>How much cost does hair transplant require?</td>\n",
       "      <td>[-17.276625401806086, 26.48903825879097, 7.334...</td>\n",
       "      <td>[-12.814834594726562, 7.418883442878723, -2.96...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>What but is the best way to send money from Ch...</td>\n",
       "      <td>What you send money to China?</td>\n",
       "      <td>[-10.838783297687769, 11.333042338490486, -12....</td>\n",
       "      <td>[-8.599780954420567, 6.087195411324501, -8.836...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Which food not emulsifiers?</td>\n",
       "      <td>What foods fibre?</td>\n",
       "      <td>[-9.172834649682045, 7.199574738740921, -5.360...</td>\n",
       "      <td>[-1.5645409300923347, 6.073038935661316, -2.94...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>How \"aberystwyth\" start reading?</td>\n",
       "      <td>How their can I start reading?</td>\n",
       "      <td>[4.6484076380729675, 12.791476726531982, -8.81...</td>\n",
       "      <td>[0.527397871017456, 5.195582449436188, -6.6782...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   test_id                                          question1  \\\n",
       "0        0  How does the Surface Pro himself 4 compare wit...   \n",
       "1        1  Should I have a hair transplant at age 24? How...   \n",
       "2        2  What but is the best way to send money from Ch...   \n",
       "3        3                        Which food not emulsifiers?   \n",
       "4        4                   How \"aberystwyth\" start reading?   \n",
       "\n",
       "                                           question2  \\\n",
       "0  Why did Microsoft choose core m3 and not core ...   \n",
       "1        How much cost does hair transplant require?   \n",
       "2                      What you send money to China?   \n",
       "3                                  What foods fibre?   \n",
       "4                     How their can I start reading?   \n",
       "\n",
       "                                  q1_vector_features  \\\n",
       "0  [3.4787711799144745, 5.263742800801992, -8.544...   \n",
       "1  [-17.276625401806086, 26.48903825879097, 7.334...   \n",
       "2  [-10.838783297687769, 11.333042338490486, -12....   \n",
       "3  [-9.172834649682045, 7.199574738740921, -5.360...   \n",
       "4  [4.6484076380729675, 12.791476726531982, -8.81...   \n",
       "\n",
       "                                  q2_vector_features  \n",
       "0  [-0.6109025329351425, 11.273080721497536, 9.50...  \n",
       "1  [-12.814834594726562, 7.418883442878723, -2.96...  \n",
       "2  [-8.599780954420567, 6.087195411324501, -8.836...  \n",
       "3  [-1.5645409300923347, 6.073038935661316, -2.94...  \n",
       "4  [0.527397871017456, 5.195582449436188, -6.6782...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "test_id               0\n",
       "question1             0\n",
       "question2             0\n",
       "q1_vector_features    0\n",
       "q2_vector_features    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check nan values\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df['q2_vector_features'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flat vectors and merge together\n",
    "\n",
    "# flat\n",
    "columns_1 = ['0_x','1_x','2_x','3_x','4_x','5_x','6_x','7_x','8_x','9_x','10_x','11_x','12_x','13_x','14_x','15_x','16_x','17_x','18_x','19_x','20_x','21_x','22_x','23_x','24_x','25_x','26_x','27_x','28_x','29_x','30_x','31_x','32_x','33_x','34_x','35_x','36_x','37_x','38_x','39_x','40_x','41_x','42_x','43_x','44_x','45_x','46_x','47_x','48_x','49_x','50_x','51_x','52_x','53_x','54_x','55_x','56_x','57_x','58_x','59_x','60_x','61_x','62_x','63_x','64_x','65_x','66_x','67_x','68_x','69_x','70_x','71_x','72_x','73_x','74_x','75_x','76_x','77_x','78_x','79_x','80_x','81_x','82_x','83_x','84_x','85_x','86_x','87_x','88_x','89_x','90_x','91_x','92_x','93_x','94_x','95_x','96_x','97_x','98_x','99_x','100_x','101_x','102_x','103_x','104_x','105_x','106_x','107_x','108_x','109_x','110_x','111_x','112_x','113_x','114_x','115_x','116_x','117_x','118_x','119_x','120_x','121_x','122_x','123_x','124_x','125_x','126_x','127_x','128_x','129_x','130_x','131_x','132_x','133_x','134_x','135_x','136_x','137_x','138_x','139_x','140_x','141_x','142_x','143_x','144_x','145_x','146_x','147_x','148_x','149_x','150_x','151_x','152_x','153_x','154_x','155_x','156_x','157_x','158_x','159_x','160_x','161_x','162_x','163_x','164_x','165_x','166_x','167_x','168_x','169_x','170_x','171_x','172_x','173_x','174_x','175_x','176_x','177_x','178_x','179_x','180_x','181_x','182_x','183_x','184_x','185_x','186_x','187_x','188_x','189_x','190_x','191_x','192_x','193_x','194_x','195_x','196_x','197_x','198_x','199_x','200_x','201_x','202_x','203_x','204_x','205_x','206_x','207_x','208_x','209_x','210_x','211_x','212_x','213_x','214_x','215_x','216_x','217_x','218_x','219_x','220_x','221_x','222_x','223_x','224_x','225_x','226_x','227_x','228_x','229_x','230_x','231_x','232_x','233_x','234_x','235_x','236_x','237_x','238_x','239_x','240_x','241_x','242_x','243_x','244_x','245_x','246_x','247_x','248_x','249_x','250_x','251_x','252_x','253_x','254_x','255_x','256_x','257_x','258_x','259_x','260_x','261_x','262_x','263_x','264_x','265_x','266_x','267_x','268_x','269_x','270_x','271_x','272_x','273_x','274_x','275_x','276_x','277_x','278_x','279_x','280_x','281_x','282_x','283_x','284_x','285_x','286_x','287_x','288_x','289_x','290_x','291_x','292_x','293_x','294_x','295_x','296_x','297_x','298_x','299_x']\n",
    "columns_2 = ['0_y','1_y','2_y','3_y','4_y','5_y','6_y','7_y','8_y','9_y','10_y','11_y','12_y','13_y','14_y','15_y','16_y','17_y','18_y','19_y','20_y','21_y','22_y','23_y','24_y','25_y','26_y','27_y','28_y','29_y','30_y','31_y','32_y','33_y','34_y','35_y','36_y','37_y','38_y','39_y','40_y','41_y','42_y','43_y','44_y','45_y','46_y','47_y','48_y','49_y','50_y','51_y','52_y','53_y','54_y','55_y','56_y','57_y','58_y','59_y','60_y','61_y','62_y','63_y','64_y','65_y','66_y','67_y','68_y','69_y','70_y','71_y','72_y','73_y','74_y','75_y','76_y','77_y','78_y','79_y','80_y','81_y','82_y','83_y','84_y','85_y','86_y','87_y','88_y','89_y','90_y','91_y','92_y','93_y','94_y','95_y','96_y','97_y','98_y','99_y','100_y','101_y','102_y','103_y','104_y','105_y','106_y','107_y','108_y','109_y','110_y','111_y','112_y','113_y','114_y','115_y','116_y','117_y','118_y','119_y','120_y','121_y','122_y','123_y','124_y','125_y','126_y','127_y','128_y','129_y','130_y','131_y','132_y','133_y','134_y','135_y','136_y','137_y','138_y','139_y','140_y','141_y','142_y','143_y','144_y','145_y','146_y','147_y','148_y','149_y','150_y','151_y','152_y','153_y','154_y','155_y','156_y','157_y','158_y','159_y','160_y','161_y','162_y','163_y','164_y','165_y','166_y','167_y','168_y','169_y','170_y','171_y','172_y','173_y','174_y','175_y','176_y','177_y','178_y','179_y','180_y','181_y','182_y','183_y','184_y','185_y','186_y','187_y','188_y','189_y','190_y','191_y','192_y','193_y','194_y','195_y','196_y','197_y','198_y','199_y','200_y','201_y','202_y','203_y','204_y','205_y','206_y','207_y','208_y','209_y','210_y','211_y','212_y','213_y','214_y','215_y','216_y','217_y','218_y','219_y','220_y','221_y','222_y','223_y','224_y','225_y','226_y','227_y','228_y','229_y','230_y','231_y','232_y','233_y','234_y','235_y','236_y','237_y','238_y','239_y','240_y','241_y','242_y','243_y','244_y','245_y','246_y','247_y','248_y','249_y','250_y','251_y','252_y','253_y','254_y','255_y','256_y','257_y','258_y','259_y','260_y','261_y','262_y','263_y','264_y','265_y','266_y','267_y','268_y','269_y','270_y','271_y','272_y','273_y','274_y','275_y','276_y','277_y','278_y','279_y','280_y','281_y','282_y','283_y','284_y','285_y','286_y','287_y','288_y','289_y','290_y','291_y','292_y','293_y','294_y','295_y','296_y','297_y','298_y','299_y']\n",
    "df_temp = df\n",
    "df_temp = df_temp.drop(['question1','question2','q1_vector_features','q2_vector_features'],axis=1)\n",
    "df_q1 = pd.DataFrame(df.q1_vector_features.values.tolist(), index= df.index, columns=columns_1)  # word-vector features\n",
    "df_q2 = pd.DataFrame(df.q2_vector_features.values.tolist(), index= df.index, columns=columns_2)  # word-vector features\n",
    "df_q1['test_id'] = df['test_id']\n",
    "df_q2['test_id'] = df['test_id']\n",
    "\n",
    "# merge\n",
    "df_vectors = df_temp.merge(df_q1, on='test_id', how='left')\n",
    "df_vectors = df_vectors.merge(df_q2, on='test_id', how='left')\n",
    "\n",
    "print(df_vectors.shape)\n",
    "df_vectors.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id       0\n",
       "0_x      0\n",
       "1_x      0\n",
       "2_x      0\n",
       "3_x      0\n",
       "        ..\n",
       "295_y    0\n",
       "296_y    0\n",
       "297_y    0\n",
       "298_y    0\n",
       "299_y    0\n",
       "Length: 601, dtype: int64"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check again because of the left join\n",
    "\n",
    "df_vectors.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tfidf weighted word2vector features...\n"
     ]
    }
   ],
   "source": [
    "# Output/Load \n",
    "\n",
    "if os.path.isfile('Features/feature_vectors.csv'):\n",
    "    df_vectors = pd.read_csv(\"Features/feature_vectors.csv\",encoding='latin-1')\n",
    "else:\n",
    "    print(\"Extracting tfidf weighted word2vector features...\")\n",
    "    # If there are no existing file then create a csv file, make sure you have run the previous code in 1.4 chapter\n",
    "    df_vectors.to_csv('Features/feature_vectors.csv')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering on Similarity Measurements\n",
    "\n",
    "Extracting sinmilarity measurements as a supplement of features. **Or**, they can be used in the model stacking because each one of the similarity measurements can be a independent creteria of duplicated/not duplicarted (i.e. similarities)\n",
    "\n",
    "- tf/tfidf cosine similarity (cosine distance actually): used\n",
    "- jaccord similarity (distance actually): used\n",
    "- simhash： \n",
    "  - Thesis reference: Detecting Near-duplicates for web crawling`\n",
    "  - https://leons.im/posts/a-python-implementation-of-simhash-algorithm/\n",
    "- LSI vetor: I was intended to use, but after reading thesis and papers i found that the usage of this algo is to find the lsi vector similarity of test text compared to the topic based model trained  on corpus (large amount of data). I think it can't be used here.\n",
    "  - LSI uses\n",
    "  - LSA(latent semantic analysis) also known as LSI(latent semantic index)，put forward by Scott Deerwester, Susan T. Dumais\n",
    "  - Deerwester, S., Dumais, S. T., Furnas, G. W., Landauer, T. K., & Harshman, R.(1990). Indexing By Latent Semantic Analysis. Journal of the American Society For Information Science, 41, 391-407. 10\n",
    "  - https://blog.csdn.net/qq_34333481/article/details/85014010\n",
    "\n",
    "- Output: features_similarity.csv\n",
    "\n",
    "- **This part is an addition to the model in STACKING step. For now it is not in the final features.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Data/train.csv\")\n",
    "df['question1'] = df['question1'].apply(lambda x: str(x))\n",
    "df['question2'] = df['question2'].apply(lambda x: str(x))\n",
    "# df['text'] = [df.question1, df.question2]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jaccard similarity based on tfidf vectors\n",
    "\n",
    "def jaccard_similarity_tfidf(s1, s2):\n",
    "    def add_space(s):\n",
    "        return ''.join(list(s))\n",
    "    \n",
    "    s1, s2 = add_space(s1), add_space(s2)\n",
    "    # convert into tfidf matrix\n",
    "    # print(s1)\n",
    "    cv = CountVectorizer(tokenizer=lambda s: s.split())\n",
    "    corpus = [s1, s2]\n",
    "    vectors = cv.fit_transform(corpus).toarray()\n",
    "    # intersection of tfidf matrix\n",
    "    numerator = np.sum(np.min(vectors, axis=0))\n",
    "    # union of tfidf matrix\n",
    "    denominator = np.sum(np.max(vectors, axis=0))\n",
    "    # calculate jaccard similarity\n",
    "    return 1.0 * numerator / denominator\n",
    "df_sim = df.copy()\n",
    "df_sim['jcs_tfidf_sim'] = df_sim.apply(lambda x: jaccard_similarity_tfidf(x['question1'],x['question2']), axis=1)\n",
    "df_sim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jaccard similarity\n",
    "\n",
    "def jaccard_similarity(s1, s2):\n",
    "    a = set(s1.split()) \n",
    "    b = set(s2.split())\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))\n",
    "\n",
    "df_sim['jcs_sim'] = df_sim.apply(lambda x: jaccard_similarity(x.loc['question1'],x.loc['question2']), axis=1)\n",
    "df_sim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tf vector cosine similarity\n",
    "\n",
    "from scipy.linalg import norm\n",
    "def tf_vector_similarity(s1, s2):\n",
    "    def add_space(s):\n",
    "        return ''.join(list(s))\n",
    "\n",
    "    s1, s2 = add_space(s1), add_space(s2)\n",
    "    # convert into tfidf matrix\n",
    "    cv = CountVectorizer(tokenizer=lambda s: s.split())\n",
    "    corpus = [s1, s2]\n",
    "    vectors = cv.fit_transform(corpus).toarray()\n",
    "    # calculate tf vector distance by cosine distance\n",
    "    return np.dot(vectors[0], vectors[1]) / (norm(vectors[0]) * norm(vectors[1]))\n",
    "\n",
    "df_sim['tf_sim'] = df_temp.apply(lambda x: tf_vector_similarity(x.loc['question1'],x.loc['question2']), axis=1)\n",
    "df_sim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf vector similarity\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "def tfidf_similarity(s1, s2):\n",
    "    def add_space(s):\n",
    "        return ''.join(list(s))\n",
    "    \n",
    "    s1, s2 = add_space(s1), add_space(s2)\n",
    "    # convert into tfidf matrix\n",
    "    cv = TfidfVectorizer(tokenizer=lambda s: s.split())\n",
    "    corpus = [s1, s2]\n",
    "    vectors = cv.fit_transform(corpus).toarray()\n",
    "    # calculate tfidf vector distance by cosine distance\n",
    "    return np.dot(vectors[0], vectors[1]) / (norm(vectors[0]) * norm(vectors[1]))\n",
    "\n",
    "df_sim['tfidf_similarity'] = df_sim.apply(lambda x: tfidf_similarity(x.loc['question1'],x.loc['question2']), axis=1)\n",
    "df_sim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simhash similarity\n",
    "\n",
    "import re\n",
    "from simhash import Simhash\n",
    "\n",
    "def simhash_similarity(s1,s2):\n",
    "    def get_features(s):\n",
    "        width = 3\n",
    "        s = s.lower()\n",
    "        s = re.sub(r'[^\\w]+', '', s)\n",
    "        return [s[i:i + width] for i in range(max(len(s) - width + 1, 1))]\n",
    "\n",
    "    return Simhash(get_features(s1)).distance(Simhash(get_features(s2)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering on Other Vectors\n",
    "\n",
    "Extracting other vectors, maybe by expanding the len(features_fianl) the result will be better. But I am not doing this right now. Need experiments.\n",
    "\n",
    "- avg_w2v: (glove based) can expand, it is more recommended now since it is directly from gLOVe pretrained model, so maybe this can conncect with LSTM with glove model.\n",
    "https://cloud.tencent.com/developer/article/1145941\n",
    "- tfidf vectors can expand\n",
    "  - When modeling, TFIDF features don't need to scale since it has regularized in the extracting proces\n",
    "- Doc2Vec: gensim\n",
    "- Word2Vec: gensim, average vector of all words in a sentence as the vector of the sentence.\n",
    "\n",
    "- **This part is an addition to the model, i am not goona put it in the model for now.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
