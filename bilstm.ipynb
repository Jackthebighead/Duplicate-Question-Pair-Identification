{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Siamese-LSTM with Bidirectional LSTM Structure\n",
    "- Use siamese bidirectional LSTM on qqp. Achieve a slightly better result.\n",
    "\n",
    "\n",
    "- **Output:**\n",
    "  - `lstm_3.csv`: 4 epoch, score: **0.18550**, better than the baseline **0.193**\n",
    "    - 1st epoch: loss: 0.2746 - acc: 0.8252 - val_loss: 0.2808 - val_acc: 0.7758\n",
    "    - last epoch: loss: 0.2168 - acc: 0.8463 - val_loss: 0.2081 - val_acc: 0.8555"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from string import punctuation\n",
    "from collections import defaultdict\n",
    "# from tqdm import tqdm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "# Keras package\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, Embedding, Dropout, Activation, LSTM, Lambda\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.layers.pooling import GlobalAveragePooling1D\n",
    "import keras.backend as K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM Stucture: lstm_223_121_0.27_0.33\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter defination\n",
    "\n",
    "# Use the following instructions to download glove and unzip it, if already installed, just comment them.\n",
    "# !wget http://nlp.stanford.edu/data/glove.840B.300d.zip\n",
    "# !unzip glove.840B.300d.zip\n",
    "EMBEDDING_FILE = 'glove.840B.300d.txt'\n",
    "\n",
    "TRAIN_DATA_FILE = 'Data/train.csv'\n",
    "TEST_DATA_FILE = 'Data/test.csv'\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 60  \n",
    "MAX_NUM_WORDS = 200000  # There are about 201000 unique words in training dataset, 200000 is enough for tokenization\n",
    "EMBEDDING_DIM = 300  # word-embedded-vector dimension(300 is for 'glove.840B.300d')\n",
    "VALIDATION_SPLIT_RATE = 0.1 \n",
    "N_HIDDEN = np.random.randint(175, 275)\n",
    "N_DENSE = np.random.randint(100, 150)\n",
    "DROPOUT_RATE_LSTM = 0.15 + np.random.rand() * 0.25  # drop-out possibility, random set to avoid outfitting\n",
    "DROUPOUT_RATE_DENSE = 0.15 + np.random.rand() * 0.25\n",
    "\n",
    "VERSION = 'Temp/lstm_2'\n",
    "print('LSTM Stucture:')\n",
    "print('Num_Lstm:', N_HIDDEN)\n",
    "print('Num_Dense:', N_DENSE)\n",
    "print('Dropout rate in LSTM layer:', DROPOUT_RATE_LSTM) \n",
    "print('Dropout rate in Dense layer::', DROUPOUT_RATE_DENSE)\n",
    "\n",
    "ACTIVE_FUNC = 'relu'\n",
    "re_weight = True  # whether to re-weight classes to fit the 17.4% share in test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create word embedding dictionary\n",
      "Found 2195892 word vectors of glove.\n",
      "Processing text dataset\n",
      "404290 texts are found in train.csv\n",
      "2345796 texts are found in test.csv\n"
     ]
    }
   ],
   "source": [
    "# Create word embedding dictionary from 'glove.840B.300d.txt', {key:value} is {word: glove vector(300,)}\n",
    "print('Create word embedding dictionary')\n",
    "\n",
    "embeddings_index = {}  # the output dictionary\n",
    "f = open(EMBEDDING_FILE, encoding='utf-8')\n",
    "for line in f:  # tqdm\n",
    "    values = line.split()\n",
    "    word = ''.join(values[:-300])   \n",
    "    coefs = np.asarray(values[-300:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found {} word vectors of glove.'.format(len(embeddings_index)))\n",
    "\n",
    "\n",
    "# Preprocess text in dataset\n",
    "print('Processing text dataset')\n",
    "\n",
    "def text_to_wordlist(text, remove_stopwords=False, stem_words=False):\n",
    "    # Clean the text, with the option to remove stopwords and to stem words.\n",
    "    \n",
    "    # Convert words to lower case and split them\n",
    "    text = text.lower().split()\n",
    "\n",
    "    # Optionally, remove stop words\n",
    "    if remove_stopwords:\n",
    "        stop_words = set(stopwords.words(\"english\"))\n",
    "        text = [w for w in text if not w in stop_words]\n",
    "    \n",
    "    text = \" \".join(text)\n",
    "\n",
    "    # Use re to clean the text\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    # text = re.sub(r\"\\0s\", \"0\", text) # It doesn't make sense to me\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    \n",
    "    # Optionally, shorten words to their stems\n",
    "    if stem_words:\n",
    "        text = text.split()\n",
    "        stemmer = SnowballStemmer('english')\n",
    "        stemmed_words = [stemmer.stem(word) for word in text]\n",
    "        text = \" \".join(stemmed_words)\n",
    "    # or nltk package\n",
    "    # if lemma:\n",
    "    #    text = text.split()\n",
    "    #    wn = nltk.WordNetLemmatizer()\n",
    "    #    lemm_words = [wn.lemmatize(word) for word in text]\n",
    "    #    text = \" \".join(lemm_words)\n",
    "    \n",
    "    # Return a list of words\n",
    "    return(text)\n",
    "\n",
    "# Load training data and process with text_to_wordlist (Preprocessing)\n",
    "train_texts_1 = []  # the preprocessed text of q1\n",
    "train_texts_2 = []  # the preprocessed text of q2\n",
    "train_labels = []  # training labels\n",
    "\n",
    "df_train = pd.read_csv(TRAIN_DATA_FILE, encoding='utf-8')  # the original training data\n",
    "df_train = df_train.fillna('empty')\n",
    "train_q1 = df_train.question1.values  # the original text of q1\n",
    "train_q2 = df_train.question2.values  # the original text fo q2\n",
    "train_labels = df_train.is_duplicate.values  # the original label('is_duplicate')\n",
    "\n",
    "for text in train_q1:\n",
    "    train_texts_1.append(text_to_wordlist(text, remove_stopwords=False, stem_words=False))\n",
    "    \n",
    "for text in train_q2:\n",
    "    train_texts_2.append(text_to_wordlist(text, remove_stopwords=False, stem_words=False))\n",
    "\n",
    "print('{} texts are found in train.csv'.format(len(train_texts_1)))\n",
    "\n",
    "# Load testing data and process with text_to_wordlist (Preprocessing)\n",
    "test_texts_1 = []  # the preprocessed text of q1_test\n",
    "test_texts_2 = []  # the preprocessed text of q2_test\n",
    "test_ids = []  # id..\n",
    "\n",
    "df_test = pd.read_csv(TEST_DATA_FILE, encoding='utf-8')  # the original testing data\n",
    "df_test = df_test.fillna('empty')\n",
    "test_q1 = df_test.question1.values  # the original text of q1_test\n",
    "test_q2 = df_test.question2.values  # the original text of q2_test\n",
    "test_ids = df_test.test_id.values  # id..\n",
    "\n",
    "for text in test_q1:\n",
    "    test_texts_1.append(text_to_wordlist(text, remove_stopwords=False, stem_words=False))\n",
    "    \n",
    "for text in test_q2:\n",
    "    test_texts_2.append(text_to_wordlist(text, remove_stopwords=False, stem_words=False))\n",
    "    \n",
    "print('{} texts are found in test.csv'.format(len(test_texts_1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120499 unique tokens are found\n",
      "Shape of train data tensor: (404290, 60)\n",
      "Shape of train labels tensor: (404290,)\n",
      "Shape of test data vtensor: (2345796, 60)\n",
      "Shape of test ids tensor: (2345796,)\n"
     ]
    }
   ],
   "source": [
    "# Keras.Tokenize for all text:\n",
    "# First construct a Tokenizer()\n",
    "# Then use tokenizer_on_texts() method to learn the dictionary of the corpus(all texts(sentences)). We can use .word_index to map between the each word (distinct) with the corresponding number.\n",
    "# Then use text_to_sequence() method to transfer every text(sentence) in texts into sequences of word_indexes.\n",
    "# Then add the same length by padding method: padding_sequences().\n",
    "# Finally use the embedding layer in keras to carry out a vectorization, and input it into LSTM.\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(train_texts_1 + train_texts_2 + test_texts_1 + test_texts_2)  # generate a token dictionary, \n",
    "\n",
    "train_sequences_1 = tokenizer.texts_to_sequences(train_texts_1)  # sequence of q1\n",
    "train_sequences_2 = tokenizer.texts_to_sequences(train_texts_2)  # sequence of q2\n",
    "test_sequences_1 = tokenizer.texts_to_sequences(test_texts_1)  # sequence of q1_test\n",
    "test_sequences_2 = tokenizer.texts_to_sequences(test_texts_2)  # sequence of q2_test\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('{} unique tokens are found'.format(len(word_index)))\n",
    "\n",
    "# Pad all train with Max_Sequence_Length: 60\n",
    "train_data_1 = pad_sequences(train_sequences_1, maxlen=MAX_SEQUENCE_LENGTH)  # padded_sequence of q1 as train_data\n",
    "train_data_2 = pad_sequences(train_sequences_2, maxlen=MAX_SEQUENCE_LENGTH)  # padded_sequence of q2 as train_data\n",
    "print('Shape of train data tensor:', train_data_1.shape)\n",
    "print('Shape of train labels tensor:', train_labels.shape)\n",
    "\n",
    "# Pad all test with Max_Sequence_Length\n",
    "test_data_1 = pad_sequences(test_sequences_1, maxlen=MAX_SEQUENCE_LENGTH)  # padded_sequence of q1_test as test_data\n",
    "test_data_2 = pad_sequences(test_sequences_2, maxlen=MAX_SEQUENCE_LENGTH)  # padded_sequence of q2_test as test_data\n",
    "print('Shape of test data vtensor:', test_data_2.shape)\n",
    "print('Shape of test ids tensor:', test_ids.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get leaky features (NLP features)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "questions = pd.concat([df_train[['question1', 'question2']], df_test[['question1', 'question2']]], axis=0).reset_index(drop='index')\n",
    "q_dict = defaultdict(set)\n",
    "for i in range(questions.shape[0]):\n",
    "        q_dict[questions.question1[i]].add(questions.question2[i])\n",
    "        q_dict[questions.question2[i]].add(questions.question1[i])\n",
    "\n",
    "def q1_freq_train(row):\n",
    "    return len(q_dict.get(row[3]))\n",
    "    # return(len(q_dict[row['question1']]))\n",
    "\n",
    "def q2_freq_train(row):\n",
    "    return len(q_dict.get(row[4]))\n",
    "    # return(len(q_dict[row['question2']]))\n",
    "\n",
    "def q1_q2_intersect_train(row):\n",
    "    return(len(set(q_dict.get(row[3])).intersection(set(q_dict.get(row[4])))))\n",
    "    # return(len(set(q_dict[row['question1']]).intersection(set(q_dict[row['question2']]))))\n",
    "\n",
    "def q1_freq_test(row):\n",
    "    return len(q_dict.get(row[1]))\n",
    "    # return(len(q_dict[row['question1']]))\n",
    "\n",
    "def q2_freq_test(row):\n",
    "    return len(q_dict.get(row[2]))\n",
    "    # return(len(q_dict[row['question2']]))\n",
    "\n",
    "def q1_q2_intersect_test(row):\n",
    "    return(len(set(q_dict.get(row[1])).intersection(set(q_dict.get(row[2])))))\n",
    "    # return(len(set(q_dict[row['question1']]).intersection(set(q_dict[row['question2']]))))\n",
    "\n",
    "df_train['q1_q2_intersect'] = df_train.apply(q1_q2_intersect_train, axis=1, raw=True)\n",
    "df_train['q1_freq'] = df_train.apply(q1_freq_train, axis=1, raw=True)\n",
    "df_train['q2_freq'] = df_train.apply(q2_freq_train, axis=1, raw=True)\n",
    "\n",
    "df_test['q1_q2_intersect'] = df_test.apply(q1_q2_intersect_test, axis=1, raw=True)\n",
    "df_test['q1_freq'] = df_test.apply(q1_freq_test, axis=1, raw=True)\n",
    "df_test['q2_freq'] = df_test.apply(q2_freq_test, axis=1, raw=True)\n",
    "\n",
    "leaks = df_train[['q1_q2_intersect', 'q1_freq', 'q2_freq']]  # the leaky feature\n",
    "test_leaks = df_test[['q1_q2_intersect', 'q1_freq', 'q2_freq']]  # the leaky feature_test\n",
    "\n",
    "\n",
    "# Make scaling for leaky feature\n",
    "ss = StandardScaler()\n",
    "ss.fit(np.vstack((leaks, test_leaks)))\n",
    "leaks = ss.transform(leaks)  # the leaky feature\n",
    "test_leaks = ss.transform(test_leaks)  # the leaky feature_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train & Validation split\n",
    "perm = np.random.permutation(len(train_data_1))\n",
    "idx_train = perm[:int(len(train_data_1)*(1-VALIDATION_SPLIT_RATE))]\n",
    "idx_val = perm[int(len(train_data_1)*(1-VALIDATION_SPLIT_RATE)):]\n",
    "\n",
    "data_1_train = np.vstack((train_data_1[idx_train], train_data_2[idx_train]))\n",
    "data_2_train = np.vstack((train_data_2[idx_train], train_data_1[idx_train]))\n",
    "leaks_train = np.vstack((leaks[idx_train], leaks[idx_train]))\n",
    "labels_train = np.concatenate((train_labels[idx_train], train_labels[idx_train]))\n",
    "\n",
    "data_1_val = np.vstack((train_data_1[idx_val], train_data_2[idx_val]))\n",
    "data_2_val = np.vstack((train_data_2[idx_val], train_data_1[idx_val]))\n",
    "leaks_val = np.vstack((leaks[idx_val], leaks[idx_val]))\n",
    "labels_val = np.concatenate((train_labels[idx_val], train_labels[idx_val]))\n",
    "\n",
    "weight_val = np.ones(len(labels_val))\n",
    "if re_weight:\n",
    "    weight_val *= 0.471544715\n",
    "    weight_val[labels_val==0] = 1.309033281\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing embedding matrix\n",
      "Null word embeddings: \n",
      "Starting the model training\n",
      "Train on 727722 samples, validate on 80858 samples\n",
      "Epoch 1/4\n",
      "727722/727722 [==============================] - 4139s 6ms/step - loss: 0.2746 - acc: 0.8252 - val_loss: 0.2808 - val_acc: 0.7758\n",
      "Epoch 2/4\n",
      "727722/727722 [==============================] - 4066s 6ms/step - loss: 0.2382 - acc: 0.8368 - val_loss: 0.2744 - val_acc: 0.8430\n",
      "Epoch 3/4\n",
      "727722/727722 [==============================] - 3748s 5ms/step - loss: 0.2255 - acc: 0.8419 - val_loss: 0.2160 - val_acc: 0.8488\n",
      "Epoch 4/4\n",
      "727722/727722 [==============================] - 3626s 5ms/step - loss: 0.2168 - acc: 0.8463 - val_loss: 0.2081 - val_acc: 0.8555\n"
     ]
    }
   ],
   "source": [
    "# Create embedding matrix for embedding layer, which is used in the keras.embedding weight as the initializer.\n",
    "from keras.layers import Bidirectional\n",
    "\n",
    "print('Preparing embedding matrix')\n",
    "\n",
    "num_words = min(MAX_NUM_WORDS, len(word_index))+1\n",
    "\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))  # the weight of Embedding layer\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "print('Null word embeddings: '.format(np.sum(np.sum(embedding_matrix, axis=1) == 0)))\n",
    "\n",
    "\n",
    "# NN Model design\n",
    "# Structure: (q1-embedding-lstm + q2-embedding-lstm + leaky-dense)-dense-sigmoid-result\n",
    "\n",
    "# The embedding layer containing the word vectors\n",
    "emb_layer = Embedding(\n",
    "    input_dim=num_words,\n",
    "    output_dim=EMBEDDING_DIM,\n",
    "    weights=[embedding_matrix],\n",
    "    input_length=MAX_SEQUENCE_LENGTH,\n",
    "    trainable=False\n",
    ")    \n",
    "\n",
    "# BiLSTM layer\n",
    "from keras.layers import Bidirectional\n",
    "lstm_layer = Bidirectional(LSTM(N_HIDDEN, dropout=DROPOUT_RATE_LSTM, recurrent_dropout=DROPOUT_RATE_LSTM))\n",
    "\n",
    "\n",
    "# Define inputs\n",
    "seq1 = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "seq2 = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "\n",
    "# Run inputs through embedding\n",
    "emb1 = emb_layer(seq1)\n",
    "emb2 = emb_layer(seq2)\n",
    "\n",
    "# Run through LSTM layers\n",
    "lstm_a = lstm_layer(emb1)\n",
    "# glob1a = GlobalAveragePooling1D()(lstm_a)\n",
    "lstm_b = lstm_layer(emb2)\n",
    "# glob1b = GlobalAveragePooling1D()(lstm_b)\n",
    "\n",
    "magic_input = Input(shape=(leaks.shape[1],))\n",
    "# magic_dense = BatchNormalization()(magic_input)\n",
    "magic_dense = Dense(int(N_DENSE/2), activation=ACTIVE_FUNC)(magic_input)\n",
    "\n",
    "merged = concatenate([lstm_a, lstm_b, magic_dense])\n",
    "merged = BatchNormalization()(merged)\n",
    "merged = Dropout(DROUPOUT_RATE_DENSE)(merged)\n",
    "\n",
    "merged = Dense(N_DENSE, activation=ACTIVE_FUNC)(merged)\n",
    "merged = BatchNormalization()(merged)\n",
    "merged = Dropout(DROUPOUT_RATE_DENSE)(merged)\n",
    "\n",
    "preds = Dense(1, activation='sigmoid')(merged)\n",
    "\n",
    "\n",
    "# Add class weight, magic feature for the unbalancement of training labels.\n",
    "if re_weight:\n",
    "    class_weight = {0: 1.309033281, 1: 0.471544715}\n",
    "else:\n",
    "    class_weight = None\n",
    "    \n",
    "\n",
    "# Train the model\n",
    "\n",
    "print('Starting the model training')\n",
    "\n",
    "model = Model(inputs=[seq1, seq2, magic_input], outputs=preds)\n",
    "model.compile(loss='binary_crossentropy', optimizer='nadam', metrics=['acc'])\n",
    "\n",
    "# Summerization of model\n",
    "model.summary()\n",
    "\n",
    "# Set early stopping (large patience should be useful)\n",
    "early_stopping =EarlyStopping(monitor='val_loss', patience=10)\n",
    "bst_model_path = VERSION + '.h5' \n",
    "model_checkpoint = ModelCheckpoint(bst_model_path, save_best_only=True, save_weights_only=True)\n",
    "\n",
    "\n",
    "hist = model.fit([data_1_train, data_2_train, leaks_train], labels_train, \\\n",
    "        validation_data=([data_1_val, data_2_val, leaks_val], labels_val, weight_val), \\\n",
    "        epochs=4, batch_size=2048, shuffle=True, \\\n",
    "        class_weight=class_weight, callbacks=[early_stopping, model_checkpoint])\n",
    "\n",
    "model.load_weights(bst_model_path) # sotre model parameters in .h5 file\n",
    "bst_val_score = min(hist.history['val_loss'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 60)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_8 (InputLayer)            (None, 60)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 60, 300)      36150000    input_7[0][0]                    \n",
      "                                                                 input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_9 (InputLayer)            (None, 3)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) (None, 446)          934816      embedding_4[0][0]                \n",
      "                                                                 embedding_4[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 60)           240         input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 952)          0           bidirectional_3[0][0]            \n",
      "                                                                 bidirectional_3[1][0]            \n",
      "                                                                 dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 952)          3808        concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 952)          0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 121)          115313      dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 121)          484         dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 121)          0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 1)            122         dropout_6[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 37,204,783\n",
      "Trainable params: 1,052,637\n",
      "Non-trainable params: 36,152,146\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making the submission\n",
      " 917504/2345796 [==========>...................] - ETA: 43:33"
     ]
    }
   ],
   "source": [
    "# Make the submission\n",
    "\n",
    "print('Making the submission')\n",
    "\n",
    "preds = model.predict([test_data_1, test_data_2, test_leaks], batch_size=8192, verbose=1)\n",
    "preds += model.predict([test_data_2, test_data_1, test_leaks], batch_size=8192, verbose=1)\n",
    "preds /= 2\n",
    "\n",
    "submission = pd.DataFrame({'test_id':test_ids, 'is_duplicate':preds.ravel()})\n",
    "submission.to_csv('Models/lstm_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
