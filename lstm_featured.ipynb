{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Siamese-BiLSTM with Features(tm+nlp)\n",
    "- A new model based on feature_engineering and Siamese-BiLSTM\n",
    "- Adding features in `feature_nlp.csv` and `feature_tm.csv` (my previous work on feature engineering) to features in the LSTM based neural network structure.\n",
    "- New: using BiLSTM adding around 40 more features to leaky features, and doubled N_DENSE in Dense layer (to achieve a balance of lstm nn features and adding features). (from N_DENSE/2 to N_DENSE).\n",
    "\n",
    "    \n",
    "- **Output:**\n",
    "  - `lstm_featured.csv`: 4 epoch, score: 0.16515, **best score in our project**\n",
    "    - 1st epoch: loss: 0.2489 - acc: 0.8280 - val_loss: 0.1927 - val_acc: 0.8630\n",
    "    - last epoch: loss: 0.1790 - acc: 0.8714 - val_loss: 0.1700 - val_acc: 0.8723"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from string import punctuation\n",
    "from collections import defaultdict\n",
    "# from tqdm import tqdm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "# Keras package\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, Embedding, Dropout, Activation, LSTM, Lambda\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.layers.pooling import GlobalAveragePooling1D\n",
    "import keras.backend as K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM Stucture:\n",
      "Num_Lstm: 256\n",
      "Num_Dense: 122\n",
      "Dropout rate in LSTM layer: 0.3902174437634266\n",
      "Dropout rate in Dense layer:: 0.3314652224209037\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter defination\n",
    "\n",
    "# Use the following instructions to download glove and unzip it, if already installed, just comment them.\n",
    "# !wget http://nlp.stanford.edu/data/glove.840B.300d.zip\n",
    "# !unzip glove.840B.300d.zip\n",
    "EMBEDDING_FILE = 'glove.840B.300d.txt'\n",
    "\n",
    "TRAIN_DATA_FILE = 'Data/train.csv'\n",
    "TEST_DATA_FILE = 'Data/test.csv'\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 60  \n",
    "MAX_NUM_WORDS = 200000  # There are about 201000 unique words in training dataset, 200000 is enough for tokenization\n",
    "EMBEDDING_DIM = 300  # word-embedded-vector dimension(300 is for 'glove.840B.300d')\n",
    "VALIDATION_SPLIT_RATE = 0.1 \n",
    "N_HIDDEN = np.random.randint(175, 275)  # 250\n",
    "N_DENSE = np.random.randint(100, 150)  # 120\n",
    "DROPOUT_RATE_LSTM = 0.15 + np.random.rand() * 0.25  # drop-out possibility, random set to avoid outfitting  # 0.20\n",
    "DROUPOUT_RATE_DENSE = 0.15 + np.random.rand() * 0.25  # 0.20\n",
    "\n",
    "VERSION = 'Temp/lstm_featured'\n",
    "print('LSTM Stucture:')\n",
    "print('Num_Lstm:', N_HIDDEN)\n",
    "print('Num_Dense:', N_DENSE)\n",
    "print('Dropout rate in LSTM layer:', DROPOUT_RATE_LSTM) \n",
    "print('Dropout rate in Dense layer::', DROUPOUT_RATE_DENSE)\n",
    "\n",
    "ACTIVE_FUNC = 'relu'\n",
    "re_weight = True  # whether to re-weight classes to fit the 17.4% share in test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create word embedding dictionary\n",
      "Found 2195892 word vectors of glove.\n",
      "Processing text dataset\n",
      "404290 texts are found in train.csv\n",
      "2345796 texts are found in test.csv\n"
     ]
    }
   ],
   "source": [
    "# Create word embedding dictionary from 'glove.840B.300d.txt', {key:value} is {word: glove vector(300,)}\n",
    "print('Create word embedding dictionary')\n",
    "\n",
    "embeddings_index = {}  # the output dictionary\n",
    "f = open(EMBEDDING_FILE, encoding='utf-8')\n",
    "for line in f:  # tqdm\n",
    "    values = line.split()\n",
    "    word = ''.join(values[:-300])   \n",
    "    coefs = np.asarray(values[-300:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found {} word vectors of glove.'.format(len(embeddings_index)))\n",
    "\n",
    "\n",
    "# Preprocess text in dataset\n",
    "print('Processing text dataset')\n",
    "\n",
    "def text_to_wordlist(text, remove_stopwords=False, stem_words=False):\n",
    "    # Clean the text, with the option to remove stopwords and to stem words.\n",
    "    \n",
    "    # Convert words to lower case and split them\n",
    "    text = text.lower().split()\n",
    "\n",
    "    # Optionally, remove stop words\n",
    "    if remove_stopwords:\n",
    "        stop_words = set(stopwords.words(\"english\"))\n",
    "        text = [w for w in text if not w in stop_words]\n",
    "    \n",
    "    text = \" \".join(text)\n",
    "\n",
    "    # Use re to clean the text\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    # text = re.sub(r\"\\0s\", \"0\", text) # It doesn't make sense to me\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    \n",
    "    # Optionally, shorten words to their stems\n",
    "    if stem_words:\n",
    "        text = text.split()\n",
    "        stemmer = SnowballStemmer('english')\n",
    "        stemmed_words = [stemmer.stem(word) for word in text]\n",
    "        text = \" \".join(stemmed_words)\n",
    "    # or nltk package\n",
    "    # if lemma:\n",
    "    #    text = text.split()\n",
    "    #    wn = nltk.WordNetLemmatizer()\n",
    "    #    lemm_words = [wn.lemmatize(word) for word in text]\n",
    "    #    text = \" \".join(lemm_words)\n",
    "    \n",
    "    # Return a list of words\n",
    "    return(text)\n",
    "\n",
    "# Load training data and process with text_to_wordlist (Preprocessing)\n",
    "train_texts_1 = []  # the preprocessed text of q1\n",
    "train_texts_2 = []  # the preprocessed text of q2\n",
    "train_labels = []  # training labels\n",
    "\n",
    "df_train = pd.read_csv(TRAIN_DATA_FILE, encoding='utf-8')  # the original training data\n",
    "df_train = df_train.fillna('empty')\n",
    "train_q1 = df_train.question1.values  # the original text of q1\n",
    "train_q2 = df_train.question2.values  # the original text fo q2\n",
    "train_labels = df_train.is_duplicate.values  # the original label('is_duplicate')\n",
    "\n",
    "for text in train_q1:\n",
    "    train_texts_1.append(text_to_wordlist(text, remove_stopwords=False, stem_words=False))\n",
    "    \n",
    "for text in train_q2:\n",
    "    train_texts_2.append(text_to_wordlist(text, remove_stopwords=False, stem_words=False))\n",
    "\n",
    "print('{} texts are found in train.csv'.format(len(train_texts_1)))\n",
    "\n",
    "# Load testing data and process with text_to_wordlist (Preprocessing)\n",
    "test_texts_1 = []  # the preprocessed text of q1_test\n",
    "test_texts_2 = []  # the preprocessed text of q2_test\n",
    "test_ids = []  # id..\n",
    "\n",
    "df_test = pd.read_csv(TEST_DATA_FILE, encoding='utf-8')  # the original testing data\n",
    "df_test = df_test.fillna('empty')\n",
    "test_q1 = df_test.question1.values  # the original text of q1_test\n",
    "test_q2 = df_test.question2.values  # the original text of q2_test\n",
    "test_ids = df_test.test_id.values  # id..\n",
    "\n",
    "for text in test_q1:\n",
    "    test_texts_1.append(text_to_wordlist(text, remove_stopwords=False, stem_words=False))\n",
    "    \n",
    "for text in test_q2:\n",
    "    test_texts_2.append(text_to_wordlist(text, remove_stopwords=False, stem_words=False))\n",
    "    \n",
    "print('{} texts are found in test.csv'.format(len(test_texts_1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120499 unique tokens are found\n",
      "Shape of train data tensor: (404290, 60)\n",
      "Shape of train labels tensor: (404290,)\n",
      "Shape of test data vtensor: (2345796, 60)\n",
      "Shape of test ids tensor: (2345796,)\n"
     ]
    }
   ],
   "source": [
    "# Keras.Tokenize for all text:\n",
    "# First construct a Tokenizer()\n",
    "# Then use tokenizer_on_texts() method to learn the dictionary of the corpus(all texts(sentences)). We can use .word_index to map between the each word (distinct) with the corresponding number.\n",
    "# Then use text_to_sequence() method to transfer every text(sentence) in texts into sequences of word_indexes.\n",
    "# Then add the same length by padding method: padding_sequences().\n",
    "# Finally use the embedding layer in keras to carry out a vectorization, and input it into LSTM.\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(train_texts_1 + train_texts_2 + test_texts_1 + test_texts_2)  # generate a token dictionary, \n",
    "\n",
    "train_sequences_1 = tokenizer.texts_to_sequences(train_texts_1)  # sequence of q1\n",
    "train_sequences_2 = tokenizer.texts_to_sequences(train_texts_2)  # sequence of q2\n",
    "test_sequences_1 = tokenizer.texts_to_sequences(test_texts_1)  # sequence of q1_test\n",
    "test_sequences_2 = tokenizer.texts_to_sequences(test_texts_2)  # sequence of q2_test\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('{} unique tokens are found'.format(len(word_index)))\n",
    "\n",
    "# Pad all train with Max_Sequence_Length: 60\n",
    "train_data_1 = pad_sequences(train_sequences_1, maxlen=MAX_SEQUENCE_LENGTH)  # padded_sequence of q1 as train_data\n",
    "train_data_2 = pad_sequences(train_sequences_2, maxlen=MAX_SEQUENCE_LENGTH)  # padded_sequence of q2 as train_data\n",
    "print('Shape of train data tensor:', train_data_1.shape)\n",
    "print('Shape of train labels tensor:', train_labels.shape)\n",
    "\n",
    "# Pad all test with Max_Sequence_Length\n",
    "test_data_1 = pad_sequences(test_sequences_1, maxlen=MAX_SEQUENCE_LENGTH)  # padded_sequence of q1_test as test_data\n",
    "test_data_2 = pad_sequences(test_sequences_2, maxlen=MAX_SEQUENCE_LENGTH)  # padded_sequence of q2_test as test_data\n",
    "print('Shape of test data vtensor:', test_data_2.shape)\n",
    "print('Shape of test ids tensor:', test_ids.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic features created\n",
      "Features(nlp&tm) Loaded\n",
      "Final features created\n"
     ]
    }
   ],
   "source": [
    "questions = pd.concat([df_train[['question1', 'question2']], df_test[['question1', 'question2']]], axis=0).reset_index(drop='index')\n",
    "q_dict = defaultdict(set)\n",
    "for i in range(questions.shape[0]):\n",
    "        q_dict[questions.question1[i]].add(questions.question2[i])\n",
    "        q_dict[questions.question2[i]].add(questions.question1[i])\n",
    "\n",
    "def q1_freq_train(row):\n",
    "    return len(q_dict.get(row[3]))\n",
    "    # return(len(q_dict[row['question1']]))\n",
    "\n",
    "def q2_freq_train(row):\n",
    "    return len(q_dict.get(row[4]))\n",
    "    # return(len(q_dict[row['question2']]))\n",
    "\n",
    "def q1_q2_intersect_train(row):\n",
    "    return(len(set(q_dict.get(row[3])).intersection(set(q_dict.get(row[4])))))\n",
    "    # return(len(set(q_dict[row['question1']]).intersection(set(q_dict[row['question2']]))))\n",
    "\n",
    "def q1_freq_test(row):\n",
    "    return len(q_dict.get(row[1]))\n",
    "    # return(len(q_dict[row['question1']]))\n",
    "\n",
    "def q2_freq_test(row):\n",
    "    return len(q_dict.get(row[2]))\n",
    "    # return(len(q_dict[row['question2']]))\n",
    "\n",
    "def q1_q2_intersect_test(row):\n",
    "    return(len(set(q_dict.get(row[1])).intersection(set(q_dict.get(row[2])))))\n",
    "    # return(len(set(q_dict[row['question1']]).intersection(set(q_dict[row['question2']]))))\n",
    "\n",
    "df_train['q1_q2_intersect'] = df_train.apply(q1_q2_intersect_train, axis=1, raw=True)\n",
    "df_train['q1_freq'] = df_train.apply(q1_freq_train, axis=1, raw=True)\n",
    "df_train['q2_freq'] = df_train.apply(q2_freq_train, axis=1, raw=True)\n",
    "\n",
    "df_test['q1_q2_intersect'] = df_test.apply(q1_q2_intersect_test, axis=1, raw=True)\n",
    "df_test['q1_freq'] = df_test.apply(q1_freq_test, axis=1, raw=True)\n",
    "df_test['q2_freq'] = df_test.apply(q2_freq_test, axis=1, raw=True)\n",
    "print('Basic features created')\n",
    "\n",
    "# Load features from previous work on feature_tm and feature_nlp\n",
    "import pandas as pd\n",
    "\n",
    "df_feature = pd.read_csv('Features/feature_tm+nlp.csv')\n",
    "df_feature = df_feature.fillna(0)\n",
    "df_feature_test = pd.read_csv('Features/feature_tm+nlp_test.csv')\n",
    "print('Features(nlp&tm) Loaded')\n",
    "\n",
    "df_train['id'] = df_feature['id']\n",
    "df_test['test_id'] = df_feature_test['test_id']\n",
    "\n",
    "df_train = df_train.merge(df_feature,on='id',how='left')\n",
    "leak = df_train.drop(['id','qid1','qid2','question1','question2','is_duplicate'], axis=1)\n",
    "df_test = df_test.merge(df_feature_test,on='test_id',how='left')\n",
    "test_leak = df_test.drop(['test_id','question1','question2'], axis=1)\n",
    "print('Final features created')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalization finished\n"
     ]
    }
   ],
   "source": [
    "# Make scaling for leaky feature (normalization)\n",
    "ss = StandardScaler()\n",
    "ss.fit(np.vstack((leak, test_leak)))\n",
    "leaks = ss.transform(leak)  # the leaky feature, array\n",
    "test_leaks = ss.transform(test_leak)  # the leaky feature_test, array\n",
    "print('Normalization finished')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.11478188, -0.05687322, -0.04436891, ..., -1.67102839,\n",
       "        -1.4782846 , -1.4782846 ],\n",
       "       [-0.11478188,  0.03837623, -0.03081647, ..., -0.20451842,\n",
       "        -0.18610259, -0.18610259],\n",
       "       [-0.11478188, -0.04326616, -0.05792136, ..., -0.7200645 ,\n",
       "        -0.5830698 , -0.5830698 ],\n",
       "       ...,\n",
       "       [-0.11478188, -0.05687322, -0.05792136, ..., -0.83913556,\n",
       "         0.44826012,  0.44826012],\n",
       "       [-0.11478188, -0.05687322, -0.05792136, ...,  1.54412763,\n",
       "         0.72736336,  0.72736336],\n",
       "       [-0.11478188, -0.05687322, -0.05792136, ..., -3.97671095,\n",
       "        -2.33599635, -2.33599635]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train & Validation split\n",
    "perm = np.random.permutation(len(train_data_1))\n",
    "idx_train = perm[:int(len(train_data_1)*(1-VALIDATION_SPLIT_RATE))]\n",
    "idx_val = perm[int(len(train_data_1)*(1-VALIDATION_SPLIT_RATE)):]\n",
    "\n",
    "data_1_train = np.vstack((train_data_1[idx_train], train_data_2[idx_train]))\n",
    "data_2_train = np.vstack((train_data_2[idx_train], train_data_1[idx_train]))\n",
    "leaks_train = np.vstack((leaks[idx_train], leaks[idx_train]))\n",
    "labels_train = np.concatenate((train_labels[idx_train], train_labels[idx_train]))\n",
    "\n",
    "data_1_val = np.vstack((train_data_1[idx_val], train_data_2[idx_val]))\n",
    "data_2_val = np.vstack((train_data_2[idx_val], train_data_1[idx_val]))\n",
    "leaks_val = np.vstack((leaks[idx_val], leaks[idx_val]))\n",
    "labels_val = np.concatenate((train_labels[idx_val], train_labels[idx_val]))\n",
    "\n",
    "weight_val = np.ones(len(labels_val))\n",
    "if re_weight:\n",
    "    weight_val *= 0.471544715\n",
    "    weight_val[labels_val==0] = 1.309033281\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing embedding matrix\n",
      "Null word embeddings: \n",
      "Starting the model training\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, 60)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            (None, 60)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 60, 300)      36150000    input_4[0][0]                    \n",
      "                                                                 input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            (None, 44)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 256)          570368      embedding_2[0][0]                \n",
      "                                                                 embedding_2[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 122)          5490        input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 634)          0           lstm_2[0][0]                     \n",
      "                                                                 lstm_2[1][0]                     \n",
      "                                                                 dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 634)          2536        concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 634)          0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 122)          77470       dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 122)          488         dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 122)          0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 1)            123         dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 36,806,475\n",
      "Trainable params: 654,963\n",
      "Non-trainable params: 36,151,512\n",
      "__________________________________________________________________________________________________\n",
      "Train on 727722 samples, validate on 80858 samples\n",
      "Epoch 1/4\n",
      "727722/727722 [==============================] - 2501s 3ms/step - loss: 0.2489 - acc: 0.8280 - val_loss: 0.1927 - val_acc: 0.8630\n",
      "Epoch 2/4\n",
      "727722/727722 [==============================] - 2454s 3ms/step - loss: 0.1965 - acc: 0.8582 - val_loss: 0.1888 - val_acc: 0.8459\n",
      "Epoch 3/4\n",
      "727722/727722 [==============================] - 2445s 3ms/step - loss: 0.1861 - acc: 0.8663 - val_loss: 0.1782 - val_acc: 0.8533\n",
      "Epoch 4/4\n",
      "727722/727722 [==============================] - 2446s 3ms/step - loss: 0.1790 - acc: 0.8714 - val_loss: 0.1700 - val_acc: 0.8723\n"
     ]
    }
   ],
   "source": [
    "# Create embedding matrix for embedding layer, which is used in the keras.embedding weight as the initializer.\n",
    "from keras.layers import Bidirectional\n",
    "print('Preparing embedding matrix')\n",
    "\n",
    "num_words = min(MAX_NUM_WORDS, len(word_index))+1\n",
    "\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))  # the weight of Embedding layer\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "print('Null word embeddings: '.format(np.sum(np.sum(embedding_matrix, axis=1) == 0)))\n",
    "\n",
    "\n",
    "# NN Model design\n",
    "# Structure: (q1-embedding-lstm + q2-embedding-lstm + leaky-dense)-dense-sigmoid-result\n",
    "\n",
    "# The embedding layer containing the word vectors\n",
    "emb_layer = Embedding(\n",
    "    input_dim=num_words,\n",
    "    output_dim=EMBEDDING_DIM,\n",
    "    weights=[embedding_matrix],\n",
    "    input_length=MAX_SEQUENCE_LENGTH,\n",
    "    trainable=False\n",
    ")    \n",
    "\n",
    "# LSTM layer\n",
    "# adding bidirectional does help to the performance, it achieves the best score which is 0.16515\n",
    "#lstm_layer = Bidirectional(LSTM(N_HIDDEN, dropout=DROPOUT_RATE_LSTM, recurrent_dropout=DROPOUT_RATE_LSTM))\n",
    "lstm_layer = LSTM(N_HIDDEN, dropout=DROPOUT_RATE_LSTM, recurrent_dropout=DROPOUT_RATE_LSTM)\n",
    "\n",
    "# Define inputs\n",
    "seq1 = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "seq2 = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "\n",
    "# Run inputs through embedding\n",
    "emb1 = emb_layer(seq1)\n",
    "emb2 = emb_layer(seq2)\n",
    "\n",
    "# Run through LSTM layers\n",
    "lstm_a = lstm_layer(emb1)\n",
    "# glob1a = GlobalAveragePooling1D()(lstm_a)\n",
    "lstm_b = lstm_layer(emb2)\n",
    "# glob1b = GlobalAveragePooling1D()(lstm_b)\n",
    "\n",
    "magic_input = Input(shape=(leaks.shape[1],))\n",
    "# magic_dense = BatchNormalization()(magic_input)\n",
    "magic_dense = Dense(int(N_DENSE), activation=ACTIVE_FUNC)(magic_input)  # change N_DENSE/2 to N_DENSE because of the addition of leaky features\n",
    "\n",
    "merged = concatenate([lstm_a, lstm_b, magic_dense])\n",
    "merged = BatchNormalization()(merged)\n",
    "merged = Dropout(DROUPOUT_RATE_DENSE)(merged)\n",
    "\n",
    "merged = Dense(N_DENSE, activation=ACTIVE_FUNC)(merged)\n",
    "merged = BatchNormalization()(merged)\n",
    "merged = Dropout(DROUPOUT_RATE_DENSE)(merged)\n",
    "\n",
    "preds = Dense(1, activation='sigmoid')(merged)\n",
    "\n",
    "\n",
    "# Add class weight, magic feature for the unbalancement of training labels.\n",
    "if re_weight:\n",
    "    class_weight = {0: 1.309033281, 1: 0.471544715}\n",
    "else:\n",
    "    class_weight = None\n",
    "    \n",
    "\n",
    "# Train the model\n",
    "\n",
    "print('Starting the model training')\n",
    "\n",
    "model = Model(inputs=[seq1, seq2, magic_input], outputs=preds)\n",
    "model.compile(loss='binary_crossentropy', optimizer='nadam', metrics=['acc'])\n",
    "\n",
    "# Summerization of model\n",
    "model.summary()\n",
    "\n",
    "# Set early stopping (large patience should be useful)\n",
    "early_stopping =EarlyStopping(monitor='val_loss', patience=10)\n",
    "bst_model_path = 'lstm_featured.h5' \n",
    "model_checkpoint = ModelCheckpoint(bst_model_path, save_best_only=True, save_weights_only=True)\n",
    "\n",
    "\n",
    "hist = model.fit([data_1_train, data_2_train, leaks_train], labels_train, \\\n",
    "        validation_data=([data_1_val, data_2_val, leaks_val], labels_val, weight_val), \\\n",
    "        epochs=4, batch_size=2048, shuffle=True, \\\n",
    "        class_weight=class_weight, callbacks=[early_stopping, model_checkpoint])\n",
    "\n",
    "model.load_weights(bst_model_path) # sotre model parameters in .h5 file\n",
    "bst_val_score = min(hist.history['val_loss'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making the submission\n",
      "2345796/2345796 [==============================] - 5589s 2ms/step\n",
      "2345796/2345796 [==============================] - 2773s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "# Make the submission\n",
    "\n",
    "print('Making the submission')\n",
    "\n",
    "preds = model.predict([test_data_1, test_data_2, test_leaks], batch_size=8192, verbose=1)\n",
    "preds += model.predict([test_data_2, test_data_1, test_leaks], batch_size=8192, verbose=1)\n",
    "preds /= 2\n",
    "\n",
    "submission = pd.DataFrame({'test_id':test_ids, 'is_duplicate':preds.ravel()})\n",
    "submission.to_csv('Models/lstm_featured_1.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison Experiments\n",
    "- Network Sturcutres \n",
    "  - We also changed the neural network structure to see if there is an improvement on the final score\n",
    "    - Here the baseline is the `lstm_featured.ipynb`, which scores 0.16515\n",
    "    - Add substract and multiply on **featured_lstm** feature\n",
    "      - Scored 0.16674, no obvious improvements\n",
    "    - Change N_DENSE to N_DENSE/2\n",
    "      - Scored 0.16681 while 0.16515 is the baseline\n",
    "    - Add Dense layer after merging features\n",
    "      - Scored 0.1840 while 0.16515 is the baseline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
