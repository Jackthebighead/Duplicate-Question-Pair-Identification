{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tuning Process of Models in `Models.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Model\n",
    "corresponding to `Model_1`\n",
    "baseline parameter: alpha:0.001, l2, log loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ex_1\n",
    "\n",
    "[0.0005,0.0015]\n",
    "\n",
    "For values of alpha =  0.00061 The log loss is: 0.4909442774314729   \n",
    "For values of alpha =  0.00062 The log loss is: 0.4909337781957245   \n",
    "For values of alpha =  0.00066 The log loss is: 0.49132529625952975   \n",
    "For values of alpha =  0.00069 The log loss is: 0.4912918646967562   \n",
    "For values of alpha =  0.00078 The log loss is: 0.4911582346504402    \n",
    "For values of alpha =  0.00083 The log loss is: 0.4911352146224537 \n",
    "For values of alpha =  0.00087 The log loss is: 0.49112285323178234  \n",
    "For values of alpha =  0.00087 The log loss is: 0.49112285323178234  \n",
    "For values of alpha =  0.0009 The log loss is: 0.4910847420506273  \n",
    "For values of alpha =  0.00091 The log loss is: 0.49111552910788075  \n",
    "For values of alpha =  0.00094 The log loss is: 0.4915385086613925  \n",
    "For values of alpha =  0.00095 The log loss is: 0.491531977211532  \n",
    "For values of alpha =  0.00102 The log loss is: 0.49149376296832886  \n",
    "For values of alpha =  0.00106 The log loss is: 0.4913569528281331  \n",
    "For values of alpha =  0.00108 The log loss is: 0.49135209893003845  \n",
    "For values of alpha =  0.00109 The log loss is: 0.4913498766056343  \n",
    "For values of alpha =  0.00118 The log loss is: 0.4914264061558155  \n",
    "For values of alpha =  0.0012 The log loss is: 0.49142184201788597  \n",
    "For values of alpha =  0.00134 The log loss is: 0.49126080115881693  \n",
    "For values of alpha =  0.00137 The log loss is: 0.49126880020867447  \n",
    "\n",
    "For values of best alpha =  0.00062 The train log loss is: 0.49097290057809445  \n",
    "Total number of data points : 2345796  \n",
    "\n",
    "- This indicates that 0.001 is not optimal enough, we can go deeper to find the best alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ex_2\n",
    "\n",
    "[0.00055,0.00065]    \n",
    " \n",
    "For values of alpha =  0.00056 The log loss is: 0.490808421736002   \n",
    "For values of alpha =  0.00056 The log loss is: 0.490808421736002   \n",
    "For values of alpha =  0.00057 The log loss is: 0.4912325581784119   \n",
    "For values of alpha =  0.00057 The log loss is: 0.4912325581784119   \n",
    "For values of alpha =  0.00058 The log loss is: 0.49097983922061034   \n",
    "For values of alpha =  0.00058 The log loss is: 0.49097983922061034   \n",
    "For values of alpha =  0.00059 The log loss is: 0.4909672866253685    \n",
    "For values of alpha =  0.00059 The log loss is: 0.4909672866253685   \n",
    "For values of alpha =  0.00059 The log loss is: 0.4909672866253685   \n",
    "For values of alpha =  0.00059 The log loss is: 0.4909672866253685   \n",
    "For values of alpha =  0.00059 The log loss is: 0.4909672866253685   \n",
    "For values of alpha =  0.00059 The log loss is: 0.4909672866253685    \n",
    "For values of alpha =  0.0006 The log loss is: 0.49095544602549124   \n",
    "For values of alpha =  0.00061 The log loss is: 0.4909442774314729   \n",
    "For values of alpha =  0.00061 The log loss is: 0.4909442774314729   \n",
    "For values of alpha =  0.00061 The log loss is: 0.4909442774314729  \n",
    "For values of alpha =  0.00062 The log loss is: 0.4909337781957245    \n",
    "For values of alpha =  0.00062 The log loss is: 0.4909337781957245   \n",
    "For values of alpha =  0.00063 The log loss is: 0.49092389458463614   \n",
    "For values of alpha =  0.00064 The log loss is: 0.49135064535894685    \n",
    "\n",
    "For values of best alpha =  0.00056 The train log loss is: 0.4912559627602768       \n",
    "Total number of data points : 2345796 \n",
    "\n",
    "- This indicates that 0.00062 may not be the best one, we can go deeper, and maybe we can expand the boundaries since the original boundary [0.0005,0.0015] maybe small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ex_3\n",
    "\n",
    "[0.0001 ,0.0006]\n",
    "\n",
    "For values of alpha =  0.00016 The log loss is: 0.49384094997969574    \n",
    "For values of alpha =  0.00016 The log loss is: 0.49384094997969574    \n",
    "For values of alpha =  0.00018 The log loss is: 0.494641002032978    \n",
    "For values of alpha =  0.00019 The log loss is: 0.4931691872942023   \n",
    "For values of alpha =  0.00024 The log loss is: 0.49218674480451446   \n",
    "For values of alpha =  0.00026 The log loss is: 0.49295067000148374  \n",
    "For values of alpha =  0.00028 The log loss is: 0.49270701198295125  \n",
    "For values of alpha =  0.00028 The log loss is: 0.49270701198295125  \n",
    "For values of alpha =  0.0003 The log loss is: 0.49249952721289186  \n",
    "For values of alpha =  0.00031 The log loss is: 0.49164649886838585  \n",
    "For values of alpha =  0.00032 The log loss is: 0.4915856890026421  \n",
    "For values of alpha =  0.00032 The log loss is: 0.4915856890026421  \n",
    "For values of alpha =  0.00036 The log loss is: 0.4913864021069505   \n",
    "For values of alpha =  0.00038 The log loss is: 0.49110436728980356  \n",
    "For values of alpha =  0.00039 The log loss is: 0.4910713803879264  \n",
    "For values of alpha =  0.00039 The log loss is: 0.4910713803879264  \n",
    "For values of alpha =  0.00044 The log loss is: 0.49175926053645874  \n",
    "For values of alpha =  0.00045 The log loss is: 0.49171123833146013  \n",
    "For values of alpha =  0.00052 The log loss is: 0.49085751433255137  \n",
    "For values of alpha =  0.00054 The log loss is: 0.49083126591310633  \n",
    "\n",
    "For values of best alpha =  0.00054 The train log loss is: 0.49129145922394674     \n",
    "Total number of data points : 2345796     \n",
    "\n",
    "- This indicates that the best alpha is not in (0.0001,0.00054)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ex_4\n",
    "[0.00054, 0.00056]\n",
    "\n",
    "For values of alpha =  0.00054 The log loss is: 0.49083126591310633   \n",
    "For values of alpha =  0.00054 The log loss is: 0.49083126591310633  \n",
    "For values of alpha =  0.00054 The log loss is: 0.49083126591310633  \n",
    "For values of alpha =  0.00054 The log loss is: 0.49083126591310633  \n",
    "For values of alpha =  0.00055 The log loss is: 0.49081944083172485  \n",
    "For values of alpha =  0.00055 The log loss is: 0.49081944083172485  \n",
    "For values of alpha =  0.00055 The log loss is: 0.49081944083172485  \n",
    "For values of alpha =  0.00055 The log loss is: 0.49081944083172485  \n",
    "For values of alpha =  0.00055 The log loss is: 0.49081944083172485  \n",
    "For values of alpha =  0.00055 The log loss is: 0.49081944083172485  \n",
    "For values of alpha =  0.00055 The log loss is: 0.49081944083172485  \n",
    "For values of alpha =  0.00055 The log loss is: 0.49081944083172485  \n",
    "For values of alpha =  0.00055 The log loss is: 0.49081944083172485  \n",
    "For values of alpha =  0.00055 The log loss is: 0.49081944083172485  \n",
    "For values of alpha =  0.00055 The log loss is: 0.49081944083172485  \n",
    "For values of alpha =  0.00055 The log loss is: 0.49081944083172485  \n",
    "For values of alpha =  0.00055 The log loss is: 0.49081944083172485  \n",
    "For values of alpha =  0.00055 The log loss is: 0.49081944083172485  \n",
    "For values of alpha =  0.00056 The log loss is: 0.490808421736002  \n",
    "For values of alpha =  0.00056 The log loss is: 0.490808421736002  \n",
    "  \n",
    "For values of best alpha =  0.00056 The train log loss is: 0.4912559627602768    \n",
    "Total number of data points : 2345796     \n",
    "\n",
    "- This indicates the best alpha in a approximate way is 0.00056, i don't plan to go futher ^^, cause too boring."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ex_5\n",
    "\n",
    "hinge loss  [0.0005 ,0.0015]   \n",
    "\n",
    "For values of alpha =  0.00061 The log loss is: 0.49463939243347016   \n",
    "For values of alpha =  0.00062 The log loss is: 0.49538971827590333   \n",
    "For values of alpha =  0.00066 The log loss is: 0.49483187296923675   \n",
    "For values of alpha =  0.00069 The log loss is: 0.4945296662128364  \n",
    "For values of alpha =  0.00078 The log loss is: 0.4944620157266401   \n",
    "For values of alpha =  0.00083 The log loss is: 0.49462959316914334  \n",
    "For values of alpha =  0.00087 The log loss is: 0.49516118443281415   \n",
    "For values of alpha =  0.00087 The log loss is: 0.49516118443281415   \n",
    "For values of alpha =  0.0009 The log loss is: 0.494937523905568   \n",
    "For values of alpha =  0.00091 The log loss is: 0.4948265848648259   \n",
    "For values of alpha =  0.00094 The log loss is: 0.4931872576192068   \n",
    "For values of alpha =  0.00095 The log loss is: 0.49290105605761186   \n",
    "For values of alpha =  0.00102 The log loss is: 0.49349716841252206   \n",
    "For values of alpha =  0.00106 The log loss is: 0.49363772286030305  \n",
    "For values of alpha =  0.00108 The log loss is: 0.4928561853878051   \n",
    "For values of alpha =  0.00109 The log loss is: 0.4934795800050535   \n",
    "For values of alpha =  0.00118 The log loss is: 0.49352106393365514   \n",
    "For values of alpha =  0.0012 The log loss is: 0.4934770305928486    \n",
    "For values of alpha =  0.00134 The log loss is: 0.49347221740238784    \n",
    "For values of alpha =  0.00137 The log loss is: 0.4934511678417422   \n",
    "    \n",
    "For values of best alpha =  0.00108 The train log loss is: 0.4944649391165179      \n",
    "Total number of data points : 2345796       \n",
    " \n",
    "- worse than "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ex_6\n",
    "hinge loss [0.0001,0.001]  \n",
    "\n",
    "For values of alpha =  0.0002 The log loss is: 0.49777270240747523   \n",
    "For values of alpha =  0.00021 The log loss is: 0.4951160285304517   \n",
    "For values of alpha =  0.00025 The log loss is: 0.49787249459461913   \n",
    "For values of alpha =  0.00027 The log loss is: 0.4943786651409376   \n",
    "For values of alpha =  0.00035 The log loss is: 0.49336874893516586  \n",
    "For values of alpha =  0.00039 The log loss is: 0.4949591074978107   \n",
    "For values of alpha =  0.00043 The log loss is: 0.49474905080665377   \n",
    "For values of alpha =  0.00043 The log loss is: 0.49474905080665377    \n",
    "For values of alpha =  0.00046 The log loss is: 0.49425263182063045   \n",
    "For values of alpha =  0.00047 The log loss is: 0.4960081701984072    \n",
    "For values of alpha =  0.00049 The log loss is: 0.49595568301924525   \n",
    "For values of alpha =  0.0005 The log loss is: 0.4950530440372129    \n",
    "For values of alpha =  0.00057 The log loss is: 0.49569112972533413    \n",
    "For values of alpha =  0.0006 The log loss is: 0.4955841287565558    \n",
    "For values of alpha =  0.00062 The log loss is: 0.49538971827590333    \n",
    "For values of alpha =  0.00063 The log loss is: 0.49527966316785277    \n",
    "For values of alpha =  0.00072 The log loss is: 0.494499775874675     \n",
    "For values of alpha =  0.00073 The log loss is: 0.49450163875471626   \n",
    "For values of alpha =  0.00085 The log loss is: 0.4949526713420252   \n",
    "For values of alpha =  0.00088 The log loss is: 0.49489975943439724    \n",
    "  \n",
    "For values of best alpha =  0.00035 The train log loss is: 0.4946444764839469   \n",
    "Total number of data points : 2345796  \n",
    "\n",
    "- This indicates we can give up this interval."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ex_7\n",
    "   \n",
    "hinge loss [0.001 ,0.00110]  \n",
    "\n",
    "For values of alpha =  0.00101 The log loss is: 0.492866118648331   \n",
    "For values of alpha =  0.00101 The log loss is: 0.492866118648331    \n",
    "For values of alpha =  0.00102 The log loss is: 0.49349716841252206   \n",
    "For values of alpha =  0.00102 The log loss is: 0.49349716841252206   \n",
    "For values of alpha =  0.00103 The log loss is: 0.49289541593372077    \n",
    "For values of alpha =  0.00103 The log loss is: 0.49289541593372077   \n",
    "For values of alpha =  0.00104 The log loss is: 0.4931175296593425    \n",
    "For values of alpha =  0.00104 The log loss is: 0.4931175296593425      \n",
    "For values of alpha =  0.00104 The log loss is: 0.4931175296593425    \n",
    "For values of alpha =  0.00104 The log loss is: 0.4931175296593425    \n",
    "For values of alpha =  0.00104 The log loss is: 0.4931175296593425     \n",
    "For values of alpha =  0.00104 The log loss is: 0.4931175296593425     \n",
    "For values of alpha =  0.00105 The log loss is: 0.4936177441668784     \n",
    "For values of alpha =  0.00106 The log loss is: 0.49363772286030305      \n",
    "For values of alpha =  0.00106 The log loss is: 0.49363772286030305     \n",
    "For values of alpha =  0.00106 The log loss is: 0.49363772286030305    \n",
    "For values of alpha =  0.00107 The log loss is: 0.49356049793330203    \n",
    "For values of alpha =  0.00107 The log loss is: 0.49356049793330203      \n",
    "For values of alpha =  0.00108 The log loss is: 0.4928561853878051     \n",
    "For values of alpha =  0.00109 The log loss is: 0.4934795800050535      \n",
    "    \n",
    "For values of best alpha =  0.00108 The train log loss is: 0.4944649391165179     \n",
    "Total number of data points : 2345796   \n",
    "\n",
    "- Seems 0.00108 is the best, but still not better than log loss.\n",
    "\n",
    "- Still l1 regularization is not better as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "corresponding to Model_2, baseline is n_estimator=100, max_depth=10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ex_1\n",
    "\n",
    "Depth=[]\n",
    "\n",
    "Depth =  5 Test Log Loss  0.502191964128032    \n",
    "Depth =  10 Test Log Loss  0.45798638148427306    \n",
    "Depth =  12 Test Log Loss  0.4448225633020868   \n",
    "Depth =  15 Test Log Loss  0.42960001452217944    \n",
    "Depth =  20 Test Log Loss  0.41408852169607335    \n",
    "Depth =  25 Test Log Loss  0.40617702126135474   \n",
    "Depth =  50 Test Log Loss  0.4023047368247194   \n",
    "For values of best depth =  50 The train log loss is: 0.11704409841871524    \n",
    "Total number of data points : 2345796      \n",
    "\n",
    "- I want to try more depth since there are 221 features in total."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ex_2\n",
    "\n",
    "Depth =  50 Test Log Loss  0.4023047368247194     \n",
    "Depth =  100 Test Log Loss  0.40241972171040963    \n",
    "Depth =  150 Test Log Loss  0.40241972171040963   \n",
    "Depth =  200 Test Log Loss  0.40241972171040963    \n",
    "best depth is: 50    \n",
    "\n",
    "- Whoops, the log loss gets sactuated as the Depth goes up, i should focus on depths in [50,100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ex_3\n",
    "\n",
    "Depth =  50 Test Log Loss  0.4023047368247194   \n",
    "Depth =  60 Test Log Loss  0.40242198037031696   \n",
    "Depth =  70 Test Log Loss  0.4024157772054408      \n",
    "Depth =  80 Test Log Loss  0.40241972171040963    \n",
    "Depth =  90 Test Log Loss  0.40241972171040963   \n",
    "Depth =  100 Test Log Loss  0.40241972171040963    \n",
    "best depth is: 50    \n",
    "\n",
    "- Seems 50 is the best depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGB\n",
    "- corresponding to model_3, below is the baseline, `scored 3.9`   \n",
    "- I plan to use RandomizeSearchCV to train a best model\n",
    "- Another problem raised is that the training time is huge, every model needs 30-60 mins to train, I only try to do some basic fine tuning in this weekend, will do that in PROCESS 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dist = {\"n_estimators\":randint(100,250),\n",
    "              \"learning_rate\":uniform(0,0.2),\n",
    "              \"max_depth\": randint(2,5),\n",
    "              \"gamma\": uniform(0,4),\n",
    "              \"subsample\":uniform(0.7,0.3),\n",
    "              \"colsample_bytree\": uniform(0.7,0.3),\n",
    "              \"min_child_weight\": randint(2, 8),\n",
    "              \"reg_alpha\":uniform(100,300),\n",
    "              \"reg_lambda\":uniform(100,300)}\n",
    "\n",
    "xgbclf = RandomizedSearchCV(xgb.XGBClassifier(random_state=25, n_jobs=-1), param_distributions=param_dist,\n",
    "                                   n_iter=8,scoring='neg_log_loss',cv=5,n_jobs=-1)\n",
    "xgbclf.fit(x_train_model_3, y_train_model_3)\n",
    "\n",
    "pickle.dump(xgbclf,open('model_xgbclf.p','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ex_1\n",
    "Below are the parameters\n",
    "\n",
    "- the best is:    \n",
    "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "              colsample_bynode=1, colsample_bytree=0.9214510922782584,\n",
    "              gamma=3.296209360896669, gpu_id=-1, importance_type='gain',\n",
    "              interaction_constraints='', learning_rate=0.16061986774209408,\n",
    "              max_delta_step=0, max_depth=6, min_child_weight=4, missing=nan,\n",
    "              monotone_constraints='()', n_estimators=211, n_jobs=-1,\n",
    "              num_parallel_tree=1, objective='binary:logistic', random_state=25,\n",
    "              reg_alpha=226.48273559068107, reg_lambda=227.12379566430604,\n",
    "              scale_pos_weight=1, subsample=0.9089921201472645,\n",
    "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
    "- Scored 0.378\n",
    "- A little better than rf model, but i expected a bigger boosting, so keep changing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dist = {\"n_estimators\":randint(200,350),\n",
    "              \"learning_rate\":uniform(0,0.2),\n",
    "              \"max_depth\": randint(2,10),\n",
    "              \"gamma\": uniform(0,4),\n",
    "              \"subsample\":uniform(0.7,0.3),\n",
    "              \"colsample_bytree\": uniform(0.7,0.3),\n",
    "              \"min_child_weight\": randint(2, 8),\n",
    "              \"reg_alpha\":uniform(100,300),\n",
    "              \"reg_lambda\":uniform(100,300)}\n",
    "\n",
    "xgbclf = RandomizedSearchCV(xgb.XGBClassifier(random_state=25, n_jobs=-1), param_distributions=param_dist,\n",
    "                                   n_iter=10,scoring='neg_log_loss',cv=5,n_jobs=-1)\n",
    "xgbclf.fit(x_train_model_3, y_train_model_3)\n",
    "\n",
    "# store the model\n",
    "pickle.dump(xgbclf,open('model_xgbclf_1.p','wb'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ex_2\n",
    "- the best is: \n",
    "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "              colsample_bynode=1, colsample_bytree=0.9637181842490654,\n",
    "              gamma=1.0416906932537264, gpu_id=-1, importance_type='gain',\n",
    "              interaction_constraints='', learning_rate=0.07345283873541475,\n",
    "              max_delta_step=0, max_depth=7, min_child_weight=4, missing=nan,\n",
    "              monotone_constraints='()', n_estimators=448, n_jobs=-1,\n",
    "              num_parallel_tree=1, objective='binary:logistic', random_state=25,\n",
    "              reg_alpha=117.00041949261467, reg_lambda=207.2740930580024,\n",
    "              scale_pos_weight=1, subsample=0.7328558841394341,\n",
    "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
    "- Test score: 0.3898591171469916\n",
    "- Scored 0.369\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from scipy.stats import randint\n",
    "from scipy.stats import uniform\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import pickle\n",
    "\n",
    "param_dist = {\"n_estimators\":randint(200,500),\n",
    "              \"learning_rate\":uniform(0,0.1),\n",
    "              \"max_depth\": randint(2,8),\n",
    "              \"gamma\": uniform(0,4),\n",
    "              \"subsample\":uniform(0.7,0.3),\n",
    "              \"colsample_bytree\": uniform(0.7,0.3),\n",
    "              \"min_child_weight\": randint(2, 8),\n",
    "              \"reg_alpha\":uniform(100,300),\n",
    "              \"reg_lambda\":uniform(100,300)}\n",
    "\n",
    "xgbclf = RandomizedSearchCV(xgb.XGBClassifier(random_state=25, n_jobs=-1), param_distributions=param_dist,\n",
    "                                   n_iter=15,scoring='neg_log_loss',cv=5,n_jobs=-1)\n",
    "xgbclf.fit(x_train_model_3, y_train_model_3)\n",
    "\n",
    "# store the model\n",
    "pickle.dump(xgbclf,open('model_xgbclf_2.p','wb'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ex_3\n",
    "\n",
    "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "              colsample_bynode=1, colsample_bytree=0.7753935870171801,\n",
    "              gamma=0.6903305586594737, gpu_id=-1, importance_type='gain',\n",
    "              interaction_constraints='', learning_rate=0.08852771036096269,\n",
    "              max_delta_step=0, max_depth=14, min_child_weight=6, missing=nan,\n",
    "              monotone_constraints='()', n_estimators=572, n_jobs=-1,\n",
    "              num_parallel_tree=1, objective='binary:logistic', random_state=25,\n",
    "              reg_alpha=112.05220197885892, reg_lambda=151.32228422889304,\n",
    "              scale_pos_weight=1, subsample=0.816762180176789,\n",
    "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
    "              \n",
    "              \n",
    "              \n",
    "              \n",
    "              \n",
    "{'colsample_bytree': 0.7753935870171801,\n",
    " 'gamma': 0.6903305586594737,\n",
    " 'learning_rate': 0.08852771036096269,\n",
    " 'max_depth': 14,\n",
    " 'min_child_weight': 6,\n",
    " 'n_estimators': 572,\n",
    " 'reg_alpha': 112.05220197885892,\n",
    " 'reg_lambda': 151.32228422889304,\n",
    " 'subsample': 0.816762180176789}\n",
    " \n",
    "- Scored 0.349\n",
    "- Quite a good result\n",
    "- Now the first round of fine tuning is done. I will try to add more features to see if the results are getting better. Good luck to my model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
